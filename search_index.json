[["getting-started-with-r.html", "Bioinformatics Workshop Gitbook Session 1 – Getting Started with R 1.1 Why do we use R? 1.2 Get a good text editor 1.3 R studio 1.4 Installing R 1.5 Annotating code 1.6 Defining a working directory in R", " Bioinformatics Workshop Gitbook Juan C. Santos &amp; Randy Ortiz 2021-03-31 Session 1 – Getting Started with R 1.1 Why do we use R? At the core of modern bioinformatics that is the user friendly is R. This is a programming language and free software environment for easy application of diverse methods for statistical, data mining, file processing and graphics. R is extremely flexible and provides an easily customizable data processing that ranges from simple to very complex analyses. Here are some advantages of R: R is free to download and use while other similar environments (e.g., SAS, SPSS, Matlab) require licenses and fees. R is available across platforms including Windows, Mac and Linux R is always evolving and at its core nature is being open source. The R community is huge and global, yet the core base software tends to be relatively stable, the expanded functionality derived by add-ons packages expand the capabilities of core R. R provides easy and accessible data processing. You can import diverse types of datasets from text files, Excel sheets, sequence data, images, etc. R provides easy data manipulation including sub-setting (i.e., you filter your source data using filters), transformation, encryption, and export to other formats. R provides one of the most extensive data visualization tools that are freely available with extensive manuals, vignettes and online boards. Such graphics range from simple boxplots to multipaneled, tiling, and even GIS maps. R statistical applications are diverse and easy to implement. Many basic univariate methods are preloaded, and newest tools and models are provided usually as add-ons developed by authors of such methods. R provides an easy way to share your methods (scripts) and reproduce illustrative examples of their implementation. Many of these reside as add-on packages, others in GitHub pages and supplementary materials of peer-reviewed publications. A) The R project: https://www.r-project.org/ This is the main repository of R, add-on packages, documentation, and source code B) Looking for help when using R (searching websites): https://stackoverflow.com Probably one of the most useful Q/A sites for quick and practical responses to punctual questions about programming and R use. This site follows a format where someone ask or presents a problem and the community will try to respond with suggestions and practical applications of the R code to address the question. Some answers try to use the data in the question to fix the issue or problem to illustrate an answer the problem. The beauty of this approach is that the readers will vote on the provided answers and those with most votes move up among other competing responses (i.e., an answer with a 50 is considered better than one with a 10). Likewise, readers can provide discussions, comments, other examples to complement responses. Likewise, you can also “google” your questions and it is very likely that you would be pointed to a “stackoverflow” response, which furthers the value that most R users consider of the answers in that site. Moreover, the stackoverflow website is oriented to answer most questions from most other programing languages (i.e., Python, java, etc). Therefore, I suggest to finish your questions/queries with “… in R” or use the prefix [r] (e.g., “[r] weighted mean”), so it limits its search to R applicable answers. https://stackoverflow.com/tags/r/info An excellent guide for R related, examples, references, index of answers relevant to R http://rseek.org This is a search engine limited to mostly R based websites C) Repositories for R-packages (add-ons to basic R): CRAN This is the main repository. To get the packages that you need, look on your left side for Software and then click on Packages. You can get packages by: Table of available packages, sorted by date of publication or Table of available packages, sorted by name. Bioconductor This archive provides tools for the analysis and comprehension of high-throughput genomic data. Bioconductor houses R-packages (i.e., open source and open development) as well as older packages that could not be found CRAN. Bioconductor provides two releases each year, and also have a very active user community. D) Looking for suggestions of what R-packages to use: http://cran.r-project.org/web/views This site provides guidance about which packages are available as R-add on libraries in the CRAN repository. Each topic will give a brief overview of the included packages and which packages should be included (or excluded) - and they are not meant to endorse the “best” packages for a given task. http://search.bioconductor.jp/ This provides a search tool for functions in Bioconductor packages. R-bloggers This a standard repository where news, conferences and other summaries relevant to the R community are posted. The R Graph Gallery This graphics gallery provides a collection of plots, charts and illustrations made with the R. As indicated by this website, hundreds of charts are displayed in several sections, always with their accompanying code. Likewise, this gallery makes a focus on the tidyverse and ggplot2. 1.2 Get a good text editor Any scripting language such as the R language requires you to write code that can be read and annotated (i.e., you add notes that will help you understand what that piece of code is doing). Therefore, finding a good text editor is essential. Here are several suggestions for free text editor: 1) Sublime (macOS, Windows, Linux): https://www.sublimetext.com/3 It is free to download, but if once in while will remind you to pay for license (i.e., … Sublime Text may be downloaded and evaluated for free, however a license must be purchased for continued use. There is currently no enforced time limit for the evaluation) It is incredible customizable, and you can select the type of language that you are using to help you read your code. Personally, I use Sublime. To improve visualization, you can start by installing Install Package Control by clicking Tools in the menu and then select Install Package Control. You can change color on this text editor by clicking Preferences then Color Scheme… and pick from the available options (I use Monokai). On the right corner you can also select an extension by clicking on current (e.g., plain text) and change to R. You can further customize the color scheme with different add-on packages for Sublime. One of those is Rainglow, to install this package your start by clicking on the menu tab as follows: Go Sublime Text -&gt; Preferences -&gt; Package Control. Select Package Control: Install Package . Type Rainglow and select it. Go to Sublime Text -&gt; Preferences -&gt; Package Control -&gt; Color Scheme…. You can now select from hundreds of color schemes and try one that is nice or makes your use of the text editor easy. 2) Atom (macOS, Windows, Linux): https://atom.io/ It is free and customizable. I am not familiar with this one, but my students have found it more appealing for it truly free nature and open source. 3) BBEdit (macOS) https://www.barebones.com/products/bbedit/ It is useful for lots of find and replace actions, or when you need a quick file exploration. It provides a free trial, this ends after 30 days. For my needs, I do not need the extra stuff after the trial has ended (i.e., this text editor will continue to work after trial, but not some of its more sophisticated features) 4) Notepad++ (Windows) https://notepad-plus-plus.org/ It is one of the best for windows and it is free. I am not very familiar with this editor yet it is one of the best for Windows (PC) computers. 1.3 R studio https://rstudio.com/products/rstudio/download/ This software application defied as an integrated development environment (IDE) is designed to work and interact with R. Many people that use R always prompts new users to use R-studio. Personally, I do not use such application as it obfuscates my coding. However, most of the applications and materials developed in this workshop should be easily implemented within R-studio. If you decide or are familiar with R-studio, you can use it. However, I will be less familiar on their implementation in such IDE and less helpful in fixing errors that prompts within R-studio. If you use this software, choose the free version. 1.4 Installing R Here is instructions to guide you in this process: 1) You need to define where you want to download R and your personalized R library (more on this later). I recommend the desktop or some easy to access folder (in the documents). 2) To install R you need to access the a repository by clicking on the mirrors page: https://cran.r-project.org/mirrors.html 3) Try selecting the closets to your location. For NYC, you can chose Case Western Reserve University, Cleveland, OH: https://cran.case.edu/ 0-Cloud: https://cloud.r-project.org/ 4) You will see a list of links understand Download and Install R. These include links to precompiled binary distributions of the base system for Windows and Mac. 5a) OSX: For MAC (macOS 10.13 – High Sierra and higher) click on Download R for (Mac) OS X. You will need to download the corresponding to latest version, e.g., R-4.0.3.pkg (notarized and signed) R 4.0.3 “Bunny-Wunnies Freak Out” released on 2020/10/10. For latest macs, you need to install also XQuartz since it is no longer part of OS X https://www.xquartz.org/ For older MACs (macOS 10.11 – El Capitan) click on Download R for (Mac) OS X and then on R-3.6.3.nn.pkg (signed). 5b) Windows: For computers running on Windows click on Download R for Windows and then on install R for the first time and then Download R 4.0.3 for Windows 5c) Linux: For computers running on Linux (or its variants) click on Download R for Linux and choose your operative system. Ask Randy Ortiz (Santos Lab) for further help. 1.5 Annotating code Throughout this gitbook, I will use # to indicate annotation for the user (you) to read what the computer will be doing, comments or results that you are expected obtain. The annotation of code is fundamental and good habit to have from the beginning. Annotation helps you to understand what a section of code is trying to accomplish. Any text after the # is ignored by the computer as you copy and paste your code on the console from the text editor. ## the next chuck of code will print on the screen &quot;DO NOT FORGET TO ANNOTATE YOUR CODE&quot; cat(&quot;\\nDO NOT FORGET TO ANNOTATE YOUR CODE\\n&quot;) #DO NOT FORGET TO ANNOTATE YOUR CODE 1.6 Defining a working directory in R R has a default directory on your computer, which can be one not easily accessible directory, or you might one specific for your project. You can get the information of the current output directory by typing getwd() ## Print my current working directory getwd() #[1] &quot;/Users/santosj&quot; In most cases, you might want to use another directory as your working directory. The usual way to change the working directory with different approaches and some are operating system specific. You can create your desired directory or select a specific directory as your output by: macOS using R menu: On R console, click on Misc then Change Working Directory… or pressing command-D. To make sure that the working directory has change use: getwd() #[1] &quot;/Users/santosj/Desktop/Teach_R/my_working_directory&quot; macOS using drag &amp; drop: Select you output directory and drag &amp; drop this folder in the R console to get its path (i.e., specific address of the folder in your hard drive). For example, you will see the path of the selected working directory ~/Desktop/Teach_R/my_working_directory #Error: unexpected &#39;/&#39; in &quot;~/&quot; Next, you can copy the path name ~/Desktop/Teach_R/my_working_directory and place within the function setwd() within quotations ## Change my working directory to the following path setwd(&quot;~/Desktop/Teach_R/my_working_directory&quot;) To make sure that the working directory has change use: getwd() #[1] &quot;/Users/santosj/Desktop/Teach_R/my_working_directory&quot; Windows: For PCs, you need to find the path of desired working directory. For this you need to highlight this folder, hold down the Shift key and right-click the file. Then, you click on Properties and copy the names (text) of Location that includes the folder name. For example, if you want to change your working directory to the folder icon my_working_directory Find its location: C:\\Users\\myPC\\Desktop You will get its path name as: C:\\Users\\myPC\\Desktop\\my_working_directory Then use the setwd() function as follows setwd(&quot;C:\\Users\\myPC\\Desktop\\my_working_directory&quot;) In some PCs, this will change the working directory to its desired path. However, in most, you will get the following error setwd(&quot;C:\\Users\\myPC\\Desktop\\my_working_directory&quot;) #Error: &#39;\\U&#39; used without hex digits in character string starting &quot;&quot;C:\\U&quot; This indicates that the R is recognizing the \\ (backslashes) as part of regular expressions. To make it work, you will need to change such \\ character to / (forward slashes). setwd(&quot;C:/Users/myPC/Desktop/my_working_directory&quot;) getwd() #[1] &quot;C:/Users/myPC/Desktop/my_working_directory&quot; Other have reported is to use double backslashes \\\\ setwd(&quot;C:\\\\Users\\\\myPC\\\\Desktop\\\\my_working_directory&quot;) getwd() #[1] &quot;C:/Users/myPC/Desktop/my_working_directory&quot; This will indicate that you have successfully change the working directory. "],["using-r-installing-packages-and-importingexporting-data.html", "Session 2 – Using R, Installing Packages and Importing/Exporting Data 2.1 Your first session 2.2 Doing some base functions and getting help 2.3 About R-packages 2.4 Installing packages 2.5 Importing and exporting data 2.6 Saving data as Rdata (.rdata, .rda)", " Session 2 – Using R, Installing Packages and Importing/Exporting Data 2.1 Your first session You will start by entering commands in the console. 1) After clicking on the R icon you should see the R console and the computer will be ready to receive commands This is the R console in a macOS computer. 2) The blinking command will be next to R prompts which are indicated by &gt;. This basically indicates where you can start typing commands like a excel or a calculator. Let’s try a simple addition: 1 + 2 #[1] 3 You can try more complicated calculations like the natural logarithm of 10: log(10) #[1] 2.302585 The R console gives you a [1] that indicates that the result is a vector of one element or scalar (i.e., 3 or 2.302585). If more that 1, this is called a vector and R will number elements sequentially in a vector. For example, you can ask for an expression for a sequence of number form 1 to 100 by addition of 1 as seq(1:100). This vector will have 100 elements. seq(1:100) #[1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #[20] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #[39] 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 #[58] 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 #[77] 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 #[96] 96 97 98 99 100 3) R provides easy command-line editing tools that will help you to recall previous commands. This help you change parameters in the expression or correct typos in it. For example, # Same result as before seq(from = 1, to = 100, by =1) # [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # [20] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # [39] 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # [58] 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 # [77] 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 # [96] 96 97 98 99 100 You can press arrow up or Ctrl-P to recall previous command and we can change the argument by = to get sequence by addition of 5 instead of 1. seq(from = 1, to = 100, by =5) #[1] 1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 Typing errors are easy to introduce and R will let you know about then. For example, I forgot to add the = after to 100. This is not valid, and an error is produced. seq(from = 1, to 100, by =5) #Error: unexpected numeric constant in &quot;seq(from = 1, to 100&quot; Now, you can press arrow up to fix the error in the previous command and it can be fixed. 4) Here are some basic functions with numeric vectors. Let’s get create a vector of numbers named my_vector by assigning seven random numbers to it using the arrow &lt;- known as the assign operator (the = also works in similar fashion) and the function c(). my_vector &lt;- c(2,4,1,12,30,5,6) The c() is a base function to combine values into a vector or list. 5) You can read or retrieve the contents of my_vector by typing its name. my_vector #[1] 2 4 1 12 30 5 6 6) You can also get the structure of my_vector by using str(). This function provides a compactly display the structure of an arbitrary R object (e.g., a vector). This provides a diagnostic description and summary what is contains. str(my_vector) #num [1:7] 2 4 1 12 30 5 6 This indicates that the structure of my_vector is a numeric vector with 7 numbers. 2.2 Doing some base functions and getting help 1) Let’s create a new my_vector. This will overwrite the previous one. my_vector &lt;- rnorm(n = 1000, mean = 50, sd = 25) # I omitted the result because it has 1000 numbers my_vector The function rnorm() will draw n random numbers (in this case 1000) of a normal distribution with mean 50 and standard deviation of 25. 2) We can get the structure of my_vector. str(my_vector) #num [1:1000] 50.3 60.3 20.4 44.1 43.8 ... The structure of the my_vector object indicates that is a numeric vector of 1000 numbers. 3) We can confirm that the mean is 50 by using the function mean() and its standard deviation of 25 by using the function sd(). mean(my_vector) #[1] 50.29036 sd(my_vector) #[1] 25.05207 As you can see in this example, the mean and standard deviation are not exactly 50 and 25, respectively. However, it is very close as these parameters inputted in the rnorm() function. 4) We can get some information about each function used with the following using function ? and help(). You put the function that you want to get information after ? or within the parenthesis of help(). # get help about function &#39;mean&#39; ?mean # another way to ask help help(mean) The functions ? and help() provide a link into a new window with the corresponding manual of that function and how it is use and (in most cases) examples. This a transcript of the popup window for the help of mean. The name within {} is the library or package that it is from in this case base. mean {base} The long name of the mean() function, in this case: Arithmetic Mean. A description of the mean() function, in this case: &quot;Generic function for the (trimmed) arithmetic mean&quot; A usage example: mean(x, ...) ## Default S3 method: mean(x, trim = 0, na.rm = FALSE, ...) An explanation of its arguments: x -- An R object. Currently there are methods for numeric/logical vectors and date, date-time and time interval objects. Complex vectors are allowed for trim = 0, only. trim -- the fraction (0 to 0.5) of observations to be trimmed from each end of x before the mean is computed. Values of trim outside that range are taken as the nearest endpoint. na.rm -- a logical value indicating whether NA values should be stripped before the computation proceeds. ... -- further arguments passed to or from other methods. Further information about the function mean() are provided in the same pop up window as value, references and related functions. Finally, examples of how to use mean() are also provided. x &lt;- c(0:10, 50) xm &lt;- mean(x) c(xm, mean(x, trim = 0.10)) 5) Some basic functions for numeric vectors. # Arithmetic Mean mean(my_vector) # Standard Deviation sd (my_vector) # Variance var (my_vector) # Median Value median (my_vector) # Maximum Value max (my_vector) # Minimum Value min (my_vector) # Sums values in vector sum(my_vector) # Provides the number of elements in the vector length(my_vector) # Round numbers to of elements in the vector to number of digits round(3.1415, digits = 2) 6) Some other basic functions between two numeric vectors. my_vector2 &lt;- c(1:1000) # get the correlation between two vectors cor(my_vector, my_vector2) # Correlation between two numeric vectors #[1] -0.001297859 # get the covariance between two vectors cov(my_vector, my_vector2) # Covariance between two numeric vectors #[1] -9.390692 7) You can print what is in a vector (or object) or return the result of calculation, you just can type its name on the console and get what is contained on it. You can also use the function print(). 1:10 #[1] 1 2 3 4 5 6 7 8 9 10 my_numbers &lt;- 1:10 my_numbers #[1] 1 2 3 4 5 6 7 8 9 10 print(my_numbers) #[1] 1 2 3 4 5 6 7 8 9 10 8) We can construct a character vector my_names with text (i.e., string vector) that contains letter or numbers that are treated as text (i.e., not quantities). my_names &lt;- c(&quot;juan&quot;, &quot;c&quot;, &quot;santos&quot;, 123) my_names #[1] &quot;juan&quot; &quot;c&quot; &quot;santos&quot; &quot;123&quot; print(my_names) #[1] &quot;juan&quot; &quot;c&quot; &quot;santos&quot; &quot;123&quot; You can revise its structure with str(). You will notice that the numbers are within quotations. str(my_names) #chr [1:4] &quot;juan&quot; &quot;c&quot; &quot;santos&quot; &quot;123&quot; 9) Some more complex R data structures like matrices, lists or data.frames can also be printed and visualized using print(). my_matrix &lt;- matrix(c(1,2,3,4,5,6), 3,2) print(my_matrix) # [,1] [,2] #[1,] 1 4 #[2,] 2 5 #[3,] 3 6 In this case, we are printing the object my_matrix that has the numbers in the sequence from 1 to 6 in 3 rows and 2 columns. 10) However, print() can only return one object at a time. print(my_numbers, my_names, my_matrix) #Error in print.default(my_numbers, my_names, my_matrix) : # invalid &#39;digits&#39; argument #In addition: Warning message: #In print.default(my_numbers, my_names, my_matrix) : # NAs introduced by coercion In this case cannot print the three objects (i.e., my_numbers, my_names, my_matrix) at the same time. 11) We can use the function like cat() that allows you to concatenate and print several objects. cat(my_numbers,my_names) #1 2 3 4 5 6 7 8 9 10 juan c santos 123 12) One common use of the function cat() is to provide users with screen reports (i.e., print on the screen) on the status of a process that the computer is doing while running iterations or loops that are recursive and time consuming. cat(&quot;\\nPrint the two vectors my_numbers and my_names separed by *** &quot;, my_numbers, &quot; *** &quot;, my_names) #Print the two vectors my_numbers and my_names separed by *** 1 2 3 4 5 6 7 8 9 10 *** juan c santos 123 13) You can also list all object “in memory” that is to display the names of objects in your workspace using function ls(). ls() #[1] &quot;my_matrix&quot; &quot;my_names&quot; &quot;my_numbers&quot; &quot;my_vector&quot; You can also list objects and their structure with the function ls.str(). ls.str() #my_matrix : num [1:3, 1:2] 1 2 3 4 5 6 #my_names : chr [1:3] &quot;juan&quot; &quot;c&quot; &quot;santos&quot; &quot;123&quot; #my_numbers : int [1:10] 1 2 3 4 5 6 7 8 9 10 #my_vector : num [1:1000] 50.3 60.3 20.4 44.1 43.8 ... 14) You can also remove objects from your workspace with the function rm(). my_names #[1] &quot;juan&quot; &quot;c&quot; &quot;santos&quot; &quot;123&quot; rm(&quot;my_names&quot;) my_names #Error: object &#39;my_names&#39; not found #In this case the object &quot;my_names&quot; has been removed from the workspace 2.3 About R-packages R packages are a collection of R scripts of functions, libraries, datasets, and add-ons that can be call and loaded upon using library(). These packages add functions to the base packages and might include code written in other languages like C, Python, Fortran, etc. The packages are stored under a library directory called in the R environment by library(). However, these add-ons packages need to install by user, many require other packages to be able to run, and called beforehand into the R environment before use. For this workshop, we will install several packages based on data analyses, graphs and other computational needs. Before you install an R package One of the strengths of the R environment is the growing set of packages that can be installed and loaded to do specific analyses, plots, calculations and data management within this environment. To date (01/11/21), a total of CRAN repository features 16902 available packages written by the R community. To find all available packages, you can check the left menu under Software and click on Packages. These are usually two list based on (1) sorted by date of publication and (2) sorted by name. Some of the few disadvantages of such diversity of R-packages is that many of these packages might overlap in their functions, their function names, vary in the difficulty of their implementations, bugs and inherent errors in the scripts/functions (i.e., there is no warranty that they will work as described, the user has to evaluate the output). Given the open nature of these R-packages, the user can access the source code and modify, improve, adapt, combine, repurpose, etc. Some packages eventually become obsolete and disappear from CRAN if they are not maintained. However, those can be found in CRAN archives and might require a more convoluted installation (described below). 2.4 Installing packages 1) Check what packages are preloaded or were loaded in the R environment before installing new one using the function search(). search() #[1] &quot;.GlobalEnv&quot; &quot;tools:RGUI&quot; &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; &quot;package:utils&quot; #[7] &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; 2) Before you install a package, you could check which are useful to your intended goal. In most cases, you will already know what you want. However, if you are unsure, you could check the suggestions by R community posted in the CRAN Task Views. Here are some of the communities relevant for our workshop: CRAN Task View: Statistical Genetics: CRAN Task View: Graphic Displays &amp; Dynamic Graphics &amp; Graphic Devices &amp; Visualization: CRAN Task View: Phylogenetics, Especially Comparative Methods: CRAN Task View: Teaching Statistics: CRAN Task View: Multivariate Statistics: CRAN Task View: Bayesian Inference: 3) To install a package that you want and you can use install.packages() as indicated below. install.packages(&quot;reshape2&quot;) You will be prompted to select a CRAN mirror. Usually take the one geographically close like: USA (OH) [https] or the cloud: 0-Cloud [http]. Once you have accepted it will proceed to install. Sometimes, some other packages are required and R will install those during the process. In some rare cases, you might need to install those on your own (i.e., not authomatic installation) install.packages(&quot;reshape2&quot;) #--- Please select a CRAN mirror for use in this session --- #trying URL &#39;https://cloud.r-project.org/bin/macosx/el-capitan/contrib/3.6/reshape2_1.4.4.tgz&#39; #Content type &#39;application/x-gzip&#39; length 333148 bytes (325 KB) #================================================== #downloaded 325 KB #The downloaded binary packages are in # /var/folders/vx/rjmrkyz566zf4khl9xjhkqyr0000gp/T//RtmpwxVTvt/downloaded_packages 4) For PC users, you might need to define a folder (R library) where you R-packages will be kept. 5) To load a R-package already installed, call for its name using library(). library(reshape) if the library is successfully loaded, you will get NO RESPONSE in most cases as above. Some packages will report some information about the package being loaded successfully. 6) If errors occurs and the selected R-package was not installed, R will let know that there is a problem in some form. ## if the package name is misspelled or does not exist in CRAN then: install.packages(&quot;my_madeup_package&quot;) # Warning message: # package ‘my_madeup_package’ is not available (for R version 3.6.2) 7) If did not installed the R-package beforehand, R will also let you know that the package is not installed in your library. library(my_madeup_package) # Error in library(my_madeup_package) : # there is no package called ‘my_madeup_package’ 8) To install packages from older versions or no longer maintained in CRAN, you need to install this R-package ‘devtools.’ install.packages(&quot;devtools&quot;) library(devtools) 9) For example, we will install an older version (i.e., ver 1.4.3) of the R-package ‘FlexParamCurve,’ which is a statistics tool to fit complex parametric curves. install_version(&quot;FlexParamCurve&quot;, version = &quot;1.4.3&quot;, repos = &quot;http://cran.us.r-project.org&quot;) #Downloading package from url: http://cran.us.r-project.org/src/contrib/Archive/FlexParamCurve/FlexParamCurve_1.4-3.tar.gz #* installing *source* package ‘FlexParamCurve’ ... #** package ‘FlexParamCurve’ successfully unpacked and MD5 sums checked #** using staged installation #** R #** data #** byte-compile and prepare package for lazy loading #** help #*** installing help indices #** building package indices #** testing if installed package can be loaded from temporary location #** testing if installed package can be loaded from final location #** testing if installed package keeps a record of temporary installation path #* DONE (FlexParamCurve) 10) Most genetic, genomic, transcriptomic R-packages are Bioconductor. To date (1/11/21), this archive has 1974 packages. To install packages present on this archive, you need to follow the next instructions. ## test if &#39;BiocManager&#39; is installed if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) {install.packages(&quot;BiocManager&quot;)} Now, we can use the function BiocManager::install(\"my_desired_bioconductor_package\"). For example, we can install an excellent sequence aligner that is the Bioconductor R-package DECIPHER. BiocManager::install(&quot;DECIPHER&quot;) Note that R will ask you to update your older packages, in most cases you decline (i.e., type ‘n’). Update all/some/none? [a/s/n]: n 11) Some experimental or not yet released R-packages are sometimes stored in the developers’ repositories like GitHub (like this gitbook). This website has become one of the most popular store websites for free software for a diversity of fields. To install such packages you use again ‘devtools’ library(devtools) # install_github(&quot;github_handler/package_name&quot; # installing httr for web API (application programming interface) like donwload of data from the web install_github(&quot;r-lib/httr&quot;) 2.5 Importing and exporting data Importing data is crucial for analyses in R as most of these are produced, retrieved or generated by other software. For example, measurements might be collected and annotated in excel sheets, sequences might be encoded in fasta/fastq forma in text files, images are stored as .tiff, .jpeg or other formats, sound might be encoded in .wav, etc. A great advantage of R is that the same base software or R-packages can serve to bring those datasets into the R environment. However, most of these are imported as standard R data structures as data frames (i.e., similar to matrices with columns and rows that contain a single value). Next, we will survey the most common forms to data input 1) The simples way to input data in R by typing data in the R console. We explain this in more detail when we describe data structures. However, this should intuitive as follows. ## create a vector with some values named &#39;my_vector&#39; my_vector &lt;- c(1,4,5,2:20,99) my_vector #[1] 1 4 5 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 99 2) In most cases, you already have your data file in some folder in your computer or plan to import it from some online repository. In the case of importing data from file from a known folder in your computer, this can be done as easily in R. First, I will describe to import data from a text file with a tab delimited columns with numeric and character values. macOS: You need to get the path (folder location in your hard drive) so you can indicate R where to retrieve such information. For example, the file that you want is known folder. You just drag &amp; drop the folder in the R console and the path information will appear on it. # ~/Desktop/Teach_R/class_datasets # Next, we can explore what files are inside such folder. # NOTE THAT THIS PATH IS SPECIFIC FOR YOUR COMPUTER setwd(&quot;~/Desktop/Teach_R/class_datasets&quot;) path_to_my_datafiles &lt;- &quot;~/Desktop/Teach_R/class_datasets&quot; Note: You can get the path directly to the file by drag &amp; drop the file into the R console. Windows: For PCs, to get the path (folder location in your hard drive) so you can indicate R where to retrieve such information. For example, the file that you want is known folder. You need to find the path to this working directory. For this you need to highlight this folder, hold down the Shift key and right-click the file. Then, you click on Properties and copy the names (text) of Location that includes the folder and file names. For example, if you want to change your working directory to the folder icon class_datasets (this is a made-up folder, you can name it as you wish or might one specific one). Find its location: C:\\Users\\myPC\\Desktop You will get its path name as: C:\\Users\\myPC\\Desktop\\my_working_directory\\class_datasets Then use the setwd() function as follows setwd(&quot;C:\\Users\\myPC\\Desktop\\my_working_directory\\class_datasets&quot;) In some PCs, this will change the working directory to its desired path. However, in most, you will get the following error setwd(&quot;C:\\Users\\myPC\\Desktop\\my_working_directory\\class_datasets&quot;) #Error: &#39;\\U&#39; used without hex digits in character string starting &quot;&quot;C:\\U&quot; This indicates that the R is recognizing the \\ (backslashes) as part of regular expressions. To make it work, you will need to change such \\ character to / (forward slashes). setwd(&quot;C:/Users/myPC/Desktop/my_working_directory/class_datasets&quot;) getwd() #[1] &quot;C:/Users/myPC/Desktop/my_working_directory/class_datasets&quot; Other have reported is to use double backslashes \\\\ setwd(&quot;C:\\\\Users\\\\myPC\\\\Desktop\\\\my_working_directory\\\\class_datasets&quot;) getwd() #[1] &quot;C:/Users/myPC/Desktop/my_working_directory/class_datasets &quot; Note: Like in macOS, you can apply the same protocol for a file rather than a folder. 3) You can get a list of files inside that working directory using file.list(). list.files(path = path_to_my_datafiles, pattern = NULL, full.names = TRUE) #[1] &quot;/Users/santosj/Desktop/Teach_R/class_datasets/airway_scaledcounts.csv&quot; #[2] &quot;/Users/santosj/Desktop/Teach_R/class_datasets/mixed_datasets.xlsx&quot; #[3] &quot;/Users/santosj/Desktop/Teach_R/class_datasets/mtcars2_csv.csv&quot; #[4] &quot;/Users/santosj/Desktop/Teach_R/class_datasets/mtcars2_tab.txt&quot; my_vector_of_paths &lt;- list.files(path = path_to_my_datafiles, full.names = TRUE) You can select the type of text pattern or extension (e.g., .txt, .csv, .xlsx) to reduce the list of files. #we could select only those that have a .csv (comma delimited) files list.files(path = path_to_my_datafiles, pattern = &quot;*.csv&quot;, full.names = TRUE) # [1] &quot;/Users/santosj/Desktop/Teach_R/class_datasets/airway_scaledcounts.csv&quot; # [2] &quot;/Users/santosj/Desktop/Teach_R/class_datasets/mtcars2_csv.csv&quot; 4) We can now import the data of tab and comma delimited files as data frames (a data structure). # for tab delimited files while preserving headers my_mtcars2_file_df &lt;- read.table (file = &quot;/Users/santosj/Desktop/Teach_R/class_datasets/mtcars2_tab.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, stringsAsFactors = FALSE) head(my_mtcars2_file_df) # cars mpg cyl disp hp drat wt qsec vs am gear carb #1 Mazda_RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 #2 Mazda_RX4_Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 #3 Datsun_710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #4 Hornet_4_Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 #5 Hornet_Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 #6 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # for comma delimited files while preserving headers my_airway_file_df &lt;- read.table (file = &quot;/Users/santosj/Desktop/Teach_R/class_datasets/airway_scaledcounts.csv&quot;, header = TRUE, sep = &quot;,&quot;, stringsAsFactors = FALSE) head(my_airway_file_df) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 ENSG00000000003 723 486 904 445 1170 1097 806 604 #2 ENSG00000000005 0 0 0 0 0 0 0 0 #3 ENSG00000000419 467 523 616 371 582 781 417 509 #4 ENSG00000000457 347 258 364 237 318 447 330 324 #5 ENSG00000000460 96 81 73 66 118 94 102 74 #6 ENSG00000000938 0 0 1 0 2 0 0 0 5) You can save data frames as tab or comma delimited files using the function write.table(). However, you should make sure that the folder where you will like your files to be saved is defined. #define the the output folder setwd(&quot;/Users/santosj/Desktop/Teach_R/class_datasets&quot;) #confirm that the output directory is correct getwd() #[1] &quot;/Users/santosj/Desktop/Teach_R/class_datasets&quot; # to determine the separation between columns is defined by the argument &#39;sep =&#39;. For tabs, you use sep =”\\t” and for commas you use sep =”,” write.table(my_mtcars2_file_df, file = &quot;mtcars2_file_tab.txt&quot;, sep = &quot;\\t&quot;, col.names = TRUE) 6) We can now import the data from Excel spreadsheet files as data frames (a data structure). We could use R-package openxlsx and its function read.xlsx(). #install package if not present in your library install.packages(&quot;openxlsx&quot;) library(openxlsx) You can load the FIRST sheet to a data frame that has headers on the first row. my_mtcars2_xlsx_df &lt;- read.xlsx(xlsxFile = &quot;/Users/santosj/Desktop/Teach_R/class_datasets/mixed_datasets.xlsx&quot;, sheet = 1, startRow = 1, colNames = TRUE) head(my_mtcars2_xlsx_df) # cars mpg cyl disp hp drat wt qsec vs am gear carb #1 Mazda_RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 #2 Mazda_RX4_Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 #3 Datsun_710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #4 Hornet_4_Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 #5 Hornet_Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 #6 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Now, you can load the SECOND sheet to a data frame that has headers on the first row. my_airway_xlsx_df &lt;- read.xlsx(xlsxFile = &quot;/Users/santosj/Desktop/Teach_R/class_datasets/mixed_datasets.xlsx&quot;, sheet = 2, startRow = 1, colNames = TRUE) head(my_airway_xlsx_df) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 ENSG00000000003 723 486 904 445 1170 1097 806 604 #2 ENSG00000000005 0 0 0 0 0 0 0 0 #3 ENSG00000000419 467 523 616 371 582 781 417 509 #4 ENSG00000000457 347 258 364 237 318 447 330 324 #5 ENSG00000000460 96 81 73 66 118 94 102 74 #6 ENSG00000000938 0 0 1 0 2 0 0 0 7) We can save a data frame as an Excel spreadsheet with R-package openxlsx and its function write.xlsx(). #define the the output folder setwd(&quot;/Users/santosj/Desktop/Teach_R/class_datasets&quot;) #confirm that the output directory is correct getwd() #[1] &quot;/Users/santosj/Desktop/Teach_R/class_datasets&quot; my_airway_xlsx_df &lt;- write.xlsx(my_mtcars2_file_df, sheetName = &quot;my_mtcars_xlsx_sheet&quot;, file = &quot;mtcars_xlsx.xlsx&quot;) 2.6 Saving data as Rdata (.rdata, .rda) You might feel compelled to save your R session. This action will store your complete R workspace or selected “objects” from a workspace in a R-Data file with extension .rdata and .rda that can be loaded back by R. In most cases, you will do this to save you time in your next session. I do not usually save my R sessions, as the .rdata and .rda files are not human readable, and in most cases my interest are in the functionality of the scripts that call data and run functions within R rather than going back a preloaded session. "],["basic-data-structures-in-r.html", "Session 3 – Basic Data Structures in R 3.1 Scalars and vectors 3.2 Matrices 3.3 Arrays 3.4 Data Frames 3.5 Tables 3.6 Lists 3.7 Data Tables 3.8 Tibbles 3.9 DNAStringSets, RNAStringSets and AAStringSets", " Session 3 – Basic Data Structures in R 3.1 Scalars and vectors We already introduced vectors and scalars on the section “Your First R Session.” We will describe with more detail vectors and continue exploring these structures and functions that apply to them. 1) A numeric vector contains numbers. Notice the functionc() that combine such values into a vector. my_numeric_vector &lt;- c(1,3,45,56,1) my_numeric_vector #[1] 1 3 45 56 1 The function str() can help to identify its numeric structure. Notice the num text in the output that this indicates that is a vector of numeric values. str(my_numeric_vector) #num [1:5] 1 3 45 56 1 Likewise, the function class() is also useful to characterize a vector. class(my_numeric_vector) #[1] &quot;numeric&quot; 2) A vector of character strings contains characters. Note the quotations to contain text strings. my_character_vector &lt;- c(&quot;my&quot;, &quot;bioinformatics&quot;, &quot;class&quot;) my_character_vector #[1] &quot;my&quot; &quot;bioinformatics&quot; &quot;class&quot; str(my_character_vector) #chr [1:3] &quot;my&quot; &quot;bioinformatics&quot; &quot;class&quot; class(my_character_vector) #[1] &quot;character&quot; 3) Notice that if you include numbers with character elements, your vector will consider such numbers as characters. my_mix_vector &lt;- c(&quot;my&quot;, &quot;bioinformatics&quot;, &quot;class&quot;, 3.141593) my_mix_vector #[1] &quot;my&quot; &quot;bioinformatics&quot; &quot;class&quot; &quot;3.141593&quot; str(my_mix_vector) #chr [1:4] &quot;my&quot; &quot;bioinformatics&quot; &quot;class&quot; &quot;3.141593&quot; class(my_mix_vector) #[1] &quot;character&quot; 4) A factor vector is similar to a character vector, but each unique element of this vector can be assigned a level. To do this, we use the function factor(). This factor vectors can be used in statistical analyses where discrete groups can be defined by a level. my_factor_vector &lt;- c(&quot;white&quot;, &quot;black&quot;, &quot;white&quot;, &quot;white&quot;, &quot;black&quot;) my_factor_vector &lt;- factor(my_factor_vector) my_factor_vector #[1] white black white white black #Levels: black white str(my_factor_vector) #Factor w/ 2 levels &quot;black&quot;,&quot;white&quot;: 2 1 2 2 1 class(my_factor_vector) #[1] &quot;factor&quot; You can also convert to a factor vector any numeric vector. my_factor_vector &lt;- c(1,0,1,1,1,0) my_factor_vector &lt;- factor(my_factor_vector) my_factor_vector #[1] 1 0 1 1 1 0 #Levels: 0 1 str(my_factor_vector) #Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 2 2 1 class(my_factor_vector) #[1] &quot;factor&quot; 5) A logical (Boolean) vector contains TRUE or FALSE values. These vectors and scalars are hugely important in any process that require some control flow during a set functions and calculations that require to define alternative processes. In other words, if the evaluation of a logical test is TRUE then the do some calculation to the result, but if FALSE do this other process. # a numeric vector with number from 1 to 10 my_numeric_vector &lt;- 1:10 my_numeric_vector #[1] 1 2 3 4 5 6 7 8 9 10 Then, we test this vector for the condition if each of its element is more than 5 using the function ifelse(). This function is very useful and it has three components: The first part is a logical test x &gt; 5 (i.e., if x more than 5 will be TRUE otherwise FALSE), a second part will provide the output for the x &gt; 5 test is TRUE (in this case assign the logical value of TRUE) and a third part will provide the output for the x &gt; 5 test is FALSE (in this case assign the logical value of FALSE). my_logical_vector &lt;- ifelse(my_numeric_vector &gt; 5, TRUE, FALSE) my_logical_vector # [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE str(my_logical_vector) #logi [1:10] FALSE FALSE FALSE FALSE FALSE TRUE ... class(my_logical_vector) #&quot;logical&quot; Here is an alternative for to get the same logical vector my_logical_vector &lt;- my_numeric_vector &gt; 5 my_logical_vector #[1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE 6) There are also special cases of vector elements that are useful, but they can also be confusing. A NULL represents the null or an empty object in R and it can be on its own, but it cannot be with other elements in the same vector. my_numeric_vector &lt;- c(1,2,3,4, NULL) my_numeric_vector #[1] 1 2 3 4 A NA element represents a missing value in R. This element can be in a vector and updated in other R objects. my_numeric_vector &lt;- c(1,2,3,4, NA) my_numeric_vector #[1] 1 2 3 4 NA 7) We can compare vectors or a vector against an scalar (i.e., an atomic quantity or object that can hold only one value at a time) using different logical operators and this will result in logical vector containing TRUE or FALSE values (also known as Boolean values). Note: Boolean values can serve as switches (ON/OFF) in conditional statements. my_numbers &lt;- 1:10 #[1] 1 2 3 4 5 6 7 8 9 10 The operator to test for equality == will determine if the values in the vector are equal to some value. my_numbers == 2 #[1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE The operator to test for inequality is !=. Notice ! which can we used in many logical function to negate (i.e., test for the opposite that function will try to determine as TRUE). my_numbers != 2 #[1] TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE The operator to test for less than &lt;. my_numbers &lt; 2 #[1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE The operator to test for less or equal than &lt;=. my_numbers &lt;= 2 #[1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE The operator to test for more than &gt;. my_numbers &gt; 2 #[1] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE The operator to test for more or equal than &gt;=. my_numbers &gt;= 2 #[1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE You can also test more complex vectors, as long as they have the same number of elements. a &lt;- c(2,10,4) # three elements b &lt;- 2:4 # three elements a == b # [1] TRUE FALSE TRUE 8) You can select specific elements of a vector by using their inherent index by its position on the set. # sequence of numbers between 1 and 10 every 2 numbers my_vector &lt;- seq(1,10,2) my_vector #[1] 1 3 5 7 9 if you want the third element of ‘my_vector’ then you use [3]. element_3 &lt;- my_vector[3] element_3 #[1] 5 9) If you want to delete an element using index, then use a minus - before the index that corresponds to the element to remove. my_vector_without_element_3 &lt;- my_vector[-3] my_vector_without_element_3 #[1] 1 3 7 9 10) We can also use a vector of indices to select multiple elements within a given vector (e.g., my_vector). my_vector[c(1,2,5)] # select elements 1, 2 and 5 #[1] 1 3 9 11) We can also use a logical operator to select elements that meet condition. # select elements that are more than 3 my_vector[my_vector &gt; 3] #[1] 5 7 9 12) Some other examples: Select even or odd elements from a vector. my_numbers #[1] 1 2 3 4 5 6 7 8 9 10 # Select even numbers my_numbers[my_numbers %% 2 == 0] #[1] 2 4 6 8 10 # Select numbers divisible by 3 my_numbers[my_numbers %% 3 == 0] #[1] 3 6 9 13) We can test if an element matches a set of terms using logical %in% that return TRUE if the left operand occurs in the right operand. my_names &lt;- c(&quot;juan&quot;, &quot;c&quot;, &quot;santos&quot;) name_key &lt;- c(&quot;peter&quot;, &quot;juan&quot;, &quot;randy&quot;, &quot;david&quot;, &quot;leeann&quot;) name_key %in% my_names #&gt; [1] FALSE TRUE FALSE FALSE FALSE name_key[name_key %in% my_names] #[1] &quot;juan&quot; If you want the opposite (i.e., return those that do not match key terms) then we add a ! (as we did above). name_key[!name_key %in% my_names] #[1] &quot;peter&quot; &quot;randy&quot; &quot;david&quot; &quot;leeann&quot; 14) Several standard arithmetic calculations with numeric vectors can also be done and include. my_numbers &lt;- 1:10 my_numbers #[1] 1 2 3 4 5 6 7 8 9 10 #Addition my_numbers + 1 #[1] 2 3 4 5 6 7 8 9 10 11 #Subtraction my_numbers - 1 #[1] 0 1 2 3 4 5 6 7 8 9 #Multiplication my_numbers * 2 #[1] 2 4 6 8 10 12 14 16 18 20 #Division my_numbers / 3 #[1] 0.3333333 0.6666667 1.0000000 1.3333333 1.6666667 2.0000000 2.3333333 2.6666667 3.0000000 3.3333333 #Exponentiation my_numbers ^ 2 #[1] 1 4 9 16 25 36 49 64 81 100 #Other functions – I did not add the corresponding results log(my_numbers) sqrt(my_numbers) sin(my_numbers) 15) You can append elements to your vector. # append 11 to vector my_numbers &lt;- c(my_numbers,11) my_numbers #[1] 1 2 3 4 5 6 7 8 9 10 11 16) You can repeat vectors or elements of vectors using the function rep()``. If you want to repeat the complete vector, for example, you specify the argument times. To repeat the vectorc(0, 0, 7)` for three times, use the following code. rep(c(0, 0, 7), times = 3) #[1] 0 0 7 0 0 7 0 0 7 You also can repeat every value by specifying the argument each, like this: rep(c(2, 4, 2), each = 3) #[1] 2 2 2 4 4 4 2 2 2 You can tell R for each value how often it has to be repeated. rep(c(0, 7), times = c(4,2)) #[1] 0 0 0 0 7 7 And you can, like in seq(), use the argument length.out to tell R how long you want it to be. R will repeat the vector until it reaches that length even if the last repetition is incomplete, like so: rep(1:3,length.out=7) #[1] 1 2 3 1 2 3 1 3.2 Matrices 17) A matrix object is defined by elements in rows and columns (i.e., n x m). These behave like vectors with dimensions, but you might need to use matrix algebra for their implementation. my_matrix &lt;- matrix(data = c(1,2,3,11,12,13), nrow = 2, ncol = 3, byrow = TRUE) my_matrix # [,1] [,2] [,3] #[1,] 1 2 3 #[2,] 11 12 13 str(my_matrix) #num [1:2, 1:3] 1 11 2 12 3 13 class(my_matrix) #[1] &quot;matrix&quot; my_matrix + 1 # [,1] [,2] [,3] #[1,] 2 3 4 #[2,] 12 13 14 my_matrix * 2 # [,1] [,2] [,3] #[1,] 2 4 6 #[2,] 22 24 26 To do arithmetic functions between matrices they should follow rules of matrix algebra. We will not use these data structures that much. 3.3 Arrays 18) These objects are similar to a matrix, but it can store data in more than 2 dimensions (e.g., x * y * z ). In array objects, you can store several matrices in cube-like organization. For example, an array with dimensions (2, 3, 5) will include 5 rectangular matrices each with 2 rows and 3 columns. These objects are created with the function array() and takes vectors as input and uses the values in the argument dim to create the 3D structure of the array. I do not use these objects much in work, so I am less familiar with their general utilization. Nevertheless, here is an example. We will create an array of 5 matrices 2x3 matrices. In other words, each of these matrices has 2 rows and 3 columns, and 5 of them are stacked the cube-like structure of the array object. # Five input vectors. vector1 &lt;- c(1,2,3) vector2 &lt;- c(4,5,6,7) vector3 &lt;- c(8,9,10,11,12) vector4 &lt;- c(13,14,15,16,17) vector5 &lt;- c(18,19,20,21,22) # Take these vectors as input to the array. my_array &lt;- array(c(vector1,vector2,vector3,vector4,vector5),dim = c(2,3,5)) my_array #, , 1 # # [,1] [,2] [,3] #[1,] 1 3 5 #[2,] 2 4 6 # #, , 2 # # [,1] [,2] [,3] #[1,] 7 9 11 #[2,] 8 10 12 # #, , 3 # # [,1] [,2] [,3] #[1,] 13 15 17 #[2,] 14 16 18 # #, , 4 # # [,1] [,2] [,3] #[1,] 19 21 1 #[2,] 20 22 2 # #, , 5 # # [,1] [,2] [,3] #[1,] 3 5 7 #[2,] 4 6 8 str(my_array) #num [1:2, 1:3, 1:5] 1 2 3 4 5 6 7 8 9 10 ... class(my_array) #[1] &quot;array&quot; ## Print the second row (2) of the fifth matrix (5) of the array my_array[2,,5] #[1] 4 6 8 You can do calculations with these array elements. ## add the matrices 2 and 5 my_array[,,2] + my_array[,,5] # [,1] [,2] [,3] #[1,] 10 14 18 #[2,] 12 16 20 ## multiply matrices 2 and 5 my_array[,,2] * my_array[,,5] # [,1] [,2] [,3] #[1,] 21 45 77 #[2,] 32 60 96 ## Use apply to calculate the sum of the rows, which is indicated by c(1), across all the matrices. It will be vector with two elements (i.e., the sums of each of the rows of matrices in the array). apply(my_array, c(1), sum) #[1] 137 152 ## Use apply to calculate the sum of the columns, which is indicated by c(2), across all the matrices. It will be a vector with three elements because each matrix has three columns. apply(my_array, c(2), sum) #[1] 91 111 87 3.4 Data Frames 19) A data frame object is an extremely flexible object in the R environment. Most packages, functions and applications use data frames as object to store, search, transform and filter data. A data frame is similar to a matrix (or an Excel worksheet), but also behaves like a list and a table. Long and Teetor (2019) provides some useful characteristics of a data frame. To an R programmer: A data frame is a hybrid data structure, part matrix and part list. A column can contain numbers, character strings, or factors, but not a mix of them. You can index the data frame just like you index a matrix. The data frame is also a list, where the list elements are the columns, so you can access columns by using list operators. Therefore, the data frame flexibility derive from some of its properties: The elements (cells) of data frame can be usually numeric, character or factor. The columns and row can have names. You can call rows and columns to vectors. You can filter and subset the data frame based on rules, functions conditions. You can store results of calculations in elements (cells) of a data frame. You can append new columns and rows to a data frame. You can coerce other data structures into data frames (e.g., collect vectors into a data frame, transform from a matrix). More complex data structures (e.g., tibbles, data.tables, biostrings objects) as usually modifications of data frames and can be also be coerced to data frames. Most data (e.g., CSV or tab delimited data sets) are imported into R as data frames. 20) You can build a data frame as follows by typing its contents in the R console (or copy and paste from text editor). my_dataframe &lt;- data.frame(my_numbers = c(1,2,3,4,5), my_text = c(&quot;apple&quot;, &quot;pear&quot;, &quot;grape&quot;, &quot;pomegranate&quot;, &quot;banana&quot;), my_boroughs = c(&quot;queens&quot;, &quot;the_bronx&quot;, &quot;brooklyn&quot;, &quot;manhattan&quot;, &quot;staten_island&quot;), stringsAsFactors = FALSE) my_dataframe # my_numbers my_text my_boroughs #1 1 apple queens #2 2 pear the_bronx #3 3 grape brooklyn #4 4 pomegranate manhattan #5 5 banana staten_island str(my_dataframe) #&#39;data.frame&#39;: 5 obs. of 3 variables: #$ my_numbers : num 1 2 3 4 5 #$ my_text : chr &quot;apple&quot; &quot;pear&quot; &quot;grape&quot; &quot;pomegranate&quot; ... #$ my_boroughs: chr &quot;queens&quot; &quot;the_bronx&quot; &quot;brooklyn&quot; &quot;manhattan&quot; ... class(my_dataframe) #[1] &quot;data.frame&quot; Notice the stringsAsFactors = FALSE argument while constructing (typing) the data.frame. This is to avoid R assigning variables (columns) as factors. 21) You can obtain the dimension of your data frame (i.e., number of columns and row) with the function dim(). # the object &#39;my_dataframe&#39; has 5 rows and 3 columns dim(my_dataframe) #[1] 5 3 22) You can also import txt files (comma delimited *.csv files or tab delimited *.txt files) as data frames. More on this the section how to import data. ## NOTE: remember to update the path to file with your dataset in your computer -- THIS IS EXCLUSIVE TO YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW my_dataframe &lt;- read.table(file = &quot;~/Desktop/Teach_R/my_dataframe_csv.csv&quot;, header = TRUE, sep = &quot;,&quot;, stringsAsFactors = FALSE) my_dataframe &lt;- read.table(file = &quot;~/Desktop/Teach_R/my_dataframe_tab.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, stringsAsFactors = FALSE) Notice that argument sep = has either a , or \\t and this indicates to separate columns using commas or a tabs, respectively. 23) As mentioned, data frames are extremely flexible and you can call columns, rows or specific elements. This returns a vector of the column named my_ boroughs. my_dataframe$my_boroughs #[1] &quot;queens&quot; &quot;the_bronx&quot; &quot;brooklyn&quot; &quot;manhattan&quot; &quot;staten_island&quot; Same result by indicating the corresponding column that has my_ boroughs. my_dataframe[,3] #[1] &quot;queens&quot; &quot;the_bronx&quot; &quot;brooklyn&quot; &quot;manhattan&quot; &quot;staten_island&quot; Same result. my_dataframe[,&quot;my_boroughs&quot;] #[1] &quot;queens&quot; &quot;the_bronx&quot; &quot;brooklyn&quot; &quot;manhattan&quot; &quot;staten_island&quot; This returns a subset data frame with the fourth row. my_dataframe[4,] # my_numbers my_text my_boroughs #4 4 pomegranate manhattan This returns an element on the first row and third column. my_dataframe[1,3] #[1] &quot;queens&quot; 24) Sub setting your data frame by a condition is usually one of the most common function applied to data frames. These conditions are used to filter or extract data from a the data frame. This returns a data frame for the column named my_boroughs. subset(my_dataframe, select = my_boroughs) # my_boroughs #1 queens #2 the_bronx #3 brooklyn #4 manhattan #5 staten_island This returns a subset data frame if the numbers of column my_numbers are more or equal to 3. subset(my_dataframe, my_numbers &gt;= 3) # my_numbers my_text my_boroughs #3 3 grape brooklyn #4 4 pomegranate manhattan #5 5 banana staten_island This returns a subset data frame if the column my_text contains banana. subset(my_dataframe, my_text %in% &quot;banana&quot;) # my_numbers my_text my_boroughs #5 5 banana staten_island This returns a subset data frame if the column my_boroughs has the text _island. Notice that this is a special case application of the function grepl() where a text pattern _island is searched in the text (words) elements (even if this not the full word) that is contained in the column my_boroughs. subset(my_dataframe, grepl(pattern = &quot;_island&quot;, my_dataframe$my_boroughs)) # my_numbers my_text my_boroughs #5 5 banana staten_island 25) You can append vectors as columns (e.g., variables) to a data frame as long as it has the same number of elements per column (i.e., same number of rows). my_dataframe$my_colors &lt;- c(&quot;red&quot;,&quot;orange&quot;,&quot;blue&quot;,&quot;black&quot;,&quot;green&quot;) my_dataframe # my_numbers my_text my_boroughs my_colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green 26) You can append rows to a data frame as long as it is another data frame with the same column names. not_NYC &lt;- data.frame(my_numbers = 6, my_text = &quot;potato&quot;, my_boroughs = &quot;not_NYC&quot;, my_colors = &quot;purple&quot;, stringsAsFactors = FALSE) my_dataframe_2 &lt;- rbind(my_dataframe,not_NYC) my_dataframe_2 # my_numbers my_text my_boroughs my_colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green #6 6 potato not_NYC purple 27) You can remove columns and rows from a data frame in similar fashion as we did for vectors. This will remove last row (i.e., row 6) of my_dataframe_2. my_dataframe_2[-6,] # my_numbers my_text my_boroughs my_colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green This will remove second column (i.e., column 2) of my_dataframe_2. my_dataframe_2[,-2] # my_numbers my_boroughs my_colors #1 1 queens red #2 2 the_bronx orange #3 3 brooklyn blue #4 4 manhattan black #5 5 staten_island green #6 6 not_NYC purple 28) You can also change the names of the columns on a data frame easily using the function names(). my_dataframe_for_names &lt;- my_dataframe #get a vector of names of columns names(my_dataframe_for_names) #[1] &quot;my_numbers&quot; &quot;my_text&quot; &quot;my_boroughs&quot; &quot;my_colors&quot; # to change names, you provide a vector with new names names(my_dataframe_for_names) &lt;- c(&quot;numbers&quot;, &quot;fruits&quot;, &quot;boroughs&quot;, &quot;colors&quot;) my_dataframe_for_names # numbers fruits boroughs colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green You can also change one column name, but you have to indicate where in the order of the list of names of the column. names(my_dataframe_for_names)[3] &lt;- &quot;the_boroughs&quot; my_dataframe_for_names # numbers fruits the_boroughs colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green 29) You can remove NA elements of a data frame, but it will remove the row and column that has such values. Let’s create a data frame that has NA elements. my_dataframe_with_NAs &lt;- data.frame (numbers_A = c(1,2,3,4), numbers_B = c(5,NA,7,NA), numbers_C = c(9, NA, 11,12), stringsAsFactors = FALSE) my_dataframe_with_NAs # numbers_A numbers_B numbers_C #1 1 5 9 #2 2 NA NA #3 3 7 11 #4 4 NA 12 We can remove all columns and rows with NA elements using the function na.omit(). na.omit(my_dataframe_with_NAs) # numbers_A numbers_B numbers_C #1 1 5 9 #3 3 7 11 We can also remove the row in that have NA values in column numbers_C using the function complete.cases(). my_dataframe_with_NAs[complete.cases(my_dataframe_with_NAs[,&quot;numbers_C&quot;]),] # numbers_A numbers_B numbers_C #1 1 5 9 #3 3 7 11 #4 4 NA 12 30) You can get an overview of your data frame with the following functions. You can get its structure with str(). The object mtcars is a build in data set for ‘Motor Trend Car Road Tests’ that is usually used to illustrate examples in R. my_long_dataframe &lt;- mtcars str(my_long_dataframe) #&#39;data.frame&#39;: 32 obs. of 11 variables: #$ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... #$ cyl : num 6 6 4 6 8 6 8 4 4 6 ... #$ disp: num 160 160 108 258 360 ... #$ hp : num 110 110 93 110 175 105 245 62 95 123 ... #$ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... #$ wt : num 2.62 2.88 2.32 3.21 3.44 ... #$ qsec: num 16.5 17 18.6 19.4 17 ... #$ vs : num 0 0 1 1 0 1 0 1 1 1 ... #$ am : num 1 1 1 0 0 0 0 0 0 0 ... #$ gear: num 4 4 4 3 3 3 3 4 4 4 ... #$ carb: num 4 4 1 1 2 1 4 2 2 4 ... See the first rows with head(). head(my_long_dataframe) # mpg cyl disp hp drat wt qsec vs am gear carb #Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 #Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 #Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 #Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 #Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 See the last rows with tail(). tail(my_long_dataframe) # mpg cyl disp hp drat wt qsec vs am gear carb #Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 #Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 #Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.5 0 1 5 4 #Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.5 0 1 5 6 #Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.6 0 1 5 8 #Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.6 1 1 4 2 You can see the size and dimensions of data frame with dim(). #returns a 2-element vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object) dim(my_long_dataframe) #[1] 32 11 You can see the number of rows with nrow(). nrow(my_long_dataframe) #[1] 32 You can see the number of columns with ncol(). ncol(my_long_dataframe) #[1] 11 31) You can get an summary of data frame that includes summary statistics about each numeric column (min, max, median, mean, etc.) of the data frame. # names of columns names(my_long_dataframe) #[1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; &quot;carb&quot; summary(my_long_dataframe) # mpg cyl disp hp drat wt qsec vs #Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 #1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 #Median :19.20 Median :6.000 Median :196.3 Median :123.0 Median :3.695 Median :3.325 Median :17.71 Median :0.0000 #Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 #3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 #Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 # am gear carb #Min. :0.0000 Min. :3.000 Min. :1.000 #1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 #Median :0.0000 Median :4.000 Median :2.000 #Mean :0.4062 Mean :3.688 Mean :2.812 #3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 #Max. :1.0000 Max. :5.000 Max. :8.000 3.5 Tables 32) A table object is similar to a data frame, but it can be used to summarizing your data, especially with categorical variables. Usually, you can use a table object to summarize a data frame. A contingency table is creates using the function table() to create an object for tabulation of counts and percentages for one or more variables. Missing values NA are excluded from the counts unless you specify the argument useNA=“ifany” or useNA=“always”. To exemplify a table, we create a version with different number of rows of my_dataframe and append those together. my_dataframe_3 &lt;- rbind(my_dataframe,my_dataframe_2, my_dataframe_2[-1,]) Now, we can tabulate values on my_dataframe_3. For example, a table for number of times each borough is repeated. table(my_dataframe_3$my_boroughs) # brooklyn manhattan not_NYC queens staten_island the_bronx # 3 3 2 2 3 3 A table for number of times the each my_text element is repeated. table(my_dataframe_3$my_text) # apple banana grape pear pomegranate potato # 2 3 3 3 3 2 A two-way table for my_borough and my_text. table(my_dataframe_3$my_boroughs, my_dataframe_3$my_text) # apple banana grape pear pomegranate potato # brooklyn 0 0 3 0 0 0 # manhattan 0 0 0 0 3 0 # not_NYC 0 0 0 0 0 2 # queens 2 0 0 0 0 0 # staten_island 0 3 0 0 0 0 # the_bronx 0 0 0 3 0 0 33) When dealing with NA values, tables can include or exclude these in tabulations. We can create a data frame to append with NA. my_dataframe_4 &lt;- data.frame(my_numbers = c(6,NA), my_text = c(NA,&quot;potato&quot;), my_boroughs = c(&quot;not_NYC&quot;,NA), my_colors = c(NA,&quot;purple&quot;), stringsAsFactors = FALSE) my_dataframe_5 &lt;- rbind(my_dataframe_3,my_dataframe_4) As indicated, we tabulated this my_dataframe_5. This will by default excluding NA values. table(my_dataframe_5$my_boroughs, my_dataframe_5$my_text) # apple banana grape pear pomegranate potato # brooklyn 0 0 3 0 0 0 # manhattan 0 0 0 0 3 0 # not_NYC 0 0 0 0 0 2 # queens 2 0 0 0 0 0 # staten_island 0 3 0 0 0 0 # the_bronx 0 0 0 3 0 0 However, we can modify this to tabulated NA values with the argument useNA = \"always\". table(my_dataframe_5$my_boroughs, my_dataframe_5$my_text, useNA = &quot;always&quot;) # apple banana grape pear pomegranate potato &lt;NA&gt; # brooklyn 0 0 3 0 0 0 0 # manhattan 0 0 0 0 3 0 0 # not_NYC 0 0 0 0 0 2 1 # queens 2 0 0 0 0 0 0 # staten_island 0 3 0 0 0 0 0 # the_bronx 0 0 0 3 0 0 0 # &lt;NA&gt; 0 0 0 0 0 1 0 34) We can label table columns using the function dimnames(). table_subset &lt;- table(my_dataframe_5$my_boroughs, my_dataframe_5$my_text) dimnames(table_subset) #[[1]] #[1] &quot;brooklyn&quot; &quot;manhattan&quot; &quot;not_NYC&quot; &quot;queens&quot; &quot;staten_island&quot; &quot;the_bronx&quot; #[[2]] #[1] &quot;apple&quot; &quot;banana&quot; &quot;grape&quot; &quot;pear&quot; &quot;pomegranate&quot; &quot;potato&quot; Labels are unnamed (i.e., no name associated to [[1]] or [[2]]). However, we can name them accordingly. names(dimnames(table_subset)) &lt;- c(&quot;my_boroughs&quot;, &quot;my_text&quot;) table_subset # my_text #my_boroughs apple banana grape pear pomegranate potato # brooklyn 0 0 3 0 0 0 # manhattan 0 0 0 0 3 0 # not_NYC 0 0 0 0 0 2 # queens 2 0 0 0 0 0 # staten_island 0 3 0 0 0 0 # the_bronx 0 0 0 3 0 0 35) You can also get proportions or percentages for the tabulations in your table object with the function prop.table(). prop.table(table_subset) # my_text #my_boroughs apple banana grape pear pomegranate potato # brooklyn 0.0000 0.0000 0.1875 0.0000 0.0000 0.0000 # manhattan 0.0000 0.0000 0.0000 0.0000 0.1875 0.0000 # not_NYC 0.0000 0.0000 0.0000 0.0000 0.0000 0.1250 # queens 0.1250 0.0000 0.0000 0.0000 0.0000 0.0000 # staten_island 0.0000 0.1875 0.0000 0.0000 0.0000 0.0000 # the_bronx 0.0000 0.0000 0.0000 0.1875 0.0000 0.0000 3.6 Lists 36) A list object is also a very flexible data structure in the R environment. You can use this to collect pretty much any of the previous objects (e.g., scalars, vectors, matrices, data frames, functions). To build a list we use the function list(). # We can put a collection of diverse vector my_list &lt;- list(c(1,2,3), my_dataframe, table_subset) my_list #[[1]] #[1] 1 2 3 #[[2]] # my_numbers my_text my_boroughs my_colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green #[[3]] # my_text #my_boroughs apple banana grape pear pomegranate potato # brooklyn 0 0 3 0 0 0 # manhattan 0 0 0 0 3 0 # not_NYC 0 0 0 0 0 2 # queens 2 0 0 0 0 0 # staten_island 0 3 0 0 0 0 # the_bronx 0 0 0 3 0 0 37) You can also create an empty list is useful to fill with elements as desired by the user. my_list_to_fill &lt;- list() my_list_to_fill #list() my_list_to_fill[[1]] &lt;- &quot;one&quot; my_list_to_fill[[2]] &lt;- 1:10 my_list_to_fill[[3]] &lt;- mean(my_list_to_fill[[2]]) # mean of an already an element of the list my_list_to_fill #[[1]] #[1] &quot;one&quot; #[[2]] #[1] 1 2 3 4 5 6 7 8 9 10 #[[3]] #[1] 5.5 38) Names can be assigned to list elements and also can select those by using the corresponding name assigned. my_list &lt;- list(some_numbers = c(1,2,3), a_data_frame = my_dataframe) my_list #$some_numbers #[1] 1 2 3 #$a_data_frame # my_numbers my_text my_boroughs my_colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green my_list[[&quot;a_data_frame&quot;]] # my_numbers my_text my_boroughs my_colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green 39) You can pull elements of a list by using its indices, and even an specific element within a list element. #fist element of list my_list[[1]] #[1] 1 2 3 #get the third element of the first list my_list[[1]][3] #[1] 3 40) Removing elements from a list can be done by assigning NULL to the selected element my_list #$some_numbers #[1] 1 2 3 #$a_data_frame # my_numbers my_text my_boroughs my_colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green my_list[[2]] &lt;- NULL my_list #$some_numbers #[1] 1 2 3 41) You can also flatten a list object (i.e., unlist or transform) into a vector by using the functionunlist(). my_list &lt;- list(some_numbers = c(1,2,3), a_data_frame = my_dataframe) my_list #$some_numbers #[1] 1 2 3 #$a_data_frame # my_numbers my_text my_boroughs my_colors #1 1 apple queens red #2 2 pear the_bronx orange #3 3 grape brooklyn blue #4 4 pomegranate manhattan black #5 5 banana staten_island green my_flatten_list &lt;- unlist(my_list) my_flatten_list # some_numbers1 some_numbers2 some_numbers3 a_data_frame.my_numbers1 a_data_frame.my_numbers2 # &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;1&quot; &quot;2&quot; # a_data_frame.my_numbers3 a_data_frame.my_numbers4 a_data_frame.my_numbers5 a_data_frame.my_text1 a_data_frame.my_text2 # &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;apple&quot; &quot;pear&quot; # a_data_frame.my_text3 a_data_frame.my_text4 a_data_frame.my_text5 a_data_frame.my_boroughs1 a_data_frame.my_boroughs2 # &quot;grape&quot; &quot;pomegranate&quot; &quot;banana&quot; &quot;queens&quot; &quot;the_bronx&quot; #a_data_frame.my_boroughs3 a_data_frame.my_boroughs4 a_data_frame.my_boroughs5 a_data_frame.my_colors1 a_data_frame.my_colors2 # &quot;brooklyn&quot; &quot;manhattan&quot; &quot;staten_island&quot; &quot;red&quot; &quot;orange&quot; # a_data_frame.my_colors3 a_data_frame.my_colors4 a_data_frame.my_colors5 # &quot;blue&quot; &quot;black&quot; &quot;green&quot; Next, we will overview special data structures relevant for extremely long tables and bioinformatics (nucleotide/amino acid sequences). These data structures are usually developed and managed within specific R-packages. 3.7 Data Tables 42) A data.table is an improved version of a data frame and usually faster to analyze and filter extremely large datasets. To access this data structure requires to install or load R-package data.table. For details more, you can see this vignette. # if you do not have installed this R-package install.packages(&quot;data.table&quot;) library(data.table) input_csv_file &lt;- &quot;https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv&quot; flights &lt;- fread(input_csv_file) flights # year month day dep_delay arr_delay carrier origin dest air_time distance hour # 1: 2014 1 1 14 13 AA JFK LAX 359 2475 9 # 2: 2014 1 1 -3 13 AA JFK LAX 363 2475 11 # 3: 2014 1 1 2 9 AA JFK LAX 351 2475 19 # 4: 2014 1 1 -8 -26 AA LGA PBI 157 1035 7 # 5: 2014 1 1 2 1 AA JFK LAX 350 2475 13 # --- #253312: 2014 10 31 1 -30 UA LGA IAH 201 1416 14 #253313: 2014 10 31 -5 -14 UA EWR IAH 189 1400 8 #253314: 2014 10 31 -8 16 MQ LGA RDU 83 431 11 #253315: 2014 10 31 -4 15 MQ LGA DTW 75 502 11 #253316: 2014 10 31 -5 1 MQ LGA SDF 110 659 8 str(flights) #Classes ‘data.table’ and &#39;data.frame&#39;: 253316 obs. of 11 variables: #$ year : int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... #$ month : int 1 1 1 1 1 1 1 1 1 1 ... #$ day : int 1 1 1 1 1 1 1 1 1 1 ... #$ dep_delay: int 14 -3 2 -8 2 4 -2 -3 -1 -2 ... #$ arr_delay: int 13 13 9 -26 1 0 -18 -14 -17 -14 ... #$ carrier : chr &quot;AA&quot; &quot;AA&quot; &quot;AA&quot; &quot;AA&quot; ... #$ origin : chr &quot;JFK&quot; &quot;JFK&quot; &quot;JFK&quot; &quot;LGA&quot; ... #$ dest : chr &quot;LAX&quot; &quot;LAX&quot; &quot;LAX&quot; &quot;PBI&quot; ... #$ air_time : int 359 363 351 157 350 339 338 356 161 349 ... #$ distance : int 2475 2475 2475 1035 2475 2454 2475 2475 1089 2422 ... #$ hour : int 9 11 19 7 13 18 21 15 15 18 ... #- attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; class(flights) #[1] &quot;data.table&quot; &quot;data.frame&quot; nrow(flights) # get number of rows of data.table #[1] 253316 3.8 Tibbles 43) A tibble is also a modified form of a data frame used in the tidyverse family of R-packages. They are a more simplified form of data frames and used by some data management R-packages, e.g., readr, where they parse (i.e., import data or read data from a file) into the R environment. I am not a favorite of tibbles as it forces user to orbit around the ‘tidyverse’ for most things that a data frame can handle. # install or load R-package &#39;readr&#39; which is a powerful data parser for large text (tab or csv) files install.packages(&#39;readr&#39;) library(readr) reader_file_path &lt;- readr_example(&quot;mtcars.csv&quot;) # this a common long dataset of car specifications my_tibble &lt;- read_delim(file = reader_file_path, delim = &quot;,&quot;) #Parsed with column specification: #cols( # mpg = col_double(), # cyl = col_double(), # disp = col_double(), # hp = col_double(), # drat = col_double(), # wt = col_double(), # qsec = col_double(), # vs = col_double(), # am = col_double(), # gear = col_double(), # carb = col_double() #) my_tibble # A tibble: 32 x 11 # mpg cyl disp hp drat wt qsec vs am gear carb # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 # 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 # 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 # 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 # 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 # 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 # 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 # 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 # 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 #10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 # … with 22 more rows 3.9 DNAStringSets, RNAStringSets and AAStringSets 44) A XStringSet object is a special and flexible type of data frame like that contains sequences with different flavor: DNAStringSet, RNAStringSet or AAStringSet. To access this data structure requires to install or load R-package Biostrings. For details more, you can see the associated vignettes in that R-package site. We will return to this object when explore nucleotide and amino acid sequence manipulation. For this introduction, we also need to install another powerful R-package rentrez. This package provides an R interface to the NCBIs EUtils API, allowing users to search databases like GenBank and PubMed, process the results of those searches and pull data into their R sessions. # if you need to install R-packages &#39;Biostrings&#39; and &#39;rentrez&#39; if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) # remember that you might not need to update all other packages (type n) BiocManager::install(&quot;Biostrings&quot;) # if you need to install “rentrez” install.packages(&quot;rentrez&quot;) # load these libraries library(Biostrings) library(rentrez) For this example, we will download some nucleotide sequences from NCBI from a charismatic frog, Allobates kingsburyi. froggy_name &lt;- &quot;Allobates kingsburyi[Organism]&quot; froggy_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term=froggy_name) # revising the structure of &#39;froggy_seq_IDs&#39; that there are 17 sequences in NCBI nuccore database str(froggy_seq_IDs) #List of 5 #$ ids : chr [1:17] &quot;1845966712&quot; &quot;1248341807&quot; &quot;328728168&quot; &quot;328728030&quot; ... #$ count : int 17 #$ retmax : int 17 #$ QueryTranslation: chr &quot;\\&quot;Allobates kingsburyi\\&quot;[Organism]&quot; #$ file :Classes &#39;XMLInternalDocument&#39;, &#39;XMLAbstractDocument&#39; &lt;externalptr&gt; #- attr(*, &quot;class&quot;)= chr [1:2] &quot;esearch&quot; &quot;list&quot; froggy_seqs_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=froggy_seq_IDs$ids, rettype=&quot;fasta&quot;) froggy_seqs_fasta #[1] &quot;&gt;MT524123.1 Allobates kingsburyi voucher QCAZA68477 large subunit ribosomal RNA gene, partial sequence; #mitochondrial\\nCCTGATTAACCATAAGAGGTCAAGCCTGCCCAGTGACATTTGTTTAACGGCCGCGGTATCCTAACCGTGC\\nGAAGGTAGCGTAATCACTTGTCCTT #TAAATGAGGACTAGTATGAACGGCTTCACGAAGGCTATGCTGTCT\\nCCTTTATCTAATCAGTTAAACTAATCTCCCCGTGAAGAAGCGGGGATACACCTATAAGACGAGAA #... cat(froggy_seqs_fasta) #&gt;MT524123.1 Allobates kingsburyi voucher QCAZA68477 large subunit ribosomal RNA gene, partial sequence; mitochondrial #CCTGATTAACCATAAGAGGTCAAGCCTGCCCAGTGACATTTGTTTAACGGCCGCGGTATCCTAACCGTGC #GAAGGTAGCGTAATCACTTGTCCTTTAAATGAGGACTAGTATGAACGGCTTCACGAAGGCTATGCTGTCT #CCTTTATCTAATCAGTTAAACTAATCTCCCCGTGAAGAAGCGGGGATACACCTATAAGACGAGAAGACCC #TATGGAGCTTTAAATACTTTAAAACACCTGAATCTGACACTAGAAACTTCCAGAAAACTTTATTTAACAT #ATCACTTTGTTTTAAACTTTAGGTTGGGGTGACCACGGAGAAAAAACCAACCTCCACGTAGAATGAAATT #TTCTTTCTAAGCGATAAGCTACATCTTTATGCATCAATACATTGACCTAAATTGACCCAATTTTTTGATC #AACGAAC # #&gt;MF580102.1 Allobates kingsburyi nicotinic acetylcholine receptor beta-2 (chrnb2) gene, partial cds #ATGACGGTTCTCCTCCTCCTCCTGCACCTCAGCCTGTTCGGCCTGGTCACCAGGAGTATGGGCACGGACA #CCGAGGAGCGGCTCGTGGAATTCCTGCTGGACCCGTCCCAGTACAACAAGCTGATCCGGCCCGCCACCAA #TGGATCCGAGCAGGTCACCGTCCAGCTGATGGTATCTCTGGCCCAGCTGATCAGCGTGCACGAGCGGGAG #... We can save these sequences in a text file in your working directory. # this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory&quot;) write(froggy_seqs_fasta, &quot;my_froggy_seqs_fasta.txt&quot;) Now we can import the nucleotide sequences from this file that are in fasta format into R. my_Biostrings_set &lt;- readDNAStringSet(filepath = &quot;~/Desktop/Teach_R/my_working_directory/my_froggy_seqs_fasta.txt&quot;, format = &quot;fasta&quot;) my_Biostrings_set #A DNAStringSet instance of length 17 # width seq names # [1] 427 CCTGATTAACCATAAGAGGTCAAGCCTGCCCAGTGACATTTGTTTAACGGC...TATGCATCAATACATTGACCTAAATTGACCCAATTTTTTGATCAACGAAC MT524123.1 Alloba... # [2] 899 ATGACGGTTCTCCTCCTCCTCCTGCACCTCAGCCTGTTCGGCCTGGTCACC...CCCCCGACGTCCCTGGACGTCCCGCTCGTCGGCAAGTACCTGATGTTCAC MF580102.1 Alloba... # [3] 4881 AAGGTTTGGTCCTAGCCTTGAAGTCAGTTACTAATTAATATACACATGCAA...CCTTTGTTTACTTCCTATCTCCCCATCCCTTCTCTGCCTGCTCAGAAACT HQ290963.1 Alloba... # [4] 576 GTACATCATAATGTGAGCAGATGGGAAAGCTTTGATGTCACACCAGCTATT...TCTACCTTGATGAAAATGAAAAAGTTGTTTTGAAAAACTATCAAGACATG HQ291024.1 Alloba... # [5] 510 AACTCCCCTTCAGGTTCACAATTTCCCTTCAGCGGCATTGACGACCGGGAA...TGCTTCAATGGGAGCATGAAATTCAGAAGCTCACGGGTGACGAGAACTTC HQ290901.1 Alloba... #... ... ... #[13] 2389 AGGCTTGGTCCTAACCTTGAAGTCAGTTACTAATTAATATACACATGCAAG...CGACCTCGATGTTGGATCAGGATGTCCCAGTGGTGCAGCAGCTACTAATG EU342528.1 Alloba... #[14] 2392 TAAAGGTTTGGTCCTAGCCTTGAAGTCAGTTACTAATTAATATACACATGC...CGACCTCGATGTTGGATCAGGATGTCCCAGTGGTGCAGCAGCTACTAATG EU342527.1 Alloba... #[15] 2393 TTAAAGGTTTGGTCCTAGCCTTGAAGTCAGTTACTAATTAATATACACATG...CGACCTCGATGTTGGATCAGGATGTCCCAGTGGTGCAGCAGCTACTAATG EU342526.1 Alloba... #[16] 2457 AAAGTTCTCCAACATAAAGGCTTGGTCCTAACCTTGAAGTCAGTTACTAAT...GTTCGTTTGTTCAACGATTAAAATCCTACGTGATCTGAGTTCAGACCGGA AY364550.1 Colost... #[17] 2446 ATTAAAGGTTTGGTCCTAGCCTTGAAGTCAGTTACTAATTAATATACACAT...TTCGTTTGTTCAACGATTAAAATCCTACGTGATCTGAGTTCAAGACCGGA AY364549.1 Colost... Next, we will download some amino acid sequences from NCBI from the same frog species, Allobates kingsburyi. froggy_name &lt;- &quot;Allobates kingsburyi[Organism]&quot; froggy_AA_IDs &lt;- entrez_search(db=&quot;protein&quot;, term=froggy_name) # Entrez search result with 11 hits (object contains 11 IDs and no web_history object) # Search term (as translated): &quot;Allobates kingsburyi&quot;[Organism] str(froggy_AA_IDs) #List of 5 #$ ids : chr [1:11] &quot;1248341808&quot; &quot;328728170&quot; &quot;328728169&quot; &quot;328728031&quot; ... #$ count : int 11 #$ retmax : int 11 #$ QueryTranslation: chr &quot;\\&quot;Allobates kingsburyi\\&quot;[Organism]&quot; #$ file :Classes &#39;XMLInternalDocument&#39;, &#39;XMLAbstractDocument&#39; &lt;externalptr&gt; #- attr(*, &quot;class&quot;)= chr [1:2] &quot;esearch&quot; &quot;list&quot; froggy_AA_fasta &lt;- entrez_fetch(db=&quot;protein&quot;, id=froggy_AA_IDs$ids, rettype=&quot;fasta&quot;) froggy_AA_fasta #[1] &quot;&gt;ATG31804.1 nicotinic acetylcholine receptor beta-2, partial [Allobates kingsburyi]\\nMTVLLLLLHLSLFGLV #TRSMGTDTEERLVEFLLDPSQYNKLIRPATNGSEQVTVQLMVSLAQLISVHERE\\nQIMTTNVWLTQEWXXXXXXXXXXXXXXXXXXXXXXXXXWLPDVVLYNNADGMY #EVSFYSNAVVSHDGSIF\\nWLPPAIYKSACKIEVKHFPFDQQNCTMKFRSWTYDRTELDLVLKSDVASLDDFTPSGEWDIIALPGRRNE\\nNPEDSTYVDITYDFIIRRKPL #... cat(froggy_AA_fasta) #&gt;ATG31804.1 nicotinic acetylcholine receptor beta-2, partial [Allobates kingsburyi] #MTVLLLLLHLSLFGLVTRSMGTDTEERLVEFLLDPSQYNKLIRPATNGSEQVTVQLMVSLAQLISVHERE #QIMTTNVWLTQEWXXXXXXXXXXXXXXXXXXXXXXXXXWLPDVVLYNNADGMYEVSFYSNAVVSHDGSIF #WLPPAIYKSACKIEVKHFPFDQQNCTMKFRSWTYDRTELDLVLKSDVASLDDFTPSGEWDIIALPGRRNE #NPEDSTYVDITYDFIIRRKPLFYTINLIIPCILITSLAILVFYLPSDCGEKMTLCISVLLALTVFLLLIS #KIVPPTSLDVPLVGKYLMFT # #&gt;AEB39272.1 NADH dehydrogenase subunit 2 (mitochondrion) [Allobates kingsburyi] #MNPYALFLIISSLALGTSIAVSSFHWILAWIGLEINTLAIIPLMTKNPHPRSIEAATKYFLTQAAASSLI #LFSCALNAWLLGEWTINNLMSPASMIFLSIALSTKLGLAPFHFWLPEVLQGLTLQTGWILSTWQKLAPLA #ILFQLSQSINLLLMMSMGLLSILVGGWGGINQNQIRKILAFSSIAHLGWMITILKISPQLSLLNFILYII #MTSALFYTFIMIDSTNISHLATTWTKIPTLTALSLMSLLSLSGLPPLTGFLPKWLIAQELINQNLIILPF #LMLMLTLLALFFYLRLTYTISLTMAPNSTSSVSLWYQKKKNNLTIFILLTLCLLPISPSLLCLL We can save these AA sequences in a text file in your working directory. # this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory&quot;) write(froggy_AA_fasta, &quot;my_froggy_AA_fasta.txt&quot;) Now we can import the amino acid sequences of this file that are in fasta format. my_AA_Biostrings_set &lt;- readAAStringSet(filepath = &quot;~/Desktop/Teach_R/my_working_directory/my_froggy_AA_fasta.txt&quot;, format = &quot;fasta&quot;) my_AA_Biostrings_set #A AAStringSet instance of length 11 # width seq names # [1] 300 MTVLLLLLHLSLFGLVTRSMGTDTEERLVEFLLDPSQYNKLIRPATNGSEQ...VFYLPSDCGEKMTLCISVLLALTVFLLLISKIVPPTSLDVPLVGKYLMFT ATG31804.1 nicoti... # [2] 344 MNPYALFLIISSLALGTSIAVSSFHWILAWIGLEINTLAIIPLMTKNPHPR...RLTYTISLTMAPNSTSSVSLWYQKKKNNLTIFILLTLCLLPISPSLLCLL AEB39272.1 NADH d... # [3] 320 LNIFTLTQSLCYMVPILLAVAFLTLLERKVLGYMQHRKGPNVIGPTGLLQP...LFLWVRASYPRFRYDQLMHLVWKNFLPMTLALTIWFITFPIIFLFSPPIL AEB39271.1 NADH d... # [4] 192 VHHNVSRWESFDVTPAIIRWIAHRQPNHGFVVEVTQLDCEKNVTKRHVRIS...STNHAIVQTLVNSVNSNIPKACCVPTELSAISMLYLDENEKVVLKNYQDM AEB39195.1 bone m... # [5] 170 NSPSGSQFPFSGIDDRENWPIVFYNRTCQCQGNFMGYNCGDCKFXFTGXNC...YASRDAFLEGDLVWQNIDFAHEAPAFLPWHRFFLLQWEHEIQKLTGDENF AEB39135.1 tyrosi... # ... ... ... # [7] 192 TTMDKRNLPESSMNSLFIKLMQADLLKNKIPKQVVNAKEIKQQSTIPKAEI...VTNKSNAIDIRGHQVAVLGEIKTGNSPVKQYFYETRCKDARPVKSGCRGI AEB39015.1 neurot... # [8] 414 TIKKPNGETTKTTVRIWNETVSNLTLMALGSSAPEILLSVIEVCGHNFQAG...GIIDDDIFEEDENFLVHLSNVRVNAETTEVNFESNHVTSLACLGSPSTAT AEB38955.1 sodium... # [9] 309 CIGLISVNGRMRNNMKAGSSPNSVSSSPTNSAITQLRHKLENGKPLGMNES...PIPLHQHERYLCKMNEEIKAVLQPSENLILNKQGMFAEKQALLLSSVLSE AEB38895.1 zinc f... #[10] 201 VRGQSGLAYPGLRTHGTLESIGGPMSSSRGGGLPSLTDTFEHVIEELLEEE...QLKQYFYETKCNPMGYMKEGCRGIDKRYWNSQCRTTQSYVRALTMDSKKK AEB38835.1 brain-... #[11] 230 GLCLIAQIITGLFLAMHYTADTTMAFSSIAHICRDVNNGWLLRSLHANGAS...VPFHAYFSYKDALGFIILLVLLSLLSLFSPNLLGDPDNFTPANPLVTPPH AEB33649.1 cytoch... "],["text-editing-and-data-transformations.html", "Session 4 – Text Editing and Data Transformations 4.1 Working with character strings 4.2 Regular expressions (Regex) 4.3 Comparing, ordering and simplifying vectors 4.4 Useful functions of stringr 4.5 Data transformations", " Session 4 – Text Editing and Data Transformations 4.1 Working with character strings 1) A character vector contains text (e.g., nucleotide or amino acid sequence) that is represented by a sequence of characters (e.g., A, T, C, G, U, Met). A character vector can also contain numbers that will be considered as text (i.e., not quantities). For example, we will download sequence of a COX1 mitochondrial gene of a charismatic poison frog Ameerega bilinguis. We will follow the example from previous sessions using the R-package rentrez. library(rentrez) # if you need to install R-package &#39;rentrez&#39;, use install.packages(&quot;rentrez&quot;) # Download some nucleotide sequences from NCBI for Ameerega bilinguis[Organism] that correspond to the gene COI froggy_name_COX1 &lt;- &quot;Ameerega bilinguis[Organism] AND COI[Gene]&quot; froggy_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term= froggy_name_COX1) #Entrez search result with 1 hits (object contains 1 IDs and no web_history object) # Search term (as translated): &quot;Ameerega bilinguis&quot;[Organism] AND COI[Gene] # revising the structure of &#39;froggy_seq_IDs&#39; that there are 1 sequence in NCBI nuccore database str(froggy_seq_IDs) #List of 5 # $ ids : chr &quot;1952638358&quot; # $ count : int 1 # $ retmax : int 1 # $ QueryTranslation: chr &quot;\\&quot;Ameerega bilinguis\\&quot;[Organism] AND COI[Gene]&quot; # $ file :Classes &#39;XMLInternalDocument&#39;, &#39;XMLAbstractDocument&#39; &lt;externalptr&gt; # - attr(*, &quot;class&quot;)= chr [1:2] &quot;esearch&quot; &quot;list&quot; froggy_seqs_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=froggy_seq_IDs$ids, rettype=&quot;fasta&quot;) froggy_seqs_fasta #[1] &quot;&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nGTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGG\\nCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAGCCGAATTAAGCCAGCCCGGGTCCTT\\nACTAGGCGATGACCAGATCTACAACGTTATTGTTACCGCCCATGCTTTCGTTATAATCTTTTTTATAGTA\\nATGCCAATTCTAATCGGTGGCTTTGGGAATTGATTAGTGCCCCTAATAATTGGAGCCCCAGACATAGCTT\\nTTCCCCGAATAAACAATATGAGCTTTTGGCTTCTTCCCCCCTCTTTCCTACTACTCCTAGCATCCGCAGG\\nCGTTGAAGCAGGCGCCGGTACTGGCTGAACTGTGTACCCTCCCCTTGCAGGCAACCTAGCTCATGCTGGC\\nCCATCAGTTGATTTAACTATTTTTTCACTTCATCTCGCCGGTGTTTCTTCTATTCTAGGGGCAATTAACT\\nTTATTACAACAACCTTAAACATAAAACCCCCTTCATTAACACAATATCAAACCCCATTATTTGTCTGATC\\nTGTATTAATTACTGCAGTCCTTCTTCTTCTCTCCCTCCCAGTTCTGGCTGCCGGAATCACTATACTCTTG\\nACTGACCGAAACCTAAACACCACCTTCTTTGACCCAGCAGGTGGAGGCGACCCTGTCCTGTACCAACACC\\nTGTTCTGATTCTTTGGTCACCCCGAAGTCTACATCCTTATCCTGCCTGGATTTGGTATCATCTCCCATGT\\nTGTCACATTCTACTCTAGCAAAAAAGAACCCTTCGGCTATATAGGAATAGTCTGAGCTATAATATCGATT\\nGGTCTCCTAGGTTTCATTGTTTGAGCTCACCACATATTCACAACAGACCTTAATGTAGACACTCGAGCCT\\nACTTTACCTCAGCTACTATAATCATCGCTATCCCAACAGGTGTCAAAGTCTTTAGCTGACTTGCCACCAT\\nGCACGGAGGAATTATTAAATGAGACGCCGCCATATTATGGGCTCTCGGATTCATCTTTTTATTTACAGTT\\nGGAGGACTAACTGGAATCGTTTTAGCCAACTCCTCTTTAGACATTGTTTTGCATGATACATATTATGTAG\\nTAGCCCACTTTCACTACGTTCTTTCTATGGGGGCAGTATTTGCCATTATAGCCGGCTTCGTACACTGATT\\nTCCTCTCTTTTCCGGATTTACCCTTCATGAAGCCTGAACAAAAATTCAATTTGGCGTCATATTTACCGGC\\nGTAAATTTAACATTCTTCCCCCAGCATTTCTTAGGTCTCGCAGGCATGCCTCGACGTTATTCAGACTACC\\nCTGACGCCTACACATTATGAAACACCGTTTCATCAATCGGCTCTTTAATCTCTCTAGTTGCAGTAATCAT\\nTATGATGTTTATCATTTGAGAAGCTTTCTCTTCCAAACGCCTACCTCTACCTGCAGAAATAACCCCAACT\\nAATGTAGAATGATTATACGGATCCCCCCCACCTTACCACACTTTTGAGGAAGCCGTTTACTCCAAAATT\\n\\n&quot; The froggy_seqs_fasta object is a character vector as indicated by the sequence of characters (i.e., letters and numbers) contained between quotes \" \". Likewise, we can identify that this vector contains a sequence in a fasta format. We will dissect this character vector. # 1) A fasta header starting with &#39;&gt;&#39;, the NCBI accession number &#39;MW042030.1&#39; # &gt;MW042030.1 # 2) A defition as is present in the GenBank format for this sequence &#39;Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial&#39; # Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial # 3) A new line coding indicated by &#39;\\n&#39; # \\n # 4) The actual nucleotide sequence in capital letters with some new line characters &#39;\\n&#39; interspersed in the sequence # GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGG\\nCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAGCCGAATTAAGCCAGCCCGGGTCCTT\\nACTAGGCGATGACCAGATCTACAACGTTATTGTTACCGCCCATGCTTTCGTTATAATCTTTTTTATAGTA\\nATGCCAATTCTAATCGGTGGCTTTGGGAATTGATTAGTGCCCCTAATAATTGGAGCCCCAGACATAGCTT\\nTTCCCCGAATAAACAATATGAGCTTTTGGCTTCTTCCCCCCTCTTTCCTACTACTCCTAGCATCCGCAGG\\nCGTTGAAGCAGGCGCCGGTACTGGCTGAACTGTGTACCCTCCCCTTGCAGGCAACCTAGCTCATGCTGGC\\nCCATCAGTTGATTTAACTATTTTTTCACTTCATCTCGCCGGTGTTTCTTCTATTCTAGGGGCAATTAACT\\nTTATTACAACAACCTTAAACATAAAACCCCCTTCATTAACACAATATCAAACCCCATTATTTGTCTGATC\\nTGTATTAATTACTGCAGTCCTTCTTCTTCTCTCCCTCCCAGTTCTGGCTGCCGGAATCACTATACTCTTG\\nACTGACCGAAACCTAAACACCACCTTCTTTGACCCAGCAGGTGGAGGCGACCCTGTCCTGTACCAACACC\\nTGTTCTGATTCTTTGGTCACCCCGAAGTCTACATCCTTATCCTGCCTGGATTTGGTATCATCTCCCATGT\\nTGTCACATTCTACTCTAGCAAAAAAGAACCCTTCGGCTATATAGGAATAGTCTGAGCTATAATATCGATT\\nGGTCTCCTAGGTTTCATTGTTTGAGCTCACCACATATTCACAACAGACCTTAATGTAGACACTCGAGCCT\\nACTTTACCTCAGCTACTATAATCATCGCTATCCCAACAGGTGTCAAAGTCTTTAGCTGACTTGCCACCAT\\nGCACGGAGGAATTATTAAATGAGACGCCGCCATATTATGGGCTCTCGGATTCATCTTTTTATTTACAGTT\\nGGAGGACTAACTGGAATCGTTTTAGCCAACTCCTCTTTAGACATTGTTTTGCATGATACATATTATGTAG\\nTAGCCCACTTTCACTACGTTCTTTCTATGGGGGCAGTATTTGCCATTATAGCCGGCTTCGTACACTGATT\\nTCCTCTCTTTTCCGGATTTACCCTTCATGAAGCCTGAACAAAAATTCAATTTGGCGTCATATTTACCGGC\\nGTAAATTTAACATTCTTCCCCCAGCATTTCTTAGGTCTCGCAGGCATGCCTCGACGTTATTCAGACTACC\\nCTGACGCCTACACATTATGAAACACCGTTTCATCAATCGGCTCTTTAATCTCTCTAGTTGCAGTAATCAT\\nTATGATGTTTATCATTTGAGAAGCTTTCTCTTCCAAACGCCTACCTCTACCTGCAGAAATAACCCCAACT\\nAATGTAGAATGATTATACGGATCCCCCCCACCTTACCACACTTTTGAGGAAGCCGTTTACTCCAAAATT # 5) Final a couple of new line &#39;\\n&#39; at the end of the vector # \\n\\n 2) We can explore the character vector froggy_seqs_fasta with several functions. We can check the length of this vector (i.e., how many elements this vector contains) with the function length(). It should just 1 element, the COX1 of Ameerega bilinguis. length(froggy_seqs_fasta) #[1] 1 We can explore other properties of this vector using the functions class() and str(). class(froggy_seqs_fasta) #[1] &quot;character&quot; str(froggy_seqs_fasta) #chr &quot;&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochon&quot;| __truncated__ We can also test if this vector is character vector using the function is.character(). The result is a logical TRUE or FALSE. We expect to be TRUE. is.character(froggy_seqs_fasta) #[1] TRUE We can also determine the actual number characters in this vector (i.e., the total number of letters, symbols, spaces, numbers) using the function nchar(). nchar(froggy_seqs_fasta) #[1] 1679 3) We see that the vector froggy_seqs_fasta has other text than the actual nucleotide sequence. We will extract this sequence using a combination of functions that work with character vectors. First, we can split this vector by the new line ‘’ using the function strsplit() and test what type of data structure we obtain using class(). froggy_seq_split &lt;- strsplit(froggy_seqs_fasta, split = &#39;\\n&#39;) froggy_seq_split #[[1]] # [1] &quot;&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial&quot; # [2] &quot;GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGG&quot; # [3] &quot;CATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAGCCGAATTAAGCCAGCCCGGGTCCTT&quot; # [4] &quot;ACTAGGCGATGACCAGATCTACAACGTTATTGTTACCGCCCATGCTTTCGTTATAATCTTTTTTATAGTA&quot; # [5] &quot;ATGCCAATTCTAATCGGTGGCTTTGGGAATTGATTAGTGCCCCTAATAATTGGAGCCCCAGACATAGCTT&quot; # [6] &quot;TTCCCCGAATAAACAATATGAGCTTTTGGCTTCTTCCCCCCTCTTTCCTACTACTCCTAGCATCCGCAGG&quot; # [7] &quot;CGTTGAAGCAGGCGCCGGTACTGGCTGAACTGTGTACCCTCCCCTTGCAGGCAACCTAGCTCATGCTGGC&quot; # [8] &quot;CCATCAGTTGATTTAACTATTTTTTCACTTCATCTCGCCGGTGTTTCTTCTATTCTAGGGGCAATTAACT&quot; # [9] &quot;TTATTACAACAACCTTAAACATAAAACCCCCTTCATTAACACAATATCAAACCCCATTATTTGTCTGATC&quot; #[10] &quot;TGTATTAATTACTGCAGTCCTTCTTCTTCTCTCCCTCCCAGTTCTGGCTGCCGGAATCACTATACTCTTG&quot; #[11] &quot;ACTGACCGAAACCTAAACACCACCTTCTTTGACCCAGCAGGTGGAGGCGACCCTGTCCTGTACCAACACC&quot; #[12] &quot;TGTTCTGATTCTTTGGTCACCCCGAAGTCTACATCCTTATCCTGCCTGGATTTGGTATCATCTCCCATGT&quot; #[13] &quot;TGTCACATTCTACTCTAGCAAAAAAGAACCCTTCGGCTATATAGGAATAGTCTGAGCTATAATATCGATT&quot; #[14] &quot;GGTCTCCTAGGTTTCATTGTTTGAGCTCACCACATATTCACAACAGACCTTAATGTAGACACTCGAGCCT&quot; #[15] &quot;ACTTTACCTCAGCTACTATAATCATCGCTATCCCAACAGGTGTCAAAGTCTTTAGCTGACTTGCCACCAT&quot; #[16] &quot;GCACGGAGGAATTATTAAATGAGACGCCGCCATATTATGGGCTCTCGGATTCATCTTTTTATTTACAGTT&quot; #[17] &quot;GGAGGACTAACTGGAATCGTTTTAGCCAACTCCTCTTTAGACATTGTTTTGCATGATACATATTATGTAG&quot; #[18] &quot;TAGCCCACTTTCACTACGTTCTTTCTATGGGGGCAGTATTTGCCATTATAGCCGGCTTCGTACACTGATT&quot; #[19] &quot;TCCTCTCTTTTCCGGATTTACCCTTCATGAAGCCTGAACAAAAATTCAATTTGGCGTCATATTTACCGGC&quot; #[20] &quot;GTAAATTTAACATTCTTCCCCCAGCATTTCTTAGGTCTCGCAGGCATGCCTCGACGTTATTCAGACTACC&quot; #[21] &quot;CTGACGCCTACACATTATGAAACACCGTTTCATCAATCGGCTCTTTAATCTCTCTAGTTGCAGTAATCAT&quot; #[22] &quot;TATGATGTTTATCATTTGAGAAGCTTTCTCTTCCAAACGCCTACCTCTACCTGCAGAAATAACCCCAACT&quot; #[23] &quot;AATGTAGAATGATTATACGGATCCCCCCCACCTTACCACACTTTTGAGGAAGCCGTTTACTCCAAAATT&quot; #[24] &quot;&quot; class(froggy_seq_split) #[1] &quot;list&quot; As you can see, the object is a list and its first element is a vector with 24 elements. The sequence of COX1 is contained in 22 elements with indices between [2] and [23]. We can paste all of these in a single character vector. For this purpose, we will use the function paste() or its variant paste0(). Notice how I indexed the elements that I want it to be collapsed in a single character vector using the argument collapse = \"\" (i.e., \"\" indicates nothing). The [[1]] indicates that we want the first element of a list and the [2:23] indicates that we want the vector elements starting from [2] to [23] (i.e., 2, 3, 4, 5, …, 23). froggy_one_fasta &lt;- paste(froggy_seq_split[[1]][2:23], collapse = &quot;&quot;) froggy_one_fasta #[1] &quot;GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAG...&quot; We can also name this vector with the corresponding accession number using the function names(). names(froggy_one_fasta) &lt;- froggy_seq_split[[1]][1] froggy_one_fasta # &gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAGCCGAATT...&quot; 4) We can make this vector more interesting by adding more COX1 sequences from three species with a similar procedure. library(rentrez) # if you need to install R-package &#39;rentrez&#39;, use install.packages(&quot;rentrez&quot;) # Download more COX1 nucleotide sequences from NCBI for 3 more species (i.e., three terms) several_sepecies_COX1 &lt;- c(&quot;Ameerega parvula[Organism] AND COI[Gene]&quot;, &quot;Epipedobates anthonyi[Organism] AND COI[Gene]&quot;, &quot;Oophaga sylvatica[Organism] AND COI[Gene]&quot;) We can only retrieve one term a time, we will use a loop to iterate between these three terms and save in an empty list my_collected_sequences. More on loops later. # we create an empty list to collect processed sequences my_collected_sequences &lt;- list() # the loop to retrieve sequences for(i in 1:length(several_sepecies_COX1)) { one_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term= several_sepecies_COX1[i]) one_seqs_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=one_seq_IDs$ids, rettype=&quot;fasta&quot;) my_collected_sequences[[i]] &lt;- one_seqs_fasta } # we have a list &#39;my_collected_sequences&#39; of 3 elements; one per species. length(my_collected_sequences) #[1] 3 5) We need to process these retrieved sequences to the format that only contains the nucleotides and the GenBank accession numbers. We will start with the my_collected_sequences[[2]] that has two concatenated sequences and the create a function to process this automatically. Notice that my_collected_sequences[[2]] has two fasta sequences named MW042033.1 and DQ502853.1. my_collected_sequences[[2]] #[1] &quot;&gt;MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nGTGATAA #TTACCCGATGATTATTCTCCACAAACCATAAAGATATCGGAACCCTATATTTAGTATTTGGGG\\nCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATTAAGTCAACCTGGGTCC #TT\\nACTGGGCGATGACCAGATTTATAATGTAATCGTAACTGCCCATGCTTTCGTCATAATCTTCTTCATGGTT\\nATGCCTATTCTAATTGGTGGATTTGGAAATTGACTGGTACCTTTAATAATTGGAGCC #CCAGACATAGCCT\\nTTCCCCGAATAAATAATATAAGTTTTTGACTTCTTCCGCCCTCATTCCTTCTTCTCTTAGCATCTGCAGG\\nAGTAGAAGCAGGAGCTGGGACAGGTTGAACTGTTTATCCTCCTCTT #GCTGGTAACCTAGCTCATGCAGGC\\nCCATCAGTTGATTTAACCATTTTTTCACTCCATCTTGCAGGGGTCTCTTCAATTCTAGGAGCAATTAATT\\nTTATTACAACCACATTAAACATAAAACCTCCTTCT #TTAACTCAATACCAAACTCCTTTATTTGTCTGATC\\nAGTTCTAATCACCGCAGTACTTCTTCTTCTTTCTCTTCCTGTCCTAGCTGCAGGAATTACAATGCTTTTA\\nACTGATCGAAACCTTAACACCACT #TTCTTTGACCCAGCAGGTGGAGGTGACCCCATTCTTTATCAACATC\\nTCTTCTGATTTTTTGGACACCCAGAAGTTTATATTCTTATTCTCCCAGGGTTTGGAATTATCTCTCATGT\\nTGTAACTTTTTAT #TCTAGCAAAAAAGAACCTTTTGGATATATAGGAATGGTCTGAGCTATAATATCAATT\\nGGCCTCTTAGGTTTTATTGTTTGAGCTCACCATATATTTACTACTGATCTTAACGTTGACACTCGAGCTT\\nAC #TTTACCTCAGCTACTATAATTATTGCCATCCCAACAGGCGTAAAAGTCTTTAGCTGACTTGCCACCAT\\nGCATGGAGGAATTATTAAATGAGATGCTGCTATACTTTGAGCCTTAGGCTTTATTTTCTTGTT #TACTGTT\\nGGTGGACTAACTGGTATCGTTTTAGCTAATTCCTCTTTGGATATTGTTCTTCATGATACCTACTACGTTG\\nTAGCCCACTTCCATTATGTTCTTTCTATAGGAGCAGTATTTGCCATTATAGC #AGGATTTGTTCATTGATT\\nTCCCTTATTTACCGGTTTTACTCTTCATGAAGCCTGAACAAAGATTCATTTTGGCGTCATATTTGCTGGT\\nGTAAATTTAACTTTCTTCCCCCAACATTTTTTAGGATTAGC #AGGAATACCTCGACGCTATTCAGATTATC\\nCTGATGCCTACACCTTATGAAACACCGTTTCTTCTGTTGGCTCCCTAATCTCCCTTGTTGCAGTAATCAT\\nCATGATATTCATTATTTGAGAAGCTTTTTC #ATCTAAACGTCTATTTTTGAATGCAGAAATAACCCCTACT\\nAATGTTGAATGATTATATGGCTCCCCTCCTCCTTATCACACATTTGAGGAAGCTGTTTATTCTAAAGTA\\n\\n #&gt;DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nAACCCTATATTTAGTATTT #GGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCA\\nGAATTAAGTCAACCTGGGTCCTTACTGGGCGATGACCAGATTTATAATGTAATCGTAACTGCCCATGCTT\\nTCGTCATAA #TCTTCTTCATGGTTATGCCTATTCTAATTGGTGGATTTGGAAATTGACTGGTACCTTTAAT\\nAATTGGAGCCCCAGACATAGCCTTTCCCCGAATAAATAATATAAGTTTTTGACTTCTTCCGCCCTCATTC\\ #nCTTCTTCTCTTAGCATCTACAGGAGTAGAAGCAGGAGCTGGGACAGGTTGAACTGTTTATCCTCCTCTTG\\nCTGGTAACCTAGCTCATGCAGGCCCATCAGTTGATTTAACCATTTTTTCACTCCATCTTGC #AGGGGTCTC\\nTTCAATTCTAGGAGCAATTAATTTTATTACAACCACATTAAACATAAAACCTCCTTCTTTAACTCAATAC\\nCAAACTCCTTTATTTGTCTGATCAGTTCTAATCACCGCAGTACTTCTTCTT #CTTTCTCTTCCTGTCCTAG\\nCTGCAGGAATTACAATGCTTTTAACTGATCGAAACCTTAACACCACTTTCTTTGACCCAGCAGGTGGAGG\\nTGACCCCATTCTTTATCAACATCTCTTC\\n\\n&quot; We will split this long character vector by the unique character that defines a new fasta entry which is indicated by the &gt;. one_species_all_fasta &lt;- my_collected_sequences[[2]] one_species_split_by_fasta &lt;- strsplit(one_species_all_fasta, split = &#39;&gt;&#39;) one_species_split_by_fasta #[[1]] #[2] &quot;MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nGTGATAATTA... #[3] &quot;DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nAACCCTATATTTAGTA... We need to remove these character vectors from the list using the function unlist(). one_species_split_by_fasta_vector &lt;- unlist(one_species_split_by_fasta) one_species_split_by_fasta_vector #[2] &quot;MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nGTGATAATTA... #[3] &quot;DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nAACCCTATATTTAGTA... We can also remove the empty element [1] \"\" with the application of the logical condition x[x != \"\"] where x is our vector. one_species_split_by_fasta_vector &lt;- one_species_split_by_fasta_vector[one_species_split_by_fasta_vector != &quot;&quot;] one_species_split_by_fasta_vector #[1] &quot;MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nGTGATAATTA... #[2] &quot;DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial\\nAACCCTATATTTAGTA... We can now process each of these element as we did on step 3), but using a loop. # we create an empty list to collect processed sequences one_species_processed_seq_list &lt;- list() # the loop to process sequences for(i in 1:length(one_species_split_by_fasta_vector)) { # split and unlist one_species_clean_data &lt;- unlist(strsplit(one_species_split_by_fasta_vector[i], split = &#39;\\n&#39;)) # remove empty element &quot;&quot; one_species_clean_data &lt;- one_species_clean_data[one_species_clean_data != &quot;&quot;] # we know the first element will be always the accession number and the rest sequence but the length migh vary so we use length() to determine that. one_species_only_seq &lt;- paste(one_species_clean_data[2:length(one_species_clean_data)], collapse = &quot;&quot;) # we name that sequence with the corresponding accession number on the first element of &#39;one_species_clean_data&#39;. names(one_species_only_seq) &lt;- one_species_clean_data[1] # finally, collected the processed sequences one_species_processed_seq_list[[i]] &lt;- one_species_only_seq } # we the unlist this final output for species to a vector one_species_end &lt;- unlist(one_species_processed_seq_list) one_species_end #MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACCCGATGATTATTCTCCACAAACCATAAAGATATCGGAACCCTATATTTAGTATTTGGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATT...&quot; #DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;AACCCTATATTTAGTATTTGGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATTAAGTCAACCTGGGTCCTTACTGGGCGATGACCAGATTTATAATGTAA...&quot; However, this was just applied to a single species, but we have three. Now, we will generalize this process into single process that show the beauty of automatization with R. 6) A generalization to create this sequence vectors with all previous steps together will use a more complex loop with two nested indices j and i. Yet, you are welcome to explore alternatives. list_final_fasta &lt;- list() for(j in 1:length(my_collected_sequences)) { # j &lt;- 3 one_species_all_fasta &lt;- my_collected_sequences[[j]] one_species_split_by_fasta &lt;- strsplit(one_species_all_fasta, split = &#39;&gt;&#39;) one_species_split_by_fasta_vector &lt;- unlist(one_species_split_by_fasta) one_species_split_by_fasta_vector &lt;- one_species_split_by_fasta_vector[one_species_split_by_fasta_vector != &quot;&quot;] one_species_processed_seq_list &lt;- list() for(i in 1:length(one_species_split_by_fasta_vector)) { one_species_clean_data &lt;- unlist(strsplit(one_species_split_by_fasta_vector[i], split = &#39;\\n&#39;)) one_species_clean_data &lt;- one_species_clean_data[one_species_clean_data != &quot;&quot;] one_species_only_seq &lt;- paste(one_species_clean_data[2:length(one_species_clean_data)], collapse = &quot;&quot;) names(one_species_only_seq) &lt;- one_species_clean_data[1] one_species_processed_seq_list[[i]] &lt;- one_species_only_seq } list_final_fasta[[j]] &lt;- unlist(one_species_processed_seq_list) } final_fasta_vector &lt;- unlist(list_final_fasta) final_fasta_vector #MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACCCGATGATTATTCTCCACAAACCATAAAGATATCGGAACCCTATATTTAGTATTTGGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATT...&quot; #DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;AACCCTATATTTAGTATTTGGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATTAAGTCAACCTGGGTCCTTACTGGGCGATGACCAGATTTATAATGTAA...&quot; #... 7) We can append to the top the first processed sequence on step 3) to this long vector. final_fasta_vector &lt;- c(froggy_one_fasta, final_fasta_vector) final_fasta_vector # &gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAGCCGAATT...&quot; #MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACCCGATGATTATTCTCCACAAACCATAAAGATATCGGAACCCTATATTTAGTATTTGGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATT...&quot; #DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;AACCCTATATTTAGTATTTGGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATTAAGTCAACCTGGGTCCTTACTGGGCGATGACCAGATTTATAATGTAA...&quot; #... 8) We will explore some properties of the resulting character vector. We can get how many elements (sequences) we have in the vector with the function length(). It has 24 sequences. length(final_fasta_vector) [1] 24 We can get the number of characters (i.e., letters or nucleotides) on each sequence the function nchar(). nchar(final_fasta_vector) #&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial # 1539 # MW042032.1 Ameerega parvula voucher QCAZ16584 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial # 1539 #MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial # 1539 # DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial # 658 # MH589864.1 Oophaga sylvatica isolate CH2 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial # 523 #... We can get change the case characters using the function tolower() that converts any upper case characters into lower case. The opposite procedure can be achieved with the function toupper() that converts any lower case characters into upper case. tolower(final_fasta_vector)[1] #&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;gtgataattactcgatgattattttctaccaaccacaaagacatcggaactttatacctagtgtttggggcatgagcaggcatagtcggcactgctcttagcctttt...&quot; toupper(final_fasta_vector)[1] #&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTA...&quot; 9) We can split the character vector into chucks of a defined length using following function. We want to split the vector into individual nucleotides. one_sequence &lt;- final_fasta_vector[1] names(one_sequence) &lt;- NULL one_sequence_nucleotides &lt;- unlist(strsplit(one_sequence, split = &quot;&quot;)) one_sequence_nucleotides # [1] &quot;G&quot; &quot;T&quot; &quot;G&quot; &quot;A&quot; &quot;T&quot; &quot;A&quot; &quot;A&quot; &quot;T&quot; &quot;T&quot; &quot;A&quot; &quot;C&quot; &quot;T&quot; &quot;C&quot; &quot;G&quot; &quot;A&quot; &quot;T&quot; &quot;G&quot; &quot;A&quot; &quot;T&quot; &quot;T&quot; &quot;A&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;C&quot; &quot;T&quot; &quot;A&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; # [33] &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;G&quot; &quot;A&quot; &quot;C&quot; &quot;A&quot; &quot;T&quot; &quot;C&quot; &quot;G&quot; &quot;G&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;A&quot; &quot;T&quot; &quot;A&quot; &quot;C&quot; &quot;C&quot; &quot;T&quot; &quot;A&quot; &quot;G&quot; &quot;T&quot; &quot;G&quot; &quot;T&quot; # [65] &quot;T&quot; &quot;T&quot; &quot;G&quot; &quot;G&quot; &quot;G&quot; &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;T&quot; &quot;G&quot; &quot;A&quot; &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;G&quot; &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;T&quot; &quot;A&quot; &quot;G&quot; &quot;T&quot; &quot;C&quot; &quot;G&quot; &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;C&quot; &quot;T&quot; &quot;G&quot; &quot;C&quot; &quot;T&quot; #... We can also split one continuous sequence into codons using a the function sapply(). This is a very useful function to process a function into recursively loop applied to a vector. one_sequence &lt;- final_fasta_vector[1] names(one_sequence) &lt;- NULL one_sequence_codons &lt;- sapply(seq(from=1, to=nchar(one_sequence), by=3), function(i) substr(one_sequence, i, i+2)) one_sequence_codons # [1] &quot;GTG&quot; &quot;ATA&quot; &quot;ATT&quot; &quot;ACT&quot; &quot;CGA&quot; &quot;TGA&quot; &quot;TTA&quot; &quot;TTT&quot; &quot;TCT&quot; &quot;ACC&quot; &quot;AAC&quot; &quot;CAC&quot; &quot;AAA&quot; &quot;GAC&quot; &quot;ATC&quot; &quot;GGA&quot; &quot;ACT&quot; &quot;TTA&quot; &quot;TAC&quot; &quot;CTA&quot; &quot;GTG&quot; #[22] &quot;TTT&quot; &quot;GGG&quot; &quot;GCA&quot; &quot;TGA&quot; &quot;GCA&quot; &quot;GGC&quot; &quot;ATA&quot; &quot;GTC&quot; &quot;GGC&quot; &quot;ACT&quot; &quot;GCT&quot; &quot;CTT&quot; &quot;AGC&quot; &quot;CTT&quot; &quot;TTA&quot; &quot;ATT&quot; &quot;CGA&quot; &quot;GCC&quot; &quot;GAA&quot; &quot;TTA&quot; &quot;AGC&quot; #[43] &quot;CAG&quot; &quot;CCC&quot; &quot;GGG&quot; &quot;TCC&quot; &quot;TTA&quot; &quot;CTA&quot; &quot;GGC&quot; &quot;GAT&quot; &quot;GAC&quot; &quot;CAG&quot; &quot;ATC&quot; &quot;TAC&quot; &quot;AAC&quot; &quot;GTT&quot; &quot;ATT&quot; &quot;GTT&quot; &quot;ACC&quot; &quot;GCC&quot; &quot;CAT&quot; &quot;GCT&quot; &quot;TTC&quot; #... 10) We can transform some characters and replace between characters using the function chartr(). We can try to replace thymine (T) with uracil (U) for DNA to RNA sequences. chartr(&quot;T&quot;, &quot;U&quot;, final_fasta_vector[1]) #&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) &gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GUGAUAAUUACUCGAUGAUUAUUUUCUACCAACCACAAAGACAUCGGAACUUUAUACCUAGUGUUUGGGGCAUGAGCAGGCAUAGUCGGCACUGCUCUUAGCCUUUUAAUUCG...&quot; 4.2 Regular expressions (Regex) Finding patterns of characters in strings can be done in many different ways, some are extremely useful, and many cases are tedious. In general, they follow a similar procedure that starts with finding a pattern and indexing it, replacing this pattern with other string, testing for the presence/absence of the pattern, and extracting characters nearby or between such known patterns. However, they can be hard to implement, and I will present some few applications that I find useful for bioinformatics while working with sequences. 1) Finding patterns with grep() and grepl(). These functions will search for matches to an argument pattern within each element of a character vector. We can try to find if any of our sequences have a particular set of characters (i.e., nucleotide sequences) and report which element has them within the final_fasta_vector. The result below indicate that only sequence with index [1] and [2] have such pattern. my_pattern &lt;- &quot;ACTGGCTGAACTGTGTACCC&quot; what_sequences &lt;- grep(pattern = my_pattern, final_fasta_vector, ignore.case = FALSE, fixed = FALSE, invert = FALSE) what_sequences #[1] 1 2 final_fasta_vector[what_sequences] #&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial &quot;GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAGCCGAA...&quot; #MW042032.1 Ameerega parvula voucher QCAZ16584 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGATATTGGAACCTTATACCTAGTGTTTGGGGCGTGAGCGGGCATAGTCGGCACTGCTCTTAGCCTCTTAATTCGAGCCGAG...&quot; A similar result can be obtained with grepl(), but it will be a logical vector and only those elements that match will have a TRUE value and are returned when what_sequences is used to index the final_fasta_vector. my_pattern &lt;- &quot;ACTGGCTGAACTGTGTACCC&quot; what_sequences &lt;- grepl(pattern = my_pattern, final_fasta_vector, ignore.case = FALSE, fixed = FALSE) what_sequences #[1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #[23] FALSE FALSE final_fasta_vector[what_sequences] #&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAGCCGAA...&quot; #MW042032.1 Ameerega parvula voucher QCAZ16584 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGATATTGGAACCTTATACCTAGTGTTTGGGGCGTGAGCGGGCATAGTCGGCACTGCTCTTAGCCTCTTAATTCGAGCCGAG...&quot; 2) Replacing patterns with sub() and gsub(). These functions will search for matches to an argument pattern and replace them with a text that you must provide. The sub() will replace the first occurrence leaving the others as they are. On the other hand, the gsub() will replace all the strings or values with the input pattern. my_pattern &lt;- &quot;CAAACAAA&quot; my_replacement &lt;-&quot;!!!found_it!!!&quot; fasta_vector_replacements &lt;- sub(pattern = my_pattern, replacement = my_replacement, final_fasta_vector) fasta_vector_replacements #... #DQ502853.1Epipedobatesanthonyiisolate838cytochromeoxidasesubunitI(COI)gene,partialcds;mitochondrial #&quot;AACCCTATATTTAGTATTTGGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATTAAGTCAACCTGGGTCCTTACT...&quot; #MH589864.1OophagasylvaticaisolateCH2cytochromeoxidasesubunitI(COI)gene,partialcds;mitochondrial #&quot;TTACATTCTCATCCTCCCAGGCTTCGGAATCATCTCCCATGTAGTCACGTTTTACT!!!found_it!!!AAAGAGCCATTTGGCTACATAGGAAT...&quot; #MH589863.1OophagasylvaticaisolateCH1cytochromeoxidasesubunitI(COI)gene,partialcds;mitochondrial #&quot;TTACATTCTCATCCTCCCAGGCTTCGGAATCATCTCTCATGTAGTCACGTTTTACT!!!found_it!!!AAAGAGCCATTTGGCTACATAGGAAT...&quot; #MH589862.1OophagasylvaticaisolateMC2cytochromeoxidasesubunitI(COI)gene,partialcds;mitochondrial #&quot;TTACATTCTCATCCTCCCAGGGTTCGGAATCATCTCCCATGTAGTCACGTTTTACT!!!found_it!!!AAAGAGCCATTTGGTTACATAGGAAT...&quot; #... # using gsub fasta_vector_replacements &lt;- gsub(pattern = my_pattern, replacement = my_replacement, final_fasta_vector) fasta_vector_replacements #... #DQ502853.1Epipedobatesanthonyiisolate838cytochromeoxidasesubunitI(COI)gene,partialcds;mitochondrial #&quot;AACCCTATATTTAGTATTTGGGGCATGAGCCGGAATAGTAGGAACAGCCCTAAGCCTCTTAATTCGAGCAGAATTAAGTCAACCTGGGTCCTTACT...&quot; #MH589864.1OophagasylvaticaisolateCH2cytochromeoxidasesubunitI(COI)gene,partialcds;mitochondrial #&quot;TTACATTCTCATCCTCCCAGGCTTCGGAATCATCTCCCATGTAGTCACGTTTTACT!!!found_it!!!AAAGAGCCATTTGGCTACATAGGAAT...&quot; #MH589863.1OophagasylvaticaisolateCH1cytochromeoxidasesubunitI(COI)gene,partialcds;mitochondrial #&quot;TTACATTCTCATCCTCCCAGGCTTCGGAATCATCTCTCATGTAGTCACGTTTTACT!!!found_it!!!AAAGAGCCATTTGGCTACATAGGAAT...&quot; #MH589862.1OophagasylvaticaisolateMC2cytochromeoxidasesubunitI(COI)gene,partialcds;mitochondrial #&quot;TTACATTCTCATCCTCCCAGGGTTCGGAATCATCTCCCATGTAGTCACGTTTTACT!!!found_it!!!AAAGAGCCATTTGGTTACATAGGAAT...&quot; #... 3) We can match multiple patterns using the | that signifies or. We will use grepl() to exemplify its use. Here we are testing if it matches any of two patterns. These might not need to be adjacent (i.e., there are some chucks of sequence between them). my_pattern &lt;- c(&quot;TCATCTCCCATGTA&quot;, &quot;GTGATAATTACTCGATGATTATTTT&quot;) my_pattern &lt;- paste0(my_pattern, collapse = &quot;|&quot;) my_pattern #[1] &quot;TCATCTCCCATGTA|GTGATAATTACTCGATGATTATTTT&quot; what_sequences &lt;- grepl(my_pattern, final_fasta_vector, ignore.case = FALSE, fixed = FALSE) what_sequences #[[1] TRUE TRUE FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #[23] TRUE TRUE Here we are testing if the vector elements match both patterns at the same time. These are not necessarily adjacent (i.e., there could be a chuck sequence between them). We can use wildcard character . (i.e., it represents any character except \\n, which is a new line) and * is a greedy character (i.e., the asterisk will always match the longest possible string). my_pattern &lt;- c(&quot;GTGATAATTACTCGATGATTATTTTCTA&quot;, &quot;ACTTTATACCTAGTGTTTGGGGCA&quot;) my_pattern &lt;- paste0(my_pattern, collapse = &quot;.*&quot;) my_pattern #[1] &quot;GTGATAATTACTCGATGATTATTTTCTA.*ACTTTATACCTAGTGTTTGGGGCA&quot; what_sequences &lt;- grepl(my_pattern, final_fasta_vector, ignore.case = FALSE, fixed = FALSE) what_sequences # [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #[23] FALSE FALSE Notice that we can overrun the wildcard nature of these characters by changing the argument fixed = TRUE. This means a literal match to .* in the sequence. my_pattern &lt;- c(&quot;GTGATAATTACTCGATGATTATTTTCTA&quot;, &quot;ACTTTATACCTAGTGTTTGGGGCA&quot;) my_pattern &lt;- paste0(my_pattern, collapse = &quot;.*&quot;) my_pattern #[1] &quot;GTGATAATTACTCGATGATTATTTTCTA.*ACTTTATACCTAGTGTTTGGGGCA&quot; what_sequences &lt;- grepl(my_pattern, final_fasta_vector, ignore.case = FALSE, fixed = TRUE) # This result in all cases as FALSE because we fixed to match EXACTLY the text pattern &quot;GTGATAATTACTCGATGATTATTTTCTA.*ACTTTATACCTAGTGTTTGGGGCA&quot; what_sequences # [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #[23] FALSE FALSE 4) We can use some characters to match only numbers, only lower case letters or special symbols. These can be used to find or replace text after or before these special characters. To illustrate some of their use we will construct a vector with random text. We will use the function cat() to concatenate and print this vector. my_random_vector &lt;- c(&quot;text_file.txt&quot;, &quot;text_file.csv&quot;, &quot;text_file2.txt&quot;, &quot;multiple\\ndots\\nin\\nthis.line&quot;, &quot;multiple\\tdots\\tin\\tthis.line&quot;, &quot;multiple.dots.in.this.line&quot;, &quot;CaPiTaL_123_NYC&quot;) my_random_vector #[1] &quot;text_file.txt&quot; &quot;text_file.csv&quot; &quot;text_file2.txt&quot; &quot;multiple\\ndots\\nin\\nthis.line&quot; #[5] &quot;multiple\\tdots\\tin\\tthis.line&quot; &quot;multiple.dots.in.this.line&quot; &quot;CaPiTaL_123_NYC&quot; cat(my_random_vector) #text_file.txt text_file.csv text_file2.txt multiple #dots #in #this.line multiple dots in this.line multiple.dots.in.this.line CaPiTaL_123_NYC We can use gsub() to replace both \\n (new line) and \\t (tabs) using | that means an or condition with a - as a replacement. Compare output before and after the replacement. my_vector_processed &lt;- gsub(pattern = &quot;\\n|\\t&quot;, replacement = &quot;-&quot;, my_random_vector) my_random_vector[4:5] #[1] &quot;multiple\\ndots\\nin\\nthis.line&quot; &quot;multiple\\tdots\\tin\\tthis.line&quot; my_vector_processed[4:5] #[1] &quot;multiple-dots-in-this.line&quot; &quot;multiple-dots-in-this.line&quot; We can use gsub() to replace all digits (i.e., 0 to 9) using [[:digit:]] with - replacement. Compare output before and after the replacement. my_vector_processed &lt;- gsub(pattern = &quot;[[:digit:]]&quot;, replacement = &quot;-&quot;, my_random_vector) my_random_vector[c(3,7)] #[1] &quot;text_file2.txt&quot; &quot;CaPiTaL_123_NYC&quot; my_vector_processed[c(3,7)] #[1] &quot;text_file-.txt&quot; &quot;CaPiTaL_---_NYC&quot; We can use gsub() to replace all non-digits using \\\\D or [^[:digit:]] with - replacement. Notice the ^ that is used to negate (i.e., the opposite of digits, will be non-digits). Compare output before and after the replacement. my_vector_processed &lt;- gsub(pattern = &quot;\\\\D&quot;, replacement = &quot;-&quot;, my_random_vector) my_random_vector #[1] &quot;text_file.txt&quot; &quot;text_file.csv&quot; &quot;text_file2.txt&quot; &quot;multiple\\ndots\\nin\\nthis.line&quot; #[5] &quot;multiple\\tdots\\tin\\tthis.line&quot; &quot;multiple.dots.in.this.line&quot; &quot;CaPiTaL_123_NYC&quot; my_vector_processed #[1] &quot;-------------&quot; &quot;-------------&quot; &quot;---------2----&quot; &quot;--------------------------&quot; #[5] &quot;--------------------------&quot; &quot;--------------------------&quot; &quot;--------123----&quot; my_vector_processed &lt;- gsub(pattern = &quot;[^[:digit:]]&quot;, replacement = &quot;-&quot;, my_random_vector) my_vector_processed #[1] &quot;-------------&quot; &quot;-------------&quot; &quot;---------2----&quot; &quot;--------------------------&quot; #[5] &quot;--------------------------&quot; &quot;--------------------------&quot; &quot;--------123----&quot; We can use gsub() to replace all periods using[.] with spaces using or with nothing as \"\". Compare output before and after the replacement. my_vector_processed &lt;- gsub(pattern = &quot;[.]&quot;, replacement = &quot; &quot;, my_random_vector) my_random_vector[6] #[1] &quot;multiple.dots.in.this.line&quot; my_vector_processed[6] #[1] &quot;multiple dots in this line&quot; my_vector_processed &lt;- gsub(pattern = &quot;[.]&quot;, replacement = &quot;&quot;, my_random_vector) my_vector_processed[6] #[1] &quot;multipledotsinthisline&quot; 5) We can use some anchor characters ^ (for START of the string) and $ (for END of the string). These are especially characters if you want to replace or find a pattern like the extension of a file. These strings have the text pattern at the beginning. these_strings &lt;- grepl(pattern = &quot;^text&quot;, my_random_vector, ignore.case = FALSE, fixed = FALSE) my_random_vector[these_strings] #[1] &quot;text_file.txt&quot; &quot;text_file.csv&quot; &quot;text_file2.txt&quot; We can get the file names that end with .txt pattern. these_are_txt_files &lt;- grepl(pattern = &quot;*.txt$&quot;, my_random_vector, ignore.case = FALSE, fixed = FALSE) my_random_vector[these_are_txt_files] #[1] &quot;text_file.txt&quot; &quot;text_file2.txt&quot; 6) An interesting application of regular expressions is to extract text (e.g., nucleotide sequence) between two patterns. This is a powerful demonstration of grepl() and gsub(). my_pattern &lt;- c(&quot;GTGATAATTACTCGATGATTATTTTCTA&quot;, &quot;ACTTTATACCTAGTGTTTGGGGCA&quot;) my_pattern &lt;- paste0(&quot;^.*&quot;, my_pattern[1], &quot;\\\\s*|\\\\s*&quot;, my_pattern[2], &quot;.*$&quot;) my_pattern #[1] &quot;^.*GTGATAATTACTCGATGATTATTTTCTA\\\\s*|\\\\s*ACTTTATACCTAGTGTTTGGGGCA.*$&quot; is_the_pattern_present &lt;- grepl(pattern = my_pattern, final_fasta_vector) is_the_pattern_present # [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #[23] FALSE FALSE sequences_that_have_pattern &lt;- final_fasta_vector[is_the_pattern_present] my_extracted_sequences &lt;- gsub(pattern = my_pattern, replacement = &#39;&#39;, sequences_that_have_pattern) my_extracted_sequences #&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #&quot;CCAACCACAAAGACATCGGA&quot; 4.3 Comparing, ordering and simplifying vectors 1) There are several operations that can be applied to vectors that can help you manage the information contained in them. They are applied to one or two vectors. We will illustrate some examples. We will build two relatively similar vectors. my_vector_1 &lt;- c(&quot;Allobates&quot;, &quot;Epipedobates&quot;, &quot;Dendrobates&quot;, &quot;Hyloxalus&quot;, &quot;Mannophryne&quot;, &quot;Hyloxalus&quot;, &quot;Silverstoneia&quot;) my_vector_2 &lt;- c(&quot;Epipedobates&quot;, &quot;Epipedobates&quot;,&quot;Dendrobates&quot;, &quot;Hyloxalus&quot;, &quot;Mannophryne&quot;, &quot;Hyloxalus&quot;, &quot;Rheobates&quot;) We can remove duplicated entries using the function unique(). unique(my_vector_1) #[1] &quot;Allobates&quot; &quot;Epipedobates&quot; &quot;Dendrobates&quot; &quot;Hyloxalus&quot; &quot;Mannophryne&quot; &quot;Silverstoneia&quot; unique(my_vector_2) #[1] &quot;Epipedobates&quot; &quot;Dendrobates&quot; &quot;Hyloxalus&quot; &quot;Mannophryne&quot; &quot;Rheobates&quot; 2) The union of elements of two vectors is performed with union(). This function will result in a new vector that contain elements from both vectors even if they are not in common but discards any duplicated values. union(my_vector_1, my_vector_2) #[1] &quot;Allobates&quot; &quot;Epipedobates&quot; &quot;Dendrobates&quot; &quot;Hyloxalus&quot; &quot;Mannophryne&quot; &quot;Silverstoneia&quot; &quot;Rheobates&quot; 3) The intersection of elements of two vectors is determined with intersect(). This function will result in a new vector that contain elements from both vectors, and these are elements in common, but discards any duplicated values. intersect(my_vector_1, my_vector_2) #[1] &quot;Epipedobates&quot; &quot;Dendrobates&quot; &quot;Hyloxalus&quot; &quot;Mannophryne&quot; 4) The function setdiff() results in a vector of elements that has those that are not in common between the two vectors. However, it depends on vector that corresponds to x on the function as setdiff(x, y). This will return elements unique to my_vector_1 and not in common with my_vector_2. setdiff(my_vector_1, my_vector_2) #[1] &quot;Allobates&quot; &quot;Silverstoneia&quot; This will return elements unique to my_vector_2 and not in common with my_vector_1. setdiff(my_vector_2, my_vector_1) #[1] &quot;Rheobates&quot; 5) The function setequal() results in a logical test (i.e., TRUE or FALSE) that determines if these two vector contain the same elements. setequal(my_vector_2, my_vector_1) #[1] FALSE 6) The function is.element() results in logical test (i.e., TRUE or FALSE) that determines if an element is present in a vector. is.element(c(&quot;Allobates&quot;, &quot;Rheobates&quot;), my_vector_1) #[[1] TRUE FALSE is.element(c(&quot;Allobates&quot;, &quot;Rheobates&quot;), my_vector_2) #[1] FALSE TRUE A similar, but more compact, form is the function %in%. c(&quot;Allobates&quot;, &quot;Rheobates&quot;) %in% my_vector_1 #[[1] TRUE FALSE c(&quot;Allobates&quot;, &quot;Rheobates&quot;) %in% my_vector_2 #[1] FALSE TRUE 7) We can organize the elements in a vector alphabetically or numerically using the function sort(). This ordering is by default increasing, but you can order in decreasing fashion by changing the argument decreasing = TRUE. sort(my_vector_1) #[1] &quot;Allobates&quot; &quot;Dendrobates&quot; &quot;Epipedobates&quot; &quot;Hyloxalus&quot; &quot;Hyloxalus&quot; &quot;Mannophryne&quot; &quot;Silverstoneia&quot; sort(my_vector_1, decreasing = TRUE) #[1] &quot;Silverstoneia&quot; &quot;Mannophryne&quot; &quot;Hyloxalus&quot; &quot;Hyloxalus&quot; &quot;Epipedobates&quot; &quot;Dendrobates&quot; &quot;Allobates&quot; 8) We can concatenate two vectors with the function c(). We can also remove duplicated entries and order the final vector with the functions unique() and sort(). my_vector_1_2 &lt;- sort(unique(c(my_vector_1,my_vector_2))) my_vector_1_2 #[1] &quot;Allobates&quot; &quot;Dendrobates&quot; &quot;Epipedobates&quot; &quot;Hyloxalus&quot; &quot;Mannophryne&quot; &quot;Rheobates&quot; &quot;Silverstoneia&quot; 4.4 Useful functions of stringr 9) There are several specialized R-packages that deal with strings and more or less simplify what we have demonstrated above. However, it forces you to be dependent on loading these packages in the R environment before you can use them. One of the most used is stringr that is part of the ‘tidyverse’ family. This package has an extensive manual and some functions can make some of the processing of string or character vectors easier. # to install R-package stringr install.packages(&quot;stringr&quot;) library(stringr) One useful function is str_count() that can count the number of matches of a pattern in a string. We can construct a data frame with the total counts of each nucleotide bases. adenine &lt;- str_count(final_fasta_vector, pattern = &quot;A&quot;) cytocine &lt;- str_count(final_fasta_vector, pattern = &quot;C&quot;) guanine &lt;- str_count(final_fasta_vector, pattern = &quot;G&quot;) thymidine &lt;- str_count(final_fasta_vector, pattern = &quot;T&quot;) nucleotide_numbers &lt;- data.frame(n_adenine = adenine, n_cytocine = cytocine, n_guanine = guanine, n_thymidine = thymidine, genbank_accession = names(final_fasta_vector), stringsAsFactors = FALSE) head(nucleotide_numbers) # n_adenine n_cytocine n_guanine n_thymidine #1 384 410 256 489 #2 369 399 273 498 #3 395 340 250 554 #4 169 161 108 220 #5 136 116 94 177 #6 137 117 92 177 # genbank_accession #1 &gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #2 MW042032.1 Ameerega parvula voucher QCAZ16584 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #3 MW042033.1 Epipedobates anthonyi voucher QCAZ16597 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #4 DQ502853.1 Epipedobates anthonyi isolate 838 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #5 MH589864.1 Oophaga sylvatica isolate CH2 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #6 MH589863.1 Oophaga sylvatica isolate CH1 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial Another useful function is str_locate_all() that can find where a pattern is located in the vector elements (i.e., the position of the characters that match the pattern). We can construct a data frame with the count number of each nucleotide bases. restriction_sites_to_cut_with_BFAL &lt;- str_locate_all(final_fasta_vector, pattern = &quot;CTAG&quot;) names(restriction_sites_to_cut_with_BFAL) &lt;- names(final_fasta_vector) restriction_sites_to_cut_with_BFAL[1:2] #`&gt;MW042030.1 Ameerega bilinguis voucher QCAZ28835 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial` # start end #[1,] 58 61 #[2,] 142 145 #[3,] 337 340 #[4,] 406 409 #[5,] 475 478 #[6,] 785 788 #[7,] 847 850 #[8,] 1384 1387 #$`MW042032.1 Ameerega parvula voucher QCAZ16584 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial` # start end #[1,] 58 61 #[2,] 475 478 #[3,] 785 788 #[4,] 847 850 #[5,] 1024 1027 #[6,] 1072 1075 #[7,] 1384 1387 4.5 Data transformations An important application of R in data analyses is transformations (e.g., one data structure into another or merging data frames). 1) Transforming vectors into other data structures (e.g., list or data frame) are usually straightforward. From vector to list using the function as.list(). my_vector &lt;- 1:10 my_vector #[1] 1 2 3 4 5 6 7 8 9 10 my_list &lt;- as.list(my_vector) my_list From list to vector using the function unlist(). unlist(my_list) #[1] 1 2 3 4 5 6 7 8 9 10 From vector to a data frame using the function as.data.frame(). my_data_frame &lt;- as.data.frame(my_vector, stringsAsFactors = FALSE) head(my_data_frame) # my_vector #1 1 #2 2 #3 3 #4 4 #5 5 #6 6 2) Apply a function as a loop to each element of a list using the function lapply(). # create a list of two vectors of numbers my_list2 &lt;- list(odd_numbers = c(1,3,5,7,9), prime_numbers = c(1,2,3,5,7,11,13,17)) my_list2 #$odd_numbers #[1] 1 3 5 7 9 #$prime_numbers #[1] 1 2 3 5 7 11 13 17 Then, we can apply the function mean() to both elements (the two numbers vectors) in my_list2 using the loop function lapply() that it is used for lists. lapply(my_list2, mean) #$odd_numbers #[1] 5 #$prime_numbers #[1] 7.375 3) Apply a function to rows or columns of a data frame or matrix using the function apply(). We can create a data frame of numbers in columns. my_data_frame &lt;- data.frame(A = c(1,2,NA,3,4), B = c(NA,4,1,67,-2), C = c(-2,3,1,99,NA), D = c(78,56,9,3,NA), stringsAsFactors = FALSE) Then, we can add the name of the with the function rownames() to each row of the data frame. rownames(my_data_frame) &lt;- c(&quot;row_1&quot;, &quot;row_2&quot;, &quot;row_3&quot;, &quot;row_4&quot;, &quot;row_5&quot;) my_data_frame # A B C D # row_1 1 NA -2 78 # row_2 2 4 3 56 # row_3 NA 1 1 9 # row_4 3 67 99 3 # row_5 4 -2 NA NA Now we can apply a function max() to every row. Notice the 1 in apply() that denotes that it will get the maximum value per each individual row. names_columns &lt;- names(my_data_frame) max_values &lt;- apply(my_data_frame,1,max) names(max_values) &lt;- rownames(my_data_frame) max_values #row_1 row_2 row_3 row_4 row_5 # NA 56 NA 99 NA Notice that some values were NA (no data), to remove those NA while applying the function max(), we need the argument na.rm = TRUE. max_values_no_NA &lt;- apply(my_data_frame,1,max, na.rm = TRUE) names(max_values_no_NA) &lt;- rownames(my_data_frame) max_values_no_NA #row_1 row_2 row_3 row_4 row_5 # 78 56 9 99 4 To apply a function max() to every column, we will use 2 in apply(). max_values_columns_no_NA &lt;- apply(my_data_frame,2,max, na.rm = TRUE) names(max_values_columns_no_NA) &lt;- names(my_data_frame) max_values_columns_no_NA # A B C D # 4 67 99 78 4) Here are some build-in and useful functions that can be applied to rows and columns. colSums (my_data_frame, na.rm = TRUE) # A B C D # 10 70 101 146 rowSums (my_data_frame, na.rm = TRUE) #row_1 row_2 row_3 row_4 row_5 # 77 65 11 172 2 colMeans(my_data_frame, na.rm = TRUE) # A B C D # 2.50 17.50 25.25 36.50 rowMeans(my_data_frame, na.rm = TRUE) # row_1 row_2 row_3 row_4 row_5 #25.666667 16.250000 3.666667 43.000000 1.000000 5) We can apply a function to every cell in a data frame using the R-package purrr that is part of the ‘tidyverse’ family and its function modify(). # install R-package &#39;purrr&#39; if not present install.packages(&quot;purrr&quot;) library(purrr) We will apply a function that assigns 1 to zero and positive numbers in the data frame and 0 to negative numbers. Notice we typed purrr::modify to indicate that we what the function modify() only from the purrr R-package. I also created a function that for every x (a cell value) will determine if this x is more or equal to 0. This function will return 1 if TRUE or 0 if FALSE by using the function ifelse(). This function has three parts: a conditional test, output_1 if TRUE and output_2 if FALSE. my_data_frame_1_0 &lt;- purrr::modify(my_data_frame,function(x)ifelse(x&gt;=0,1,0)) my_data_frame_1_0 # A B C D #row_1 1 NA 0 1 #row_2 1 1 1 1 #row_3 NA 1 1 1 #row_4 1 1 1 1 #row_5 1 0 NA NA 6) You can add a new column in a data frame to store the result of a calculation. As an example, we will create a sum of all values per row names into a column named sum_of_my_rows. my_data_frame$sum_of_my_rows &lt;- rowSums (my_data_frame, na.rm = TRUE) my_data_frame # A B C D sum_of_my_rows #row_1 1 NA -2 78 77 #row_2 2 4 3 56 65 #row_3 NA 1 1 9 11 #row_4 3 67 99 3 172 #row_5 4 -2 NA NA 2 7) You can also add a new row to the data frame and store the result of a calculation using the function rbind(). my_column_sums &lt;- colSums (my_data_frame, na.rm = TRUE) # we add this row using rbind() and modify my_data_frame my_data_frame &lt;- rbind(my_data_frame, my_column_sums) my_data_frame # A B C D sum_of_my_rows #row_1 1 NA -2 78 77 #row_2 2 4 3 56 65 #row_3 NA 1 1 9 11 #row_4 3 67 99 3 172 #row_5 4 -2 NA NA 2 #6 10 70 101 146 327 Next, we can update the names of the added rows following the format row_x where x is the number of the new row. rownames(my_data_frame)[6] &lt;- &quot;row_6&quot; my_data_frame # A B C D sum_of_my_rows #row_1 1 NA -2 78 77 #row_2 2 4 3 56 65 #row_3 NA 1 1 9 11 #row_4 3 67 99 3 172 #row_5 4 -2 NA NA 2 #row_6 10 70 101 146 327 8) Another very useful application that can modify data frames is the result of applying the function merge() that can concatenate two data frames using a common column that serves as an index. For example, we could merge my_data_frame and my_data_frame_1_0. For this purpose, we need to prepare to my_data_frame_1_0 to have the same number of rows and different column names. First, add an extra row to my_data_frame_1_0, so both data frames have the same number of rows with the same row names. my_data_frame_1_0[6,] &lt;- c(NA,NA,NA,NA) rownames(my_data_frame_1_0)[6] &lt;- &quot;row_6&quot; Second, we will change the names of columns of my_data_frame_1_0 so these do not overlap with those of my_data_frame. names_my_data_frame_1_0 &lt;- names(my_data_frame_1_0) new_names_my_data_frame_1_0 &lt;- paste0(names_my_data_frame_1_0, &quot;_as_1_0&quot;) names(my_data_frame_1_0) &lt;- new_names_my_data_frame_1_0 my_data_frame_1_0 # A_as_1_0 B_as_1_0 C_as_1_0 D_as_1_0 #row_1 1 NA 0 1 #row_2 1 1 1 1 #row_3 NA 1 1 1 #row_4 1 1 1 1 #row_5 1 0 NA NA #row_6 NA NA NA NA Third, we will create a common column with same name, i.e., index, in both data frames that we want to merge. In this case using the names of the rows as cell values for the common column named index. my_data_frame$index &lt;- rownames(my_data_frame) my_data_frame_1_0$index &lt;- rownames(my_data_frame_1_0) Finally, we can now merge both data frames using the column named index using merge() with the argument by = 'index'. my_combined_data_frame &lt;- merge(my_data_frame, my_data_frame_1_0, by = &#39;index&#39;) my_combined_data_frame # index A B C D sum_of_my_rows A_as_1_0 B_as_1_0 C_as_1_0 D_as_1_0 #1 row_1 1 NA -2 78 77 1 NA 0 1 #2 row_2 2 4 3 56 65 1 1 1 1 #3 row_3 NA 1 1 9 11 NA 1 1 1 #4 row_4 3 67 99 3 172 1 1 1 1 #5 row_5 4 -2 NA NA 2 1 0 NA NA #6 row_6 10 70 101 146 327 NA NA NA NA "],["getting-biological-data-from-public-repositories.html", "Session 5 – Getting Biological Data from Public Repositories 5.1 NCBI databases: GenBank and others 5.2 Accessing NCBI within R: rentrez 5.3 Retrieving PubMed references 5.4 Comparing sequences: BLAST 5.5 Other databases: UniProt 5.6 Other databases: Ensembl 5.7 Other databases: Taxonomy, phylogenetic trees and species distributions", " Session 5 – Getting Biological Data from Public Repositories 5.1 NCBI databases: GenBank and others 1) The NCBI (National Center for Biotechnology Information) is likely the ultimate source of public information that includes nucleotide sequences (including genomes, transcriptomes) and protein data. The NCBI goal, as is stated, is to advance science and health by providing access to biomedical and genomic information. Therefore, at some point during any your research (if that involves DNA, RNA or proteins), you will visit NCBI to retrieve or deposit molecular data. Likewise, you can access through the NCBI website an extensive literature repository named PubMed. This archive comprises more than 30 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full-text content from PubMed Central and publisher web sites. During this class, we will focus on only few aspects of the NCBI and mostly relevant to retrieve sample data (or your own project data). 2) We can start by exploring data deposited from a poison frog Allobates kingsburyi. For this purpose, we type (or copy-and-paste) the name of this species on the search tab. For this taxon, NCBI will return results found in 6 databases (1 error). In the Literature database, it will return 3 results in the PubMed Central and it will provide link to the PMC database that contains 3 publications that include this species name. In the Genes database, it will return 12 results in the PopSet and it will provide link to the PopSet database that contains a collection sequences associated to this species name. In the Proteins database, it will return 11 results in both the Identical Protein Groups and Protein. The latter will provide link to the Protein database that contains a collection 11 amino acid sequences associated to this species name. In the Genomes database, it will return 17 results in the Nucleotide and it will provide link to the Nucleotide database that contains a collection 17 nucleotide sequences associated to this species name. Likewise, Genomes database will also return 1 result in the Taxonomy and it will provide link to the Taxonomy database that contains the taxonomic link to the species that we are searching Allobates kingsburyi. 3) For a model system like Xenopus laevis, we will get far more results on our NCBI search. In this case, you will get an access to the genome assembly of this species (i.e., Xenopus_laevis_v2). Some other highlights include that this taxon has more than 43,660 associated publications, gene expression omnibus (GEO) has 184K entries, 149K protein entries, ~1.5M nucleotide entries, 2,249 SRA (NGS: sequence read archive) entries, and much more. This example provides the extent of the NCBI as central and extensive archive of biological information. 4) We will explore NCBI nucleotide database to understand how the R-package rentrez can retrieve data as data frames in the R environment. We will collect some acetylcholine receptor CHRNB2 data of poison frogs. For this purpose, we need to retrieve the NCBI accession numbers. Each of these is a unique identifier for a sequence record and it applies to the complete record. These ID numbers are usually a combination of a letter(s) and numbers. Versions on these accession numbers are indicated by a number after a period, e.g., MF580102.1 will be version one of this entry. For our example taxon, we first search Allobates kingsburyi on the NCBI search tab. Next, we click on PopSet under the Genes menu. This will take us to a collection of studies that include sequences of our target species Allobates kingsburyi. We are interested on Hyloidea nicotinic acetylcholine receptor beta-2 (chrnb2) gene, complete cds. This is a phylogenetic study that includes 30 sequences of mostly frogs that include sequences of Allobates kingsburyi. We will click on the link to get the individual accession numbers and information on the associated publication. Notice that the PopSet number of our focal study is 1248341763. However, we can copy single accession numbers (e.g., MF580109.1, MF580108.1, MF580107.1, etc) and can be used to download individually when we use the R-package rentrez. Likewise, you can get the fasta sequences by clickling on the FASTA link. If you selected to see on the browser the FASTA sequences. You can copy and paste the output text in a file to import these sequences into R at a later time. 5.2 Accessing NCBI within R: rentrez 5) The above approach could be tedious and depended on lots of copy-and-paste actions. However, this can be streamlined using rentrez as long as you know what accession numbers you want to download. We will proceed to get all the sequences associated to the corresponding poison frog PopSet 1248341763. For this purpose, we start by installing (if necesseray) or load rentrez in to the R environment. ## We have downloaded this R-package before, so you just need to load it in the R environment. library(rentrez) ## We define our working folder, where we can download our retrieved sequences -- THIS IS EXCLUSIVE FOR YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW setwd(&quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory&quot;) Second, we can get an idea of the databases that rentrez can access from the NCBI using the EUtils API. As mentioned on 1), the NCBI provides extensive documentation for each of their databases. The information about these databases can be easily accessed using the function entrez_dbs() of rentrez. entrez_dbs() # [1] &quot;pubmed&quot; &quot;protein&quot; &quot;nuccore&quot; &quot;ipg&quot; &quot;nucleotide&quot; &quot;structure&quot; # [7] &quot;genome&quot; &quot;annotinfo&quot; &quot;assembly&quot; &quot;bioproject&quot; &quot;biosample&quot; &quot;blastdbinfo&quot; #[13] &quot;books&quot; &quot;cdd&quot; &quot;clinvar&quot; &quot;gap&quot; &quot;gapplus&quot; &quot;grasp&quot; #[19] &quot;dbvar&quot; &quot;gene&quot; &quot;gds&quot; &quot;geoprofiles&quot; &quot;homologene&quot; &quot;medgen&quot; #[25] &quot;mesh&quot; &quot;ncbisearch&quot; &quot;nlmcatalog&quot; &quot;omim&quot; &quot;orgtrack&quot; &quot;pmc&quot; #[31] &quot;popset&quot; &quot;proteinclusters&quot; &quot;pcassay&quot; &quot;protfam&quot; &quot;biosystems&quot; &quot;pccompound&quot; #[37] &quot;pcsubstance&quot; &quot;seqannot&quot; &quot;snp&quot; &quot;sra&quot; &quot;taxonomy&quot; &quot;biocollections&quot; #[43] &quot;gtr&quot; We can access at least 43 databases. To download the acetylcholine receptor dataset, we know that this is a PopSet with a accession number 1248341763. We notice that this database is present as \"popset\" is an output of entrez_dbs() . Moreover, we can explore this database with three functions of rentrez: entrez_db_summary(), entrez_db_searchable() and entrez_db_links(). # Brief description of what the database is entrez_db_summary(&quot;popset&quot;) #DbName: popset #MenuName: PopSet #Description: PopSet sequence record #DbBuild: Build210116-1232m.1 #Count: 356706 #LastUpdate: 2021/01/16 14:57 # Set of search terms that can used with this database entrez_db_searchable(&quot;popset&quot;) #Searchable fields for database &#39;popset&#39; # ALL All terms from all searchable fields # UID Unique number assigned to each sequence # FILT Limits the records # WORD Free text associated with record # TITL Words in definition line # KYWD Nonstandardized terms provided by submitter # AUTH Author(s) of publication # JOUR Journal abbreviation of publication # VOL Volume number of publication # ISS Issue number of publication # PAGE Page number(s) of publication # ORGN Scientific and common names of organism, and all higher levels of taxonomy # ACCN Accession number of sequence # PACC Does not include retired secondary accessions # GENE Name of gene associated with sequence # PROT Name of protein associated with sequence # ECNO EC number for enzyme or CAS registry number # PDAT Date sequence added to GenBank # MDAT Date of last update # SUBS CAS chemical name or MEDLINE Substance Name # PROP Classification by source qualifiers and molecule type # SQID String identifier for sequence # GPRJ BioProject # FKEY Feature annotated on sequence # PCNT Number of proteins in the set # NCNT Number of nucleotides in the set # STRN Strain # ISOL Isolate # CULT Cultivar # BRD Breed # Set of databases that might contain linked records entrez_db_links(&quot;popset&quot;) #Databases with linked records for database &#39;popset&#39; #[1] biocollections bioproject nuccore pmc popset protein pubmed #[8] taxonomy We have the required information to fetch the PopSet 1248341763 in the fasta format. For that purpose we use the function entrez_fetch(). acetylcholine_receptor_data &lt;- entrez_fetch(db = &quot;popset&quot;, id = 1248341763, rettype = &quot;fasta&quot;) acetylcholine_receptor_data #[1] &quot;&gt;MF580080.1 Espadarana callistomma nicotinic acetylcholine receptor beta-2 (chrnb2) gene, complete cds\\nATGACGGTTCTCCTC...&quot; Finally, we can save this as a character vector in a text file that will exactly as we did in 4) using the function write(). name_of_file &lt;- paste0(&quot;acetylcholine_receptor_popset_1248341763.txt&quot;) name_of_file #[1] &quot;acetylcholine_receptor_popset_1248341763.txt&quot; write(acetylcholine_receptor_data, file = name_of_file) 6) It is important to reiterate that rentrez can get us more information associated to our focal dataset: PopSet 1248341763. Here are some interesting explorations on this accession number. We can get the associated data link to the PopSet 1248341763. Using the function entrez_link() my_popset_links &lt;- entrez_link(dbfrom=&#39;popset&#39;, id=1248341763, db=&#39;all&#39;) str(my_popset_links) #List of 2 #$ links:List of 7 # ..$ popset_nuccore : chr [1:30] &quot;1248341821&quot; &quot;1248341819&quot; &quot;1248341817&quot; &quot;1248341815&quot; ... # ..$ popset_pmc : chr &quot;5834227&quot; # ..$ popset_popset : chr [1:3] &quot;1248759970&quot; &quot;1248759962&quot; &quot;1248341823&quot; # ..$ popset_protein : chr [1:30] &quot;1248341822&quot; &quot;1248341820&quot; &quot;1248341818&quot; &quot;1248341816&quot; ... # ..$ popset_pubmed : chr &quot;28935799&quot; # ..$ popset_taxonomy : chr [1:63] &quot;2648769&quot; &quot;2483034&quot; &quot;2023954&quot; &quot;1004470&quot; ... # ..$ popset_taxonomy_tree: chr [1:63] &quot;2648769&quot; &quot;2483034&quot; &quot;2023954&quot; &quot;1004470&quot; ... # ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;elink_classic&quot; &quot;list&quot; #$ file :Classes &#39;XMLInternalElementNode&#39;, &#39;XMLInternalNode&#39;, &#39;XMLAbstractNode&#39; &lt;externalptr&gt; #- attr(*, &quot;content&quot;)= chr &quot; $links: IDs for linked records from NCBI\\n &quot; #- attr(*, &quot;class&quot;)= chr [1:2] &quot;elink&quot; &quot;list&quot; We see that the literature database popset_pmc has one entry and this is likely the publication associated with PopSet 1248341763. We can retrieve this information with the function entrez_fetch(). In this case, we will use a complex notation to retrieve a pmc dataset as my_popset_links$links$popset_pmc. This indicates that with the list named my_popset_links, we want to retrieve the list named links and the element named popset_pmc. When ready, we will use the function entrez_fetch() with the return format as rettype=\"native\". my_popset_links$links$popset_pmc pmc_ids &lt;- my_popset_links$links$popset_pmc pmc_ids #[1] &quot;5834227&quot; my_literature &lt;- entrez_fetch(db = &quot;pmc&quot;, id = pmc_ids, rettype=&quot;native&quot;) my_literature #[1] &quot;1: Interacting Amino Acid Replacements Allow Poison Frogs to Evolve Epibatidine Resistance\\nRebecca D. Tarvin, Cecilia M. Borghese, Wiebke Sachs, Juan C. Santos, Ying Lu, Lauren A. O’Connell, David C. Cannatella, R. Adron Harris, Harold H. Zakon\\nScience. Author manuscript; available in PMC 2018 Mar 22.Published in final edited form as: Science. 2017 Sep 22; 357(6357): 1261–1266. doi: 10.1126/science.aan5061\\nPMCID: PMC5834227\\n\\n&quot; cat(my_literature) #1: Interacting Amino Acid Replacements Allow Poison Frogs to Evolve Epibatidine Resistance #Rebecca D. Tarvin, Cecilia M. Borghese, Wiebke Sachs, Juan C. Santos, Ying Lu, Lauren A. O’Connell, David C. Cannatella, R. Adron Harris, Harold H. Zakon #Science. Author manuscript; available in PMC 2018 Mar 22.Published in final edited form as: Science. 2017 Sep 22; 357(6357): 1261–1266. doi: 10.1126/science.aan5061 #PMCID: PMC5834227 We can also retrieve the amino acid sequence associated with the sequences in our PopSet 1248341763 and save it in a text file using fasta format. my_popset_links$links$popset_protein protein_ids &lt;- my_popset_links$links$popset_protein protein_ids #[[1] &quot;1248341822&quot; &quot;1248341820&quot; &quot;1248341818&quot; &quot;1248341816&quot; &quot;1248341814&quot; &quot;1248341812&quot; &quot;1248341810&quot; &quot;1248341808&quot; # [9] &quot;1248341806&quot; &quot;1248341804&quot; &quot;1248341802&quot; &quot;1248341800&quot; &quot;1248341798&quot; &quot;1248341796&quot; &quot;1248341794&quot; &quot;1248341792&quot; #[17] &quot;1248341790&quot; &quot;1248341788&quot; &quot;1248341786&quot; &quot;1248341784&quot; &quot;1248341782&quot; &quot;1248341780&quot; &quot;1248341778&quot; &quot;1248341776&quot; #[25] &quot;1248341774&quot; &quot;1248341772&quot; &quot;1248341770&quot; &quot;1248341768&quot; &quot;1248341766&quot; &quot;1248341764&quot; my_popset_protein &lt;- entrez_fetch(db = &quot;protein&quot;, id = protein_ids, rettype=&quot;fasta&quot;) my_popset_protein #[1] &quot;&gt;ATG31811.1 nicotinic acetylcholine receptor beta-2, partial [Phyllobates aurotaenia]\\nADGMYEVSFYSNAVVSHDGSIFWLPP...&quot; name_of_file &lt;- paste0(&quot;acetylcholine_receptor_popset_1248341763_protein.txt&quot;) name_of_file #[1] &quot;acetylcholine_receptor_popset_1248341763.txt&quot; write(my_popset_protein, file = name_of_file) 7) To import these data back to R , we need the path to the files that contain the data using the R-package Biostrings. We can import the nucleotide sequences as DNAstrings. ## you might have already installed the library Biostrings library(Biostrings) ## get the path to the file with the nucleotide sequences -- THIS IS EXCLUSIVE FOR YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW my_popset_as_dna_stringset &lt;- readDNAStringSet(filepath = &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/acetylcholine_receptor_popset_1248341763.txt&quot;, format = &quot;fasta&quot;) my_popset_as_dna_stringset # A DNAStringSet instance of length 30 # width seq names # [1] 1506 ATGACGGTTCTCCTCNNNNNNNNNNNNCTCGGTCTGCTCGGCC...ACGTTATTACAGTTGAACCATGCAGCTCCCGCCTCCAACTAA MF580080.1 Espada... # [2] 1506 ATGACGGCTCTCCTCCTCGTCCTGCACCTCAGCCTGATCGGCC...ACGTTACTACAGCTCAACCACGCGATTCCTGCCTCCAACTAG MF580081.1 Boana ... # [3] 1506 ATGACGGTTCTCCTCCTCGTCCTGCACCTGAGCCTGCTCGGCC...ACGTTACTGCAGCTCAACCACGCGGCTCCCGCCTCCAACTGA MF580082.1 Incili... # [4] 1515 ATGACGGTTCTCCTCCTCGTCCTGCACCTCAGCCTGCTCGGCC...CAGCTGAACCACGGGGCTCCCGCCTCCAACTAAAGGGGCGCC MF580083.1 Hyloxa... # [5] 1506 ATGACGGTTCTCCTCCTCGTCCTGCACCTGAGCCTGCTCGGCC...ACGTTACTGCAGCTCAACCACGCAGCTCCCGCCTCCAACTGA MF580084.1 Atelop... # ... ... ... #[26] 1506 ATGGCGGCTCTCCTCNNNCTCCTACACCTCGGCCTGCTCGGCA...ACGTTACTGCAGCTGAACCCCGCAGCTCCCGCCTCTAAGTGA MF580105.1 Gastro... #[27] 134 GCTGATGGGATGTACGAGGTCTCCTTCTACTCCAACGCGGTGG...CGCCTGTAAGATCGAGGTGAAGCACTTTCCGTTCGACCAGCA MF580106.1 Rheoba... #[28] 203 GCTGATGGGATGTATGAGGTCTCCTTCTACTCTAACGCGGTGG...GACTTATGACCGCACTGAGCTGGACCTGGTGCTGAAGAGTGA MF580107.1 Ranito... #[29] 1506 ATGACGGCTCTCCTCCTCGTCCTGCACCTCAGCCTGCTCGGCC...ACGTTAATACAGCTGAACCATGGGACCCCCGCCTCCAACTAA MF580108.1 Phyllo... #[30] 195 GCTGATGGGATGTATGAGGTCTCCTTCTACTCTAACGCGGTGG...CGCTCGTGGACTTATGACCGCACCGAGCTGGACCTGGTGCTG MF580109.1 Phyllo... We can also import the amino acid sequences as AAstrings. # get the path to the file with the amino acid sequences -- THIS IS EXCLUSIVE FOR YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW my_popset_as_aa_stringset &lt;- readAAStringSet(filepath = &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/acetylcholine_receptor_popset_1248341763_protein.txt&quot;, format = &quot;fasta&quot;) my_popset_as_aa_stringset # A AAStringSet instance of length 30 # width seq names # [1] 65 ADGMYEVSFYSNAVVSHDGSIFWLPPAIYKSACKIEVKHFPFDQQNCTMKFRSWTYDRTELDLVL ATG31811.1 nicoti... # [2] 501 MTALLLVLHLSLLGLVTRSMGTDTEERLVEFLLDPSRYNKLIR...LFLWIFVFVCVFGTIGMFLQPLFQNYTSNTLIQLNHGTPASN ATG31810.1 nicoti... # [3] 67 ADGMYEVSFYSNAVVSHDGSIFWLPPAIYKSACKIEVKHFPFDQQNCTMKFRSWTYDRTELDLVLKS ATG31809.1 nicoti... # [4] 44 ADGMYEVSFYSNAVVSHDGSIFWLPPAIYKSACKIEVKHFPFDQ ATG31808.1 nicoti... # [5] 501 MAALLXLLHLGLLGIVSRCLCTDTEERLVEFLLDPSRYNKLIR...LFLWIFIFVCVFGTIGMFLQPLFQNYTTNTLLQLNPAAPASK ATG31807.1 nicoti... # ... ... ... #[26] 501 MTVLLLVLHLSLLGLVTRSLGTDTEERLVEFLLDPSRYNKLIR...LFLWVFVFVCVFGTIGMFLQPLFQNYTSNTLLQLNHAAPASN ATG31786.1 nicoti... #[27] 501 MTVLLLVLHLSLLGLVTRSMGTDTEERLVEFLLDPSRYNKLIR...LFLWIFVFVCVFGTIGMFLQPLFQNYTTNTLLQLNHGAPASN ATG31785.1 nicoti... #[28] 501 MTVLLLVLHLSLLGLVTRSLGTDTEERLVEFLLDPSRYNKLIR...LFLWVFVFVCVFGTIGMFLQPLFQNYTSNTLLQLNHAAPASN ATG31784.1 nicoti... #[29] 501 MTALLLVLHLSLIGLVTRILGTDTEERLVEFLLDSSRYNKLIR...LFLWIFVFVCVFGTIGMFLQPLFQNYTTNTLLQLNHAIPASN ATG31783.1 nicoti... #[30] 501 MTVLLXXXXLGLLGLVTRCLXTDTEERLVEFLLDSSRYNKLIR...LFLWIFVFVCVFGTIGMFLQPLFQNYTTNTLLQLNHAAPASN ATG31782.1 nicoti... 5.3 Retrieving PubMed references We can use rentrez to retrive publication infomration relevant to a term (i.e., text argument) that you can use for citations or references. The main source of such information is PubMed and it claims to include &gt;30 million citations for biomedical literature from MEDLINE, life science journals, and online books. Likewise, such citations may include links to full text content from PubMed Central and publisher web sites. However, from R, you will be able to access the citation, abstract and other identifiers that you can use later on PubMed website. 8) we can use publications relevant to a term of interest, for example, an genus of frogs Ameerega using the following procedure. ## make sure that &#39;rentrez&#39; is loaded require(rentrez) ## retrieve IDs from NCBI genus_publications_IDs &lt;- entrez_search(db=&quot;pubmed&quot;, term=&quot;Ameerega&quot;) str(genus_publications_IDs) #List of 5 # $ ids : chr [1:17] &quot;32230685&quot; &quot;31716726&quot; &quot;31586688&quot; &quot;31125660&quot; ... # $ count : int 17 # $ retmax : int 17 # $ QueryTranslation: chr &quot;Ameerega[All Fields]&quot; # $ file :Classes &#39;XMLInternalDocument&#39;, &#39;XMLAbstractDocument&#39; &lt;externalptr&gt; # - attr(*, &quot;class&quot;)= chr [1:2] &quot;esearch&quot; &quot;list&quot; genus_publications &lt;- entrez_fetch(db=&quot;pubmed&quot;, id=genus_publications_IDs, rettype=&quot;native&quot;) cat(genus_publications) #1. Zootaxa. 2019 Dec 19;4712(2):zootaxa.4712.2.3. doi: 10.11646/zootaxa.4712.2.3. # #Systematics of the Ameerega rubriventris complex (Anura: Dendrobatidae) #with descriptions of two new cryptic species from the East-Andean versant of #Peru. # #Brown JL(1), Siu-Ting K, VON May R, Twomey E, Guillory WX, Deutsch MS, Chávez G. # #Author information: #(1)Department of Zoology, Southern Illinois University, 1125 Lincoln Drive, #Carbondale, IL 62901, USA.. jasonleebrown@gmail.com. # #We describe two new species of poison frog from central and southern Peru that #have been referred to as Ameerega picta, A. hahneli, or A. altamazonica #... We can save these publication refereces into a text file in your working directory. # this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory&quot;) write(genus_publications, &quot;my_genus_publications.txt&quot;) 5.4 Comparing sequences: BLAST One of the most important tools of the NCBI is the Basic Local Alignment Search Tool (BLAST). This tool finds regions of local similarity between sequences by comparing nucleotide or protein sequences to sequence databases and calculates the statistical significance of matches. BLAST can be accessed form the NCBI website or link provided. It also has a friendly guide. Here is a table from the reference manual that provide you with key features of the BLAST search pages in the “Basic BLAST” category guide. Search page Query &amp; database Alignment Programs &amp; Description nucleotide blast nucleotide vs nucleotide nucleotide megablast: for sequence identification, intra-species comparison, discontiguous megablast: for cross-species comparison, searching with coding sequences; blastn: for searching with shorter queries, cross-species comparison protein blast protein vs protein protein blastp: general sequence identification and similarity searches; Quick BLASTP: with a kmer match to accelerate search speed for very similar proteins; DELTA-BLAST[3]: protein similarity search with higher sensitivity than blastp; PSI-BLAST: iterative search for position-specific score matrix (PSSM) to identify distant relatives for a protein family; PHI-BLAST: protein alignment with input pattern as anchor_constraint blastx nucleotide (translated) vs protein protein blastx: for identifying potential protein products encoded by a nucleotide query tblastn protein vs nucleotide (translated) protein tblastn: for identifying database sequences encoding proteins similar to the query tblastx nucleotide (translated) vs nucleotide (translated) protein tblastx: for identifying nucleotide sequences similar to the query based on their coding potential We will focus on nucleotide blast. 9) If you are interested in comparing a nucleotide query against the nucleotide reference collection. You can use [blastn] or standard nucleotide BLAST. For a simple query, you usually start by copying and pasting the nucleotide sequence that you want to BLAST. We will use a short sequence of the MF580102.1 NCBI accession number of the nicotinic acetylcholine receptor beta-2 (chrnb2) from Allobates kingsburyi. &gt;MF580102.1 Allobates kingsburyi nicotinic acetylcholine receptor beta-2 (chrnb2) gene, partial cds ATGACGGTTCTCCTCCTCCTCCTGCACCTCAGCCTGTTCGGCCTGGTCACCAGGAGTATGGGCACGGACA CCGAGGAGCGGCTCGTGGAATTCCTGCTGGACCCGTCCCAGTACAACAAGCTGATCCGGCCCGCCACCAA TGGATCCGAGCAGGTCACCGTCCAGCTGATGGTATCTCTGGCCCAGCTGATCAGCGTGCACGAGCGGGAG CAGATAATGACGACGAATGTCTGGCTCACTCAGGAATGGGANNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNCTGGCTGCCAGACGTCGTCCTGTACAA CAACGCTGATGGGATGTACGAGGTCTCATTCTACTCGAATGCGGTGGTCTCGCATGACGGCAGCATCTTC TGGCTACCCCCCGCCATCTATAAGAGTGCCTGTAAGATTGAGGTGAAGCATTTCCCCTTTGATCAGCAGA ACTGCACCATGAAGTTTCGCTCATGGACKTACGACCGCACGGAGCTGGACCTGGTGCTGAAGAGCGACGT GGCCAGTTTGGATGACTTCACCCCCAGCGGGGAGTGGGACATCATCGCCCTGCCGGGACGCCGGAATGAG AACCCCGAGGACTCCACCTACGTTGACATCACTTACGATTTTATCATCCGCCGCAAGCCGCTGTTCTACA CCATCAACTTGATCATCCCCTGCATCCTCATCACCTCACTGGCAATTCTGGTCTTCTACCTGCCCTCGGA CTGTGGCGAGAAGATGACGCTCTGCATCTCCGTGCTGCTGGCGCTCACCGTCTTCTTACTGCTCATCTCC AAGATTGTGCCCCCGACGTCCCTGGACGTCCCGCTCGTCGGCAAGTACCTGATGTTCAC Notice that you can paste multiple sequences in fasta format. 10) You have several options under Choose Search Set. The first set is to choose a database and the default is Nucleotide collection (nr/nt). However, you have other 15 databases to choose. In most cases, the Nucleotide collection (nr/nt) is the best option. However, the others are useful if looking in reference collections (e.g., refseq_select, refseq_rna, refseq_representative_genomes, etc.) Other dataset are more ameneable for genomic/transcriptomic datasets (e.g., wgs, SRA, TSA) and in most cases they lack annotation (i.e., gene soruce is unknown). Next, you can select the organism taxonomic level that you want to restrict you search. If left blank, it will all references in that database. The lower the taxonomic level (e.g., genus versus family), the less sequences to compare the faster the BLAST will run. In this example, we will select the order Anura (all frogs.) In the program selection, the default is highly similar sequences (megablast). This is the strictest, this approach is intended for comparing a query to closely related sequences and works best if the target percent identity is 95% or more but is very fast. The option somewhat similar sequences (blastn) is slow but allows a word-size down to seven bases (you can recover more matches). Likewise, you can modify the General Parameters from its default values. In most cases before running BLAST, you leave them in their default. For our example, we will select blastn with other parameters in default. Then, we click BLAST. 11) After you submit your query, you might need to wait for seconds to minutes. This time is not consistent and depend on the NCBI servers. Here is the output of our search. Some key information is provided here. The Description will provided the name of the submission that matched your search. In this case, Allobates kingsburyi nicotinic acetylcholine receptor beta-2 (chrnb2) gene, partial cds is a perfect match because we searched this exact sequence. The matched are presented in order from the top as closets matches to the bottom as the worst. The three most important numbers that guide you in your reading of the results are: Query Cover – numbers close to 100% are preferred and 91% indicates that this percentage of the sequence in the query covered the sequence in description; E value – numbers close to 0 are preferred and in our case was 0 so it is an extremely close match and Per. Inden – numbers close to 100% are preferred and in our case is 100% or a perfect match. Other relevant information is Accession or the number in the NCBI that you can retrieve using from R such as using rentrez. 12) If you click on any of the matches, the browser will take you to the alignment. In there, you can find nucleotide sites that matched (i.e., Identities) or not. The reading frame of the query as a Strand (e.g., Plus/Plus is standard; if Plus/Minus one needs to reverse and complement) You can also click on the GenBank link to take you to the accession entry of the reference. In this case, MF580103.1 that is the sequence of Allobates zaparo. 5.5 Other databases: UniProt 13) Other source of important sequence information is the UniProt database, which provides high-quality and freely accessible protein sequences and functional information. These sequences can be retrieved by their entry number using the R-package UniprotR. #install and load the package install.packages(&quot;UniprotR&quot;) library(UniprotR) We can retrieve UniProt sequences of the same gene that presented before the acetylcholine receptor CHRNB2. For this purpose, we need to retrieve the list of entry numbers in the UniProt databases. For example, we will retrieve CHRNB2 from the following taxa: human P17787, mouse Q9ERK7, chicken P09484, Bos taurus A0A3Q1MJN8, Danio rerio E7F4S7, Coelacanth H3B5V5, Anolis H9GMJ0, Xenopus tropicalis A4IIS6. We will use the function GetSequences(). other_CHRNB2_ids &lt;- c(&quot;P17787&quot;, &quot;Q9ERK7&quot;, &quot;P09484&quot;, &quot;A0A3Q1MJN8&quot;, &quot;E7F4S7&quot;, &quot;H3B5V5&quot;, &quot;H9GMJ0&quot;, &quot;A4IIS6&quot;) other_CHRNB2_raw &lt;- UniprotR::GetSequences(other_CHRNB2_ids) class(other_CHRNB2_raw) #[1] &quot;data.frame&quot; # this is a complex data structure and we need the sequences that can be later process as AAStringSet str(other_CHRNB2_raw) #&#39;data.frame&#39;: 8 obs. of 23 variables: #$ Fragment : logi NA NA NA NA NA NA ... #$ Gene.encoded.by : logi NA NA NA NA NA NA ... #$ Alternative.products..isoforms.: logi NA NA NA NA NA NA ... #$ Erroneous.gene.model.prediction: logi NA NA NA NA NA NA ... #$ Erroneous.initiation : logi NA NA NA NA NA NA ... #$ Erroneous.termination : logi NA NA NA NA NA NA ... #$ Erroneous.translation : logi NA NA NA NA NA NA ... #$ Frameshift : logi NA NA NA NA NA NA ... #$ Mass.spectrometry : logi NA NA NA NA NA NA ... #$ Polymorphism : logi NA NA NA NA NA NA ... #$ RNA.editing : logi NA NA NA NA NA NA ... #$ Sequence.caution : logi NA NA NA NA NA NA ... #$ Length : int 502 501 491 509 543 478 509 499 #$ Mass : Factor w/ 8 levels &quot;57,019&quot;,&quot;57,113&quot;,..: 1 2 3 4 5 6 7 8 #$ Sequence : Factor w/ 8 levels &quot;MARRCGPVALLLGFGLLRLCSGVWGTDTEERLVEHLLDPSRYNKLIRPATNGSELVTVQLMVSLAQLISVHEREQIMTTNVWLTQEWEDYRLTWKPEEFDNMKKVRLPSKH&quot;| __truncated__,..: 1 2 3 4 5 6 7 8 #$ Alternative.sequence : logi NA NA NA NA NA NA ... #$ Natural.variant : Factor w/ 1 level &quot;VARIANT 287; /note=V -&gt; L (in ENFL3; dbSNP:rs74315291); /evidence=ECO:0000269|PubMed:11062464; /id=VAR_01271&quot;| __truncated__: 1 NA NA NA NA NA NA NA #$ Non.adjacent.residues : logi NA NA NA NA NA NA ... #$ Non.standard.residue : logi NA NA NA NA NA NA ... #$ Non.terminal.residue : logi NA NA NA NA NA NA ... #$ Sequence.conflict : Factor w/ 2 levels &quot;CONFLICT 26; /note=T -&gt; A (in Ref. 4; CAA05108); /evidence=ECO:0000305; CONFLICT 426; /note=E -&gt; A (in Ref. &quot;| __truncated__,..: 1 2 NA NA NA NA NA NA #$ Sequence.uncertainty : logi NA NA NA NA NA NA ... #$ Version..sequence. : int 1 2 1 1 1 1 1 1 We notice that from the above data frame, we only need the $ Sequence slot. However, this is a factor element and has to be transformed into vector. We will proceed as follows. other_CHRNB2_aa &lt;- other_CHRNB2_raw$Sequence class(other_CHRNB2_aa) #[1] &quot;factor&quot; # we transform to a character vector other_CHRNB2_aa &lt;- as.character(other_CHRNB2_aa) class(other_CHRNB2_aa) other_CHRNB2_aa #[1] &quot;MARRCGPVALLLGFGLLRLCSGVWGTDTEERLVEHLLDPSRYNKLIRPATNGSELVTVQLMVSLAQLISVHEREQIMTTNVWLTQEWEDYRLTWKPEEFDNMKKVRL...&quot; #[2] &quot;MARCSNSMALLFSFGLLWLCSGVLGTDTEERLVEHLLDPSRYNKLIRPATNGSELVTVQLMVSLAQLISVHEREQIMTTNVWLTQEWEDYRLTWKPEDFDNMKKVRL...&quot; #[3] &quot;MALLRVLCLLAALRRSLCTDTEERLVEYLLDPTRYNKLIRPATNGSQLVTVQLMVSLAQLISVHEREQIMTTNVWLTQEWEDYRLTWKPEDFDNMKKVRLPSKHIWL...&quot; We notice that the and UniProt entries are not associated to the sequences and can get these names form the row names from the other_CHRNB2_raw data frame. By assigning such names, we can improve our ability to identify the source of the sequence by adding organism and the starting fasta character &gt;. We can also prepare this object to build a fasta output by adding the corresponding new line character \\n. # get a names character vector names_of_other_CHRNB2_aa &lt;- rownames(other_CHRNB2_raw) names_of_other_CHRNB2_aa #[1] &quot;P17787&quot; &quot;Q9ERK7&quot; &quot;P09484&quot; &quot;A0A3Q1MJN8&quot; &quot;E7F4S7&quot; &quot;H3B5V5&quot; &quot;H9GMJ0&quot; &quot;A4IIS6&quot; names_of_organism &lt;- c(&quot;human&quot;, &quot;mouse&quot;, &quot;chicken&quot;, &quot;Bos_taurus&quot;, &quot;Danio_rerio&quot;, &quot;coelacanth&quot;, &quot;Anolis&quot;, &quot;Xenopus_tropicalis&quot;) # we can combine names final_names &lt;- character() for(i in 1:length(names_of_organism)) { final_names[i] &lt;- paste0(&quot;&gt;&quot;,names_of_organism[i],&quot;_&quot;,names_of_other_CHRNB2_aa[i],&quot;\\n&quot;)} final_names #[1] &quot;&gt;human_P17787\\n&quot; &quot;&gt;mouse_Q9ERK7\\n&quot; &quot;&gt;chicken_P09484\\n&quot; #[4] &quot;&gt;Bos_taurus_A0A3Q1MJN8\\n&quot; &quot;&gt;Danio_rerio_E7F4S7\\n&quot; &quot;&gt;coelacanth_H3B5V5\\n&quot; #[7] &quot;&gt;Anolis_H9GMJ0\\n&quot; &quot;&gt;Xenopus_tropicalis_A4IIS6\\n&quot; We can build the final FASTA output to be written in a text file. By collapsing both vectors with a new line character \\n. end_uniprot_fasta &lt;- paste0(final_names,other_CHRNB2_aa, collapse = &quot;\\n&quot;) end_uniprot_fasta #[1] &quot;&gt;human_P17787\\nMARRCGPVALLLGFGLLRLCSGVWGTDTEERLVEHLLDPSRYNKLIRPATNGSELVTVQLMVSLAQLISVHEREQIMTTNVWLTQEWEDYRLTWKPEEFDNMKKVRL.. cat(end_uniprot_fasta) #&gt;human_P17787 #MARRCGPVALLLGFGLLRLCSGVWGTDTEERLVEHLLDPSRYNKLIRPATNGSELVTVQLMVSLAQLISVHEREQIMTTNVWLTQEWEDYRLTWKPEEFDNMKKVRLPSKHIWLPDVVLYNNADGMY... #&gt;mouse_Q9ERK7 #MARCSNSMALLFSFGLLWLCSGVLGTDTEERLVEHLLDPSRYNKLIRPATNGSELVTVQLMVSLAQLISVHEREQIMTTNVWLTQEWEDYRLTWKPEDFDNMKKVRLPSKHIWLPDVVLYNNADGMY... #&gt;chicken_P09484 #MALLRVLCLLAALRRSLCTDTEERLVEYLLDPTRYNKLIRPATNGSQLVTVQLMVSLAQLISVHEREQIMTTNVWLTQEWEDYRLTWKPEDFDNMKKVRLPSKHIWLPDVVLYNNADGMYEVSFYSN... # we write the output to a text file -- REMEMBER TO SET A PATH TO THE FOLDER THAT YOU WANT IN YOUR COMPUTER name_of_file &lt;- paste0(&quot;acetylcholine_receptor_Uniprot.txt&quot;) name_of_file #[1] &quot;acetylcholine_receptor_popset_1248341763.txt&quot; write(end_uniprot_fasta, file = name_of_file) Finally, we can import this updated file into R using Biostrings. # get the path to the uniprot file with the amino acid sequences -- THIS IS EXCLUSIVE FOR YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW my_uniprot_as_aa_stringset &lt;- readAAStringSet(filepath = &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/acetylcholine_receptor_Uniprot.txt&quot;, format = &quot;fasta&quot;) my_uniprot_as_aa_stringset # A AAStringSet instance of length 8 # width seq names #[1] 502 MARRCGPVALLLGFGLLRLCSGVWGTDTEERLVEHLLDPSRYN...RLFLWIFVFVCVFGTIGMFLQPLFQNYTTTTFLHSDHSAPSSK human_P17787 #[2] 501 MARCSNSMALLFSFGLLWLCSGVLGTDTEERLVEHLLDPSRYN...RLFLWIFVFVCVFGTIGMFLQPLFQNYTATTFLHSDHSAPSSK mouse_Q9ERK7 #[3] 491 MALLRVLCLLAALRRSLCTDTEERLVEYLLDPTRYNKLIRPAT...RLFLWIFVFVCVFGTVGMFLQPLFQNYATNSLLQLGQGTPTSK chicken_P09484 #[4] 509 MAWLSGPKAMLLSFGLLGLCSGVWGTDTEERLVEHLLDPSRYN...FVCVFGTIGMFLQPLFQNYATATFLHADHSAPSSKCVLSPPEI Bos_taurus_A0A3Q1... #[5] 543 MMALWTLFCILAIVKSGYGADTEERLVEHLLNPAHYNKLIRPA...VAMVIDRLFLWIFVFVCVFGTIGMFLQPLFQNYTAKTITHTPG Danio_rerio_E7F4S7 #[6] 478 SLAMDTEERLVGHLLNPAHYNKLIRPATNRSEVVTVQLMVSLA...MVIDRLFLWIFVFVCIFGTLGMFLQPVFQNSSFDSLPQKTNAA coelacanth_H3B5V5 #[7] 509 CAASSHFAACPASGSLLGLSAGVLGTDTEERLVEHLLDPLRYN...RLFLWIFVFVCVFGTIGMFLQPLFQNYATNSLLQIHQGAPGSK Anolis_H9GMJ0 #[8] 499 MIRTGMAPLLAALYLLLGLLPGCLGTDTEERLVEHLLDPSRYN...VIDRLFLWIFVFVCVFGTIGMFLQPLFQNYTTNALVHMNHAAN Xenopus_tropicali... 14) We can also append both AAStringSets: my_popset_as_aa_stringset and my_uniprot_as_aa_stringset for further downstream analyses with the function c(). all_aa_stringset &lt;- c(my_popset_as_aa_stringset, my_uniprot_as_aa_stringset) all_aa_stringset # A AAStringSet instance of length 38 # width seq names # [1] 65 ADGMYEVSFYSNAVVSHDGSIFWLPPAIYKSACKIEVKHFPFDQQNCTMKFRSWTYDRTELDLVL ATG31811.1 nicoti... # [2] 501 MTALLLVLHLSLLGLVTRSMGTDTEERLVEFLLDPSRYNKLIR...LFLWIFVFVCVFGTIGMFLQPLFQNYTSNTLIQLNHGTPASN ATG31810.1 nicoti... # [3] 67 ADGMYEVSFYSNAVVSHDGSIFWLPPAIYKSACKIEVKHFPFDQQNCTMKFRSWTYDRTELDLVLKS ATG31809.1 nicoti... # [4] 44 ADGMYEVSFYSNAVVSHDGSIFWLPPAIYKSACKIEVKHFPFDQ ATG31808.1 nicoti... # [5] 501 MAALLXLLHLGLLGIVSRCLCTDTEERLVEFLLDPSRYNKLIR...LFLWIFIFVCVFGTIGMFLQPLFQNYTTNTLLQLNPAAPASK ATG31807.1 nicoti... # ... ... ... #[34] 509 MAWLSGPKAMLLSFGLLGLCSGVWGTDTEERLVEHLLDPSRYN...VCVFGTIGMFLQPLFQNYATATFLHADHSAPSSKCVLSPPEI Bos_taurus_A0A3Q1... #[35] 543 MMALWTLFCILAIVKSGYGADTEERLVEHLLNPAHYNKLIRPA...AMVIDRLFLWIFVFVCVFGTIGMFLQPLFQNYTAKTITHTPG Danio_rerio_E7F4S7 #[36] 478 SLAMDTEERLVGHLLNPAHYNKLIRPATNRSEVVTVQLMVSLA...VIDRLFLWIFVFVCIFGTLGMFLQPVFQNSSFDSLPQKTNAA coelacanth_H3B5V5 #[37] 509 CAASSHFAACPASGSLLGLSAGVLGTDTEERLVEHLLDPLRYN...LFLWIFVFVCVFGTIGMFLQPLFQNYATNSLLQIHQGAPGSK Anolis_H9GMJ0 #[38] 499 MIRTGMAPLLAALYLLLGLLPGCLGTDTEERLVEHLLDPSRYN...IDRLFLWIFVFVCVFGTIGMFLQPLFQNYTTNALVHMNHAAN Xenopus_tropicali... 5.6 Other databases: Ensembl Ensembl is an online archive for vertebrate genomes that are used in comparative genomics, evolution, sequence variation and transcriptional regulation. Ensembl is quite extensive and convoluted to access and export data. The R-package biomaRt provides a somehow easy access to datasets present or crosslinked with Ensembl. 15) We will illustrate how to access the acetylcholine receptor CHRNB2 sequence in the genome of Xenopus tropicalis frog model system. First, you might need to install biomaRt from Bioconductor. BiocManager::install(&quot;biomaRt&quot;) library(biomaRt) We can get a list to which biomaRt can connect using the function listMarts(). listMarts() # biomart version #1 ENSEMBL_MART_ENSEMBL Ensembl Genes 102 #2 ENSEMBL_MART_MOUSE Mouse strains 102 #3 ENSEMBL_MART_SNP Ensembl Variation 102 #4 ENSEMBL_MART_FUNCGEN Ensembl Regulation 102 We can also type ensembl using the function useMart() to access all accessible databases. mart &lt;- useMart(&#39;ensembl&#39;) # this will give you at least 203 dataset to search listDatasets(mart) # dataset description version #1 acalliptera_gene_ensembl Eastern happy genes (fAstCal1.2) fAstCal1.2 #2 acarolinensis_gene_ensembl Anole lizard genes (AnoCar2.0) AnoCar2.0 #3 acchrysaetos_gene_ensembl Golden eagle genes (bAquChr1.2) bAquChr1.2 #4 acitrinellus_gene_ensembl Midas cichlid genes (Midas_v5) Midas_v5 #5 amelanoleuca_gene_ensembl Panda genes (ailMel1) ailMel1 #... For our example, we will use the Xenopus tropicalis dataset named in the list above as \"xtropicalis_gene_ensembl\". However, we need to be more specific how we want to search that database. For that purpose, we can use the function listFilters() to list the filters available in our selected dataset. xenopus_mart &lt;- useMart(&quot;ensembl&quot;, dataset=&quot;xtropicalis_gene_ensembl&quot;) listFilters(xenopus_mart) # name description #1 chromosome_name Chromosome/scaffold name #2 start Start #3 end End #4 strand Strand #5 chromosomal_region e.g. 1:100:10000:-1, 1:100000:200000:1 #6 with_entrezgene_trans_name With EntrezGene transcript name ID(s) #7 with_embl With European Nucleotide Archive ID(s) #8 with_arrayexpress With Expression Atlas ID(s) #9 with_go With GO ID(s) #... We will retrieve based on the name of gene in UniProt (i.e., CHRNB2) so two filter seem to be useful based on the description column hgnc_symbol and uniprot_gn_symbol. We will try to retrieve such gene sequence using the function getSequence(). xenopus_CHRNB2_seq_hgnc_symbol &lt;- getSequence(id = &quot;chrnb2&quot;, type = &quot;uniprot_gn_symbol&quot;, seqType = &quot;peptide&quot;, mart = xenopus_mart) xenopus_CHRNB2_seq_uniprot_id &lt;- getSequence(id = &quot;A4IIS6&quot;, type = &quot;uniprot_gn_id&quot;, seqType = &quot;peptide&quot;, mart = xenopus_mart, verbose = TRUE) show(xenopus_CHRNB2_seq_uniprot_id) 5.7 Other databases: Taxonomy, phylogenetic trees and species distributions Accessing other biological repositories through R is in some cases a hit-and-miss. Some with stand and be reliable, but the depend on the source database. Here is a list of some R-packages and the databases that you can access. Most of these will have vignettes for you to follow: taxize – Taxonomic Information from Around the Web. BIEN – Tools for Accessing the Botanical Information and Ecology Network Database. rotl – Interface to the ‘Open Tree of Life’ API. rgbif – Interface to the Global ‘Biodiversity’ Information Facility API. rvertnet – Search ‘Vertnet,’ a ‘Database’ of Vertebrate Specimen Records. spocc – Interface to Species Occurrence Data Sources. "],["basic-statistics-in-r.html", "Session 6 – Basic Statistics in R 6.1 Exploring your data 6.2 Normalizing your data 6.3 What test should I use? 6.4 Comparing means: t-test 6.5 Beyond t-test: Wilcoxon signed rank test 6.6 Beyond t-test: Binomial test 6.7 Beyond t-test: Chi-square goodness of fit 6.8 Beyond t-test: Fisher’s exact test 6.9 Correlations 6.10 Comparing distributions", " Session 6 – Basic Statistics in R This workshop is not designed to give you an extensive overview on all statistical methods that can be performed in R. Yet, this computing environment was born to provide a comprehensive platform for the implementation of statistical methods. We will overview of some basic and advanced methods with some illustrative graphs (these will be expanded in the session devoted to graphics). However, you can ask me if you want to explore a specific method that you would me to add to this gitbook. 6.1 Exploring your data 1) To start, we need to import a dataset that we want to explore and analyze. As graduate students, you are encouraged to use your own datasets (i.e., this is ideal as you will know more about its contents, metrics and intended use). For this exercise, I will use two previously show datasets: (1) the small preloaded mtcars dataset and (2) the large airway_scaledcounts.csv dataset. These datasets are in our class GitHub repository and you can donwload those to your computer. ## NOTE: remember to update the path to files with the datasets where you downloaded in your computer -- THIS IS EXCLUSIVE TO YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW #define a working directory setwd(&quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory&quot;) #load get mtcars dataset -- this a preloaded dataset in R cars_data &lt;- mtcars cars_data # mpg cyl disp hp drat wt qsec vs am gear carb #Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 #Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 #Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 #... #load get &#39;airway_scaledcounts.csv&#39; dataset airway_data &lt;- read.table(&quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/airway_scaledcounts.csv&quot;, header = TRUE, sep = &quot;,&quot;, stringsAsFactors = FALSE) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 ENSG00000000003 723 486 904 445 1170 1097 806 604 #2 ENSG00000000005 0 0 0 0 0 0 0 0 #3 ENSG00000000419 467 523 616 371 582 781 417 509 #4 ENSG00000000457 347 258 364 237 318 447 330 324 #5 ENSG00000000460 96 81 73 66 118 94 102 74 #6 ENSG00000000938 0 0 1 0 2 0 0 0 #... 2) We can get some basic descriptive parameters of each data set including number of entries, number and names of the columns (i.e., variables), type of variables (e.g., numbers or characters), number of NA values. For this purpose, the easiest approach is to use the function summary() summary(cars_data) # mpg cyl disp hp drat wt qsec vs # Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 # 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 # Median :19.20 Median :6.000 Median :196.3 Median :123.0 Median :3.695 Median :3.325 Median :17.71 Median :0.0000 # Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 # 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 # Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 # am gear carb # Min. :0.0000 Min. :3.000 Min. :1.000 # 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 # Median :0.0000 Median :4.000 Median :2.000 # Mean :0.4062 Mean :3.688 Mean :2.812 # 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 # Max. :1.0000 Max. :5.000 Max. :8.000 summary(airway_data) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 # Length:38694 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0 # Class :character 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0 # Mode :character Median : 1.0 Median : 1.0 Median : 1.0 Median : 0.0 Median : 1 # Mean : 546.5 Mean : 501.1 Mean : 673.6 Mean : 405.3 Mean : 649 # 3rd Qu.: 203.0 3rd Qu.: 172.0 3rd Qu.: 237.0 3rd Qu.: 128.0 3rd Qu.: 228 # Max. :287372.0 Max. :244921.0 Max. :510107.0 Max. :304818.0 Max. :410979 # SRR1039517 SRR1039520 SRR1039521 # Min. : 0.0 Min. : 0.0 Min. : 0.0 # 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 # Median : 1.0 Median : 1.0 Median : 1.0 # Mean : 822.5 Mean : 509.0 Mean : 565.7 # 3rd Qu.: 261.0 3rd Qu.: 185.8 3rd Qu.: 183.0 # Max. :416837.0 Max. :378108.0 Max. :372973.0 These summary output provide a general descriptive and summary statistics of data distribution of the values on each column (variable). For continuous variables, minimum and maximum values are provided. Other statistics include mean and median that provide a general measurement of central location (i.e., average). The 1st quartile (Q1) is the middle value between the minimum and the median; and it also represents measurement that marks where 25% of the data is below this point. The 3rd quartile (Q3) is the middle value between the median and the maximum; and also represents 75% of the data is below this point. Notice the column named ensgene is a discrete (character) variable and only provides the number of elements in that column. We can visualize these summaries using boxplots. The base package has the function boxplot(). #car data boxplot boxplot(mpg~cyl, data=cars_data, col=(c(&quot;#DA291CFF&quot;, &quot;#56A8CBFF&quot;, &quot;#53A567FF&quot;)), main=&quot;car data boxplots&quot;, xlab=&quot;cylinders&quot;) You can also visualize these summaries using boxplots for all variables using the R-package reshape2 and its function melt(). This will reshape the data frame to be more amenable for boxplots. # you might need to install R-package &#39;reshape2&#39; install.packages(&quot;reshape2&quot;) library(reshape2) # reduce the &quot;airway_data&quot; to only the continuous variables. airway_data_reduced2 &lt;- airway_data[,2:ncol(airway_data_very_high_low)] head(airway_data_reduced2) # SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 723 486 904 445 1170 1097 806 604 #2 0 0 0 0 0 0 0 0 #3 467 523 616 371 582 781 417 509 #4 347 258 364 237 318 447 330 324 #5 96 81 73 66 118 94 102 74 #6 0 0 1 0 2 0 0 0 airway_data_reduced2_melted &lt;- melt(airway_data_reduced2) #No id variables; using all as measure variables # we will change the names of the columns to reflect what they represent. names(airway_data_reduced2_melted) &lt;- c(&quot;sample&quot;, &quot;expression&quot;) head(airway_data_reduced2_melted) # sample expression #1 SRR1039508 723 #2 SRR1039508 0 #3 SRR1039508 467 #4 SRR1039508 347 #5 SRR1039508 96 #6 SRR1039508 0 #airway_data boxplots boxplot(expression~sample, data=airway_data_reduced2_melted, main=&quot;airway_data boxplots&quot;, xlab=&quot;samples&quot;) This figure is not very elegant (we will work with graph in that section). However, these boxplots are extremely skewed to low expression values. 3) We can discretize our data by creating groups based on criterium (e.g., threshold value). Such discrete groups can help us to define if a treatment or condition is reflected in our data or experimental groups. We will create a binary data frame for airway_data for the columns (variables) that represent the normalized gene expression for the samples that start withSRR. For this example, we will consider an arbitrary scale to divide into groups as: very high (x =&gt; 10000), high (10000 &gt; x =&gt; 100) and low (x &lt; 100) expression levels. This task can be perform using a specialized R-package: purrr and its function modify() with a conditional ifelse(). We already introduce purrr on previous sessions. Notice that when we close some of the commands we use complex punctuation (i.e., (, ), {, }) and many mistakes can happen if you forget to close such punctuation pairs (i.e., missing one is a very common error).. # we already installed R-package &#39;purrr&#39; library(purrr) # we create an alternative object, so we original raw data airway_data airway_data_very_high_low &lt;- airway_data # We will apply a the conditional airway_data_very_high_low &lt;- purrr::modify(airway_data_very_high_low,function(x) {ifelse(x &gt;= 10000, &quot;very_high&quot;, ifelse( x &gt;= 1000, &quot;high&quot;, &quot;low&quot;))}) head(airway_data_very_high_low) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 very_high low low low low high high low low #2 very_high low low low low low low low low #3 very_high low low low low low low low low #4 very_high low low low low low low low low #5 very_high low low low low low low low low #6 very_high low low low low low low low low #... During the process above, we have “lost” the values of our variable ensgene that contains the names of the loci. We can add it back from our original data frame airway_data as indicated next. airway_data_very_high_low$ensgene &lt;- airway_data$ensgene head(airway_data_very_high_low) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 ENSG00000000003 low low low low high high low low #2 ENSG00000000005 low low low low low low low low #3 ENSG00000000419 low low low low low low low low #4 ENSG00000000457 low low low low low low low low #5 ENSG00000000460 low low low low low low low low #6 ENSG00000000938 low low low low low low low low #... 4) With this discretized data frame airway_data_very_high_low, we can perform contingency tables. These are a type of table in a matrix format that displays the frequency distribution of the variables tested. We will exclude the first column ensgene because it has unique values. airway_data_reduced &lt;- airway_data_very_high_low[,2:ncol(airway_data_very_high_low)] # collect frequencies on samples summary_frequencies_airway_data &lt;- list() for(i in 1:ncol(airway_data_reduced)) { sample_name &lt;- names(airway_data_reduced[i]) one_table &lt;- table(airway_data_reduced[i]) # from table to vector one_contigency_vector &lt;- as.numeric(one_table) # add names of categories names(one_contigency_vector) &lt;- names(one_table) one_data_frame &lt;- as.data.frame(one_contigency_vector) # add name of sample names(one_data_frame) &lt;- sample_name # collect results in list summary_frequencies_airway_data[[i]] &lt;- one_data_frame } summary_frequencies_airway_data #[[1]] # SRR1039508 #high 3732 #low 34691 #very_high 271 #[[2]] # SRR1039509 #high 3444 #low 34986 #very_high 264 #... With the list summary_frequencies_airway_data, we can cover it to a data frame by binding each vector as a column using two functions do.call() and cbind(). final_summary_all_samples_df &lt;- do.call(cbind, summary_frequencies_airway_data) final_summary_all_samples_df # SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #high 3732 3444 4118 2607 4155 4703 3432 3683 #low 34691 34986 34216 35902 34196 33500 35024 34725 #very_high 271 264 360 185 343 491 238 286 5) With this data frame, we can test these discretized variables for independence using a chi-squared test with the function summary(). For example, we can compare two of the airway_data_reduced samples (i.e., columns). Notice that I use a ; command at the end of one line of commands, this allows you to paste several commands next to each other as if they were in new lines. sample_names &lt;- names(airway_data_reduced)[1:2] title_test &lt;- paste0(sample_names, collapse =&quot;..vs..&quot;) contigency_result &lt;- summary(table(airway_data_reduced[,1], airway_data_reduced[,2])) cat(title_test, &quot;\\n&quot;); print(contigency_result) #SRR1039508..vs..SRR1039509 #ENumber of cases in table: 38694 #Number of factors: 2 #Test for independence of all factors: # Chisq = 58193, df = 4, p-value = 0 # Chi-squared approximation may be incorrect These results indicate that we can conclude that these samples are likely independent, but the large number of cases 38694 might have inflated the Chisq statistic. Next we have an interesting loop for you to dissect, it will run all pairwise contingency tests, report the process, and add names of the pairs being compared. Notice the iterative variable counter and function break that breaks the loop when it has finished. counter &lt;- 0 collect_test_independence &lt;- list() for(i in 1:ncol(airway_data_reduced)) { # i &lt;- 1 names_sample_i &lt;- names(airway_data_reduced)[i] n &lt;- i + 1 upper_limit &lt;- ncol(airway_data_reduced)+1 if(n &lt; upper_limit) { cat(&quot;sample:&quot;,names_sample_i, &quot;..vs..&quot;) for(j in n:ncol(airway_data_reduced)) { counter &lt;- counter + 1 names_sample_j &lt;- names(airway_data_reduced)[j] cat(names_sample_j, &quot;....&quot;) list_name &lt;- paste0(names_sample_i,&quot;..vs..&quot;, names_sample_j) collect_test_independence[[counter]] &lt;- summary(table(airway_data_reduced[,i], airway_data_reduced[,j])) names(collect_test_independence)[counter] &lt;- list_name } cat(&quot;\\n&quot;) } else {break} } #sample: SRR1039508 ..vs..SRR1039509 ....SRR1039512 ....SRR1039513 ....SRR1039516 ....SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039509 ..vs..SRR1039512 ....SRR1039513 ....SRR1039516 ....SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039512 ..vs..SRR1039513 ....SRR1039516 ....SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039513 ..vs..SRR1039516 ....SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039516 ..vs..SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039517 ..vs..SRR1039520 ....SRR1039521 .... #sample: SRR1039520 ..vs..SRR1039521 .... # if you call the collect_test_independence, you will get the statistics for independence collect_test_independence #$SRR1039508..vs..SRR1039509 #Number of cases in table: 38694 #Number of factors: 2 #Test for independence of all factors: # Chisq = 58193, df = 4, p-value = 0 # Chi-squared approximation may be incorrect # #$SRR1039508..vs..SRR1039512 #Number of cases in table: 38694 #Number of factors: 2 #Test for independence of all factors: # Chisq = 58274, df = 4, p-value = 0 # Chi-squared approximation may be incorrect 6.2 Normalizing your data 6) Normalizing or scaling your data is sometimes useful when you are trying to compare datasets that might have extremely different units (e.g., tiny fractions and others in the hundreds of thousands). This can be done with the function scale(). head(cars_data) # mpg cyl disp hp drat wt qsec vs am gear carb #Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 #Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 #Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 #Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 #Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 cars_data_scaled &lt;- purrr::modify(cars_data, scale) # head(cars_data_scaled) # mpg cyl disp hp drat wt qsec vs am gear carb #Mazda RX4 0.1508848 -0.1049878 -0.57061982 -0.5350928 0.5675137 -0.610399567 -0.7771651 -0.8680278 1.1899014 0.4235542 0.7352031 #Mazda RX4 Wag 0.1508848 -0.1049878 -0.57061982 -0.5350928 0.5675137 -0.349785269 -0.4637808 -0.8680278 1.1899014 0.4235542 0.7352031 #Datsun 710 0.4495434 -1.2248578 -0.99018209 -0.7830405 0.4739996 -0.917004624 0.4260068 1.1160357 1.1899014 0.4235542 -1.1221521 #Hornet 4 Drive 0.2172534 -0.1049878 0.22009369 -0.5350928 -0.9661175 -0.002299538 0.8904872 1.1160357 -0.8141431 -0.9318192 -1.1221521 #Hornet Sportabout -0.2307345 1.0148821 1.04308123 0.4129422 -0.8351978 0.227654255 -0.4637808 -0.8680278 -0.8141431 -0.9318192 -0.5030337 #Valiant -0.3302874 -0.1049878 -0.04616698 -0.6080186 -1.5646078 0.248094592 1.3269868 1.1160357 -0.8141431 -0.9318192 -1.1221521 7) Common statistical tests are parametric and most of them assume that your data has a normal (Gaussian) distribution, i.e., a typical bell-shaped curve. In other words, the distribution of values of variable that has a normal distribution will produce values (as they are being measured) with an equal chance to fall above and below the mean value of all measurements. The normal distribution is expected to have very close values for its mean (average of all values), median (mid-point value at the center of the distribution) and mode (the most frequent value). If a distribution is not normal, these three parameters will differ and the distribution is considered skewed (i.e., not normal). # Create a sequence of numbers between 0 and 100 by 1 x &lt;- seq(0, 100, by = 1) # Choose the mean as 50 and standard deviation as 17.5 y &lt;- dnorm(x, mean = 50, sd = 17.5) # Plot this distribution plot(x,y) 8) We can test for normal (Gaussian) distribution of our data using a normality test. We can use the Shapiro-Wilk’s test that determines if the null hypothesis (i.e., the sample has normal distribution) versus the alternative that it does not. If the test is significant, the distribution has a non-normal distribution. This test is sensitive to sample size, which means that large samples most often do not pass normality tests (i.e., show to be significant). Therefore, it’s important that you use visual inspection of your data distribution before you decide to transform your data. The Shapiro-Wilk’s test is a build-in function and can be call as shapiro.test(). However, this test cannot be applied to large vectors (i.e., &gt; 5000 entries). # we can perfome in a variable of a data frame applying it as vector shapiro.test(cars_data$hp) # Shapiro-Wilk normality test # #data: cars_data$hp #W = 0.93342, p-value = 0.04881 shapiro.test(cars_data$qsec) # Shapiro-Wilk normality test # #data: cars_data$qsec #W = 0.97325, p-value = 0.5935 Based on these results, we can conclude that the variable hp has not normal distribution, while qsec has normal one. If you want to apply the shapiro.test() to all you variables in your data frame use apply with 2 argument for columns. apply(cars_data,2,shapiro.test) 9) In most cases, you might like to apply a transformation to your data before you apply a parametric statistical test. One of the most frequent transformations for continuous data is to normalize them, this means that change the distribution of the data to a quasinormal or normal one. Such transformations will make our data more amenable for parametric tests. However, the interpretation of the results has to be framed using the units of the transformed data (i.e., if log-transformed, you have to interpret in log-transformed unit). To determine how if your data need to be normalized, it is a good idea to visualize its distribution. To exemplify this, we will describe this using the hp variable of the car_data. hp_vector &lt;- cars_data$hp hp_vector #[1] 110 110 93 110 175 105 245 62 95 123 123 180 180 180 205 215 230 66 52 65 97 150 150 245 175 66 91 113 264 175 335 109 # we test for normality shapiro.test(hp_vector) # Shapiro-Wilk normality test # #data: cars_data$hp #W = 0.93342, p-value = 0.04881 boxplot(hp_vector, xlab = &quot;hp&quot;, ylab = &quot;values&quot;) In this boxplot, we can see an outlier and it is skewed towards lower values of hp. Some of the most common transformations are to square root the data using the function sqrt() and to perform logarithms using log10() with base 10 or natural logarithms ‘log().’ hp_vector_sqrt &lt;- sqrt(hp_vector) shapiro.test(hp_vector_sqrt) #Shapiro-Wilk normality test # #data: hp_vector_sqrt #W = 0.96456, p-value = 0.3642 hp_vector_log10 &lt;- log10(hp_vector) shapiro.test(hp_vector_log10) # Shapiro-Wilk normality test # #data: hp_vector_log10 #W = 0.97026, p-value = 0.5065 boxplot(hp_vector_log10, xlab = &quot;hp&quot;, ylab = &quot;values_transformed_by_log10&quot;) Here is the resulting boxplot afterlog10() transformation. In some extreme cases, there are several R-packages that can help you to transform your data to have a normal distribution. One of these is bestNormalize and its function boxcox(). This provides a Box-Cox power transformation that can be useful for extremely skewed distributions. Likewise, this function returns, by default, transformed values that are also centered and scaled. # if you need to install the R-package bestNormalize install.packages(&quot;bestNormalize&quot;) # load library library(bestNormalize) # perform Box-Cox power tranformation. hp_vector_boxcox_list &lt;- bestNormalize::boxcox(hp_vector) hp_vector_boxcox_list #Standardized Box Cox Transformation with 32 nonmissing obs.: # Estimated statistics: # - lambda = 0.0940865 # - mean (before standardization) = 6.21272 # - sd (before standardization) = 0.7515468 str(hp_vector_boxcox_list) #List of 8 # $ x.t : num [1:32] -0.401 -0.401 -0.745 -0.401 0.582 ... #$ x : num [1:32] 110 110 93 110 175 105 245 62 95 123 ... #$ mean : num 6.21 #$ sd : num 0.752 #$ lambda : num 0.0941 #$ n : int 32 #$ norm_stat : num 1.4 #$ standardize: logi TRUE #- attr(*, &quot;class&quot;)= chr [1:2] &quot;boxcox&quot; &quot;list&quot; hp_vector_boxcox &lt;- hp_vector_boxcox_list$x.t #(1] -0.4005863 -0.4005863 -0.7454816 -0.4005863 0.5821478 -0.4967035 1.3216259 -1.5563448 -0.7020701 -0.1680641 -0.1680641 0.6431660 0.6431660 0.6431660 0.9269679 #[16] 1.0317736 1.1809841 -1.4333227 -1.8985905 -1.4634315 -0.6594788 0.2511055 0.2511055 1.3216259 0.5821478 -1.4333227 -0.7897473 -0.3447992 1.4889763 0.5821478 #[31] 2.0305623 -0.4194885 boxplot(hp_vector_boxcox, xlab = &quot;hp&quot;, ylab = &quot;values_transformed_by_boxcox&quot;) 6.3 What test should I use? The task of choosing the best statistical analyses given your dataset are one of the most important steps for any biological research. Your choice of methods depends on your understanding of your dataset and these include the following: (a) the number of dependent or outcome variables; (b) the number of your independent or predictor variables; (c) the characteristics of your dependent and independent variables, namely whether they are continuous (interval or ratio) variables (e.g., temperature, growth area, enzyme activity, dose amount, reaction rate, concentration, weight, length); (d) they are ordinal variables where order matters but differences between adjacent categories do not necessarily have the same meaning such as in treatment response (“low,” ”middle”, ”high”) or location (“understory,” “canopy”); (e) they are categorical variables if the categories that do not have a natural order or ranking such in colors (e.g., red, blue, green), populations (e.g., A, B, C, D); and (f) the type of distribution of the variables (it is normally distributed so parametric methods could be applied or not). The table of statistical choices has been modified form here. Dependents (Number) Dependents (Type) Independents (Number) Independents (Type) Test Section 1 &amp; one population continuous (normal) N/A N/A one-sample t-test Session 6 9 1 &amp; one population continuous (non-normal); ordinal N/A N/A one-sample median or Wilcoxon signed rank test Session 6 11 1 &amp; one population categorical (2 categories) N/A N/A binomial test Session 6 12 1 &amp; one population categorical (multiple categories) N/A N/A Chi-square goodness-of-fit Session 6 13 1 &amp; two populations continuous (normal) N/A N/A two-sample t-test Session 6 10 1 &amp; two populations continuous (non-normal); ordinal N/A N/A two-sample median or Wilcoxon signed rank test Session 6 11 1 &amp; two populations categorical (multiple categories) compare agreements N/A N/A Chi-square goodness-of-fit Session 6 14 1 &amp; two populations categorical (multiple categories) N/A N/A Fisher’s exact test Session 6 15 1 &amp; one population continuous (normal) 1 categorical (2 or more categories) one-way ANOVA Session 7 12 1 &amp; one population continuous (non-normal); ordinal 1 categorical (2 or more categories) one-way ANOVA Session 7 15 1 &amp; one population continuous (normal) 1 continuous (normal) correlation Session 7 12 1 &amp; one population continuous (normal) 1 continuous (normal) simple linear regression Session 7 1 1 &amp; one population continuous (normal) many continuous (normal) multiple linear regression Session 7 3 1 &amp; one population continuous 1 continuous or ordinal non-parametric correlation Session 7 10 1 &amp; one population ordinal 1 continuous or ordinal simple logistic regression Session 7 12 Here is another chart about selecting a statistial test for your project. The original source is in here. 6.4 Comparing means: t-test 9) We can perform one sample t-test on vectors using the function t.test(). For this test, you can test if its mean is statistically different from a given value specified by the mu argument equal such value. # we will test the mean of sample SRR1039508 equal to mu = 546.5 for names(cars_data)[7] [1] &quot;qsec&quot; t.test(cars_data$qsec, mu = 11) # # One Sample t-test # #data: cars_data$qsec #t = 21.681, df = 31, p-value &lt; 2.2e-16 #alternative hypothesis: true mean is not equal to 11 #95 percent confidence interval: # 17.20449 18.49301 #sample estimates: #mean of x #17.84875 # we will test the mean of sample SRR1039508 equal to mu = 546.5 for names(airway_data)[2] #[1] &quot;SRR1039508&quot; t.test(airway_data[,2], mu = 546.5) # # One Sample t-test # #data: airway_data[, 2] #t = -0.00053579, df = 38693, p-value = 0.9996 #alternative hypothesis: true mean is not equal to 546.5 #95 percent confidence interval: # 501.7696 591.2060 #sample estimates: #mean of x # 546.4878 These result indicate that the mean of qsec from the cars_data is statically different form 11, while the mean for sample SRR1039508 from the airway_data is not different from the value 546.6 (this value was derived from the summary see 2)). One sample t-test does not assume that the population is normally distributed. However, it is better to have a normalized distribution in our data when we apply parametric test like this one. The output of the function t.test() also provides 95% confidence interval of the mean value. In these examples, qsec has a mean 17.84875 with 95CI (17.20449, 18.49301) and the SRR1039508 sample mean is 546.4878 with 95CI (501.7696, 591.2060). Our interpretation is that the true value for the mean of the measurements of each of these variables is in the proposed range with a 95% confidence level. 10) For two samples, we can also perform t-tests with two vectors using the same function t.test(). In this case, you are testing if two samples each from different populations (e.g., control and experimental) have the same mean. If the test is significant, you can conclude that the difference might derive from a treatment (as long as you have controlled for other causal agents). These t-tests can be consider as paired (i.e., argument paired = TRUE) or unpaired (i.e., argument paired = FALSE). For paired, the t-test will compare the means of the same group under two separate conditions. In contrast, an unpaired t-test compares the means of two independent groups. In an unpaired t-test, the variance between groups is assumed to be equal. We will test this for airway_data data that have several samples from the same group the argument paired = TRUE. names_pairs &lt;- names(airway_data)[2:3] names_pairs &lt;- paste0(names_pairs, collapse = &quot;..vs..&quot;) #[1] &quot;SRR1039508..vs..SRR1039509&quot; out_put_ttest_paired &lt;- t.test(airway_data[,2], airway_data[,3], paired = TRUE) cat(names_pairs); print(out_put_ttest_paired) #SRR1039508..vs..SRR1039509 # Paired t-test # #data: airway_data[, 2] and airway_data[, 3] #t = 6.9876, df = 38693, p-value = 2.842e-12 #alternative hypothesis: true difference in means is not equal to 0 #95 percent confidence interval: # 32.66123 58.12758 #sample estimates: #mean of the differences # 45.3944 The above results suggest that there is strong difference between these two samples of the same group. Now, we will also consider the case of unpaired for comparison. out_put_ttest_unpaired &lt;- t.test(airway_data[,2], airway_data[,3], paired = FALSE) cat(names_pairs); print(out_put_ttest_unpaired) #SRR1039508..vs..SRR1039509 # Welch Two Sample t-test # #data: airway_data[, 2] and airway_data[, 3] #t = 1.5108, df = 75614, p-value = 0.1309 #alternative hypothesis: true difference in means is not equal to 0 #95 percent confidence interval: # -13.49803 104.28683 #sample estimates: #mean of x mean of y # 546.4878 501.0934 In this case, the result is the opposite and there is no evidence of a difference in the mean values if these samples were considered of different groups. Note that a careful determination of what parametrization of the t-test is determined by your experimental group and other considerations. Likewise, a t-test is invalid for small samples from non-normal distributions, but it is valid for large samples from non-normal distributions (like our example above with &gt;30,000 entries). These considerations are beyond this bioinformatics workshop and are the topic of a biostatistics course. 6.5 Beyond t-test: Wilcoxon signed rank test 11) Alternatives to the parametric t-test exist, one of the most common is the one/two-sample median test or the Wilcoxon signed rank test. This test is used to compare two unpaired and paired samples or repeated measurements on a single sample to assess whether their population mean ranks differ. The Wilcoxon signed rank test is implemented with the function wilcox.test() in a similar fashion as is in a t.test(). # test for mean of sample wilcox.test(airway_data[,2], mu = 546.5) # # Wilcoxon signed rank test with continuity correction # #data: airway_data[, 2] #V = 145919557, p-value &lt; 2.2e-16 #alternative hypothesis: true location is not equal to 546.5 # test for two samples of different group wilcox.test(airway_data[,2], airway_data[,3], paired = FALSE) # # Wilcoxon rank sum test with continuity correction # #data: airway_data[, 2] and airway_data[, 3] #W = 753579582, p-value = 0.08979 #alternative hypothesis: true location shift is not equal to 0 # test for two samples of same group, paired-test wilcox.test(airway_data[,2], airway_data[,3], paired = TRUE) # # Wilcoxon signed rank test with continuity correction # #data: airway_data[, 2] and airway_data[, 3] #V = 147963112, p-value &lt; 2.2e-16 #alternative hypothesis: true location shift is not equal to 0 6.6 Beyond t-test: Binomial test 12) The binomial test will determine the significance of deviations from expected distribution of observations into two groups of categorical variable. One common use of this test is when an experiment has two possible outcomes (i.e. success/failure) and you have an expectation (i.e., probability) success. For example, if you flip fair coin you have expectation of 50/50 chance of heads of tail every flip. You can apply a binomial test to a population with two categories using the function prop.test(). For example, we can test if the number of females in a population is p = 0.51 (i.e., the chance of being female is 51%). We will use function rbinom() to create such population of 1000 individuals. ## we create a binomial variable assigned to males of females. ## we assume that if female is 1 and 0 if male one_population &lt;- rbinom(n = 1000, size = 1, prob= 0.51) str(one_population) #int [1:1000] 0 0 0 0 1 0 1 0 0 1 ... prop.test(sum(one_population), length(one_population), p = 0.51) # # 1-sample proportions test with continuity correction # #data: sum(one_population) out of length(one_population), null probability 0.51 #X-squared = 0.04902, df = 1, p-value = 0.8248 #alternative hypothesis: true p is not equal to 0.51 #95 percent confidence interval: # 0.4745504 0.5374029 #sample estimates: # p #0.506 In the case above, we found a non-significant result p-value = 0.8248 and this population is likely to have 51% chance of females. However, less create a biased population towards males. ## we create a binomial variable assigned to males of females. ## we assume that if female is 1 and 0 if male other_population &lt;- rbinom(n = 1000, size = 1, prob= 0.40) str(other_population) #int [1:1000] 0 0 0 0 1 1 0 1 0 0 ... prop.test(sum(other_population), length(other_population), p = 0.51) # # 1-sample proportions test with continuity correction # #data: sum(other_population) out of length(other_population), null probability 0.51 #X-squared = 45.387, df = 1, p-value = 1.617e-11 #alternative hypothesis: true p is not equal to 0.51 #95 percent confidence interval: # 0.3725342 0.4342204 #sample estimates: # p #0.403 In the case above, we found a significant result p-value = 1.617e-11 and this population unlikely has 51% chance of females. 6.7 Beyond t-test: Chi-square goodness of fit 13) The Chi-square goodness of fit test* will determine the significance of deviations from expected probabilities of multiple categories in a population. For example, you might want to compare the observed distribution of categories of a population to an expected distribution of such categories. We will perform this test using a table object with multiple categories with the function chisq.test() and an argument of probabilities for each category p. For example, we can test if the proportion of nitrogen fixing bacteria in a population collected of dataset rhizobium from R package ade4. ## load rhizobium dataset from ade4 R package library(ade4) data(rhizobium) rhizobium_pop &lt;- as.character(rhizobium$pop) str(rhizobium_pop) #chr [1:215] &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; ... ## create a table object rhizobium_pop_table &lt;- table(rhizobium_pop) rhizobium_pop_table #rhizobium_pop # FTmdc FTmlt TELmlt TETmdc TETmlt THLmlt THTmlt # 46 43 20 20 24 20 42 ## we have 7 categories. We will assume that they should be in equal proportion (i.e., 1/7 each). chisq.test(rhizobium_pop_table, p = c(1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7)) # # Chi-squared test for given probabilities # #data: rhizobium_pop_table #X-squared = 29.349, df = 6, p-value = 5.225e-05 14) You can also cross tabulate results of two measurements of population that result in two or more categories for each and determine if they are related using a Chi-square goodness of fit test. The goal is to create a contingency table where your compare the outcomes for two different measurements of the same population and determine if they tend to be in agreement or not. ## we can have a population and determine a categorical variable that result in two category (0, 1) like for example: species_A and species_B sample_species &lt;- rbinom(n = 100, size = 1, prob= 0.40) species_names &lt;- ifelse(sample_species == 1, &quot;species_B&quot;, &quot;species_A&quot;) names(sample_species) &lt;- species_names head(sample_species) #species_A species_A species_B species_B species_A species_A # 0 0 1 1 0 0 ## we can have the same population and determine a different categorical variable that result in two categories (0, 1) like for example: parasite and not_parasite sample_parasite &lt;- rbinom(n = 100, size = 1, prob= 0.40) parasite_names &lt;- ifelse(sample_parasite == 1, &quot;parasite&quot;, &quot;not_parasite&quot;) names(sample_parasite) &lt;- parasite_names head(sample_parasite) # parasite not_parasite not_parasite not_parasite parasite parasite # 1 0 0 0 1 1 We cross-tabulate these results and it is important to understand this table. The outcomes for sample_species and sample_parasite are 0 1 as indicated in the first row and column. The agreements are when the outcomes of both experiments are the same as each experiment was executed 0-0 and 1-1. All others are disagreements (e.g., 0-1, 1-0). cross_tab &lt;- table(sample_species, sample_parasite) cross_tab # sample_parasite #sample_species 0 1 # 0 26 33 # 1 27 14 We can now estimate if such agreements and disagreements are different using Chi-squared Test for Count Data with the function chisq.test(). chisq.test(cross_tab) # # Pearson&#39;s Chi-squared test with Yates&#39; continuity correction # #data: cross_tab #X-squared = 3.776, df = 1, p-value = 0.05199 In this case, we found that they were barely non-significant different. In this case, if the species is species_A this is more likely a not a parasite. 6.8 Beyond t-test: Fisher’s exact test 15) This is a similar test to the Chi-square goodness of fit and can be used with populations with small samples. The Fisher’s exact test will examine the significance of the association (contingency) between the two kinds of classification. The null hypothesis is that both classifications are dependent. We can use the same dataset as above with the function fisher.test(). fisher.test(cross_tab) # #Fisher&#39;s Exact Test for Count Data # #data: cross_tab #p-value = 0.04194 #alternative hypothesis: true odds ratio is not equal to 1 #95 percent confidence interval: # 0.1638957 1.0038597 #sample estimates: #odds ratio # 0.4122915 In this case, we found that they were barely significant different. In this case, if the species is species_A this is more likely to be a parasite. 6.9 Correlations 12) We can determine correlation between two variables using cor() and test for its significance using cor.test(). Correlation is the dependence or association between variables and it usually refers to the degree to which a pair of variables are linearly related. For practical uses, correlations can provide a predictive relationship that can derived from one variable on the other. However, the evidence of correlation does not mean a causal relationship (i.e., A does not necessarily cause B). Likewise, it is important to determine if such correlations are significant and these can be performed for normal or non-normal distributions (in this case use the use argument method = \"spearman\"). We will determine a correlation and test for its significance between two samples of the airway_data. cor(airway_data[,2], airway_data[,3]) #[1] 0.9646265 # we will assume that these are not normal cor.test(airway_data[,2], airway_data[,3], method = &quot;spearman&quot;) # # Spearman&#39;s rank correlation rho # #data: airway_data[, 2] and airway_data[, 3] #S = 4.8345e+11, p-value &lt; 2.2e-16 #alternative hypothesis: true rho is not equal to 0 #sample estimates: # rho #0.9499302 6.10 Comparing distributions 13) We can also determine if two variables come from the same distribution using Kolmogorov-Smirnov test ks.test(). The data does not require to be normal. We will determine if two samples of the airway_data have the same distribution. ks.test(airway_data[,2], airway_data[,3]) # # Two-sample Kolmogorov-Smirnov test # #data: airway_data[, 2] and airway_data[, 3] #D = 0.013413, p-value = 0.001896 #alternative hypothesis: two-sided # #Warning message: #In ks.test(airway_data[, 2], airway_data[, 3]) : # p-value will be approximate in the presence of ties We can conclude that these two samples do not have the same distribution (i.e., p-value = 0.001896). 14) We can compare two groups that have some individuals in such grops being exposed treatment verus control using from the R-package pROC using the functions roc() and roc.test(). This test will compare the AUC (area under the curve) of two ROC (receiver operating characteristic) curves. These are graphical plots useful in the diagnostic of a binary classifier (e.g., “control-0” or “exposed-1”) as its discrimination threshold is varied (i.e., what is the cutoff to consider a measurement as “exposed-1”). We will illustrate this test using two airway_data by assigning “control” and “exposed” to sample of rows. # you need to instal the package &quot;pROC&quot; install.packages(&quot;pROC&quot;) library(pROC) We can prepare two samples of airway_data and asign some of elements of each as “control” and “exposed.” ## set two samples sample_SRR1039508 &lt;- airway_data$SRR1039508 sample_SRR1039509 &lt;- airway_data$SRR1039509 ## get summaries of each summary_SRR1039508 &lt;- summary(sample_SRR1039508) summary_SRR1039508 # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.0 0.0 1.0 546.5 203.0 287372.0 summary_SRR1039509 &lt;- summary(sample_SRR1039509) summary_SRR1039509 # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.0 0.0 1.0 501.1 172.0 244921.0 ## assign &quot;control&quot; if less than 3rd quartile of each SRR1039508_binary &lt;- ifelse(sample_SRR1039508 &gt; summary_SRR1039508[5], &quot;exposed&quot;, &quot;control&quot;) ## assign &quot;control&quot; if more than 3rd quartile of each -- a reverse from last sample SRR1039509_binary &lt;- ifelse(sample_SRR1039509 &gt; summary_SRR1039509[5], &quot;control&quot;, &quot;exposed&quot;) ## create a roc object for each sample roc_SRR1039508 &lt;- roc(SRR1039508_binary, sample_SRR1039508, direction = &quot;&lt;&quot;) #Setting levels: control = control, case = exposed roc_SRR1039509 &lt;- roc(SRR1039509_binary, sample_SRR1039509, direction = &quot;&lt;&quot;) #Setting levels: control = control, case = exposed ## perform the roc.test and increase boot.n for a more precise p-value: roc.test(roc_SRR1039508, roc_SRR1039509, method=&quot;bootstrap&quot;, boot.n=1000) # |=========================================================================================================================================================================| 100% # # Bootstrap test for two ROC curves # #data: roc_SRR1039508 and roc_SRR1039509 #D = Inf, boot.n = 1000, boot.stratified = 1, p-value &lt; 2.2e-16 #alternative hypothesis: true difference in AUC is not equal to 0 #sample estimates: #AUC of roc1 AUC of roc2 # 1 0 We can conclude that for SRR1039508 and SRR1039509 are different in their distribution in reference to outcomes for “control” versus “exposed.” "],["basic-statistics-in-r-1.html", "Session 7 – Basic Statistics in R 7.1 Exploring your data 7.2 Normalizing your data 7.3 What test should I use? 7.4 Comparing means: t-test 7.5 Beyond t-test: Wilcoxon signed rank test 7.6 Beyond t-test: Binomial test 7.7 Beyond t-test: Chi-square goodness of fit 7.8 Beyond t-test: McNemar’s test 7.9 Beyond t-test: Fisher’s exact test 7.10 Correlations 7.11 Comparing distributions", " Session 7 – Basic Statistics in R This workshop is not designed to give you an extensive overview on all statistical methods that can be performed in R. Yet, this computing environment was born to provide a comprehensive platform for the implementation of statistical methods. We will overview of some basic and advanced methods with some illustrative graphs (these will be expanded in the session devoted to graphics). However, you can ask me if you want to explore a specific method that you would me to add to this gitbook. 7.1 Exploring your data 1) To start, we need to import a dataset that we want to explore and analyze. As graduate students, you are encouraged to use your own datasets (i.e., this is ideal as you will know more about its contents, metrics and intended use). For this exercise, I will use two previously show datasets: (1) the small preloaded mtcars dataset and (2) the large airway_scaledcounts.csv dataset. These datasets are in our class GitHub repository and you can donwload those to your computer. ## NOTE: remember to update the path to files with the datasets where you downloaded in your computer -- THIS IS EXCLUSIVE TO YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW #define a working directory setwd(&quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory&quot;) #load get mtcars dataset -- this a preloaded dataset in R cars_data &lt;- mtcars cars_data # mpg cyl disp hp drat wt qsec vs am gear carb #Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 #Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 #Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 #... #load get &#39;airway_scaledcounts.csv&#39; dataset airway_data &lt;- read.table(&quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/airway_scaledcounts.csv&quot;, header = TRUE, sep = &quot;,&quot;, stringsAsFactors = FALSE) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 ENSG00000000003 723 486 904 445 1170 1097 806 604 #2 ENSG00000000005 0 0 0 0 0 0 0 0 #3 ENSG00000000419 467 523 616 371 582 781 417 509 #4 ENSG00000000457 347 258 364 237 318 447 330 324 #5 ENSG00000000460 96 81 73 66 118 94 102 74 #6 ENSG00000000938 0 0 1 0 2 0 0 0 #... 2) We can get some basic descriptive parameters of each data set including number of entries, number and names of the columns (i.e., variables), type of variables (e.g., numbers or characters), number of NA values. For this purpose, the easiest approach is to use the function summary() summary(cars_data) # mpg cyl disp hp drat wt qsec vs # Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 # 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 # Median :19.20 Median :6.000 Median :196.3 Median :123.0 Median :3.695 Median :3.325 Median :17.71 Median :0.0000 # Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 # 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 # Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 # am gear carb # Min. :0.0000 Min. :3.000 Min. :1.000 # 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 # Median :0.0000 Median :4.000 Median :2.000 # Mean :0.4062 Mean :3.688 Mean :2.812 # 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 # Max. :1.0000 Max. :5.000 Max. :8.000 summary(airway_data) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 # Length:38694 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0 # Class :character 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0 # Mode :character Median : 1.0 Median : 1.0 Median : 1.0 Median : 0.0 Median : 1 # Mean : 546.5 Mean : 501.1 Mean : 673.6 Mean : 405.3 Mean : 649 # 3rd Qu.: 203.0 3rd Qu.: 172.0 3rd Qu.: 237.0 3rd Qu.: 128.0 3rd Qu.: 228 # Max. :287372.0 Max. :244921.0 Max. :510107.0 Max. :304818.0 Max. :410979 # SRR1039517 SRR1039520 SRR1039521 # Min. : 0.0 Min. : 0.0 Min. : 0.0 # 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 0.0 # Median : 1.0 Median : 1.0 Median : 1.0 # Mean : 822.5 Mean : 509.0 Mean : 565.7 # 3rd Qu.: 261.0 3rd Qu.: 185.8 3rd Qu.: 183.0 # Max. :416837.0 Max. :378108.0 Max. :372973.0 These summary output provide a general descriptive and summary statistics of data distribution of the values on each column (variable). For continuous variables, minimum and maximum values are provided. Other statistics include mean and median that provide a general measurement of central location (i.e., average). The 1st quartile (Q1) is the middle value between the minimum and the median; and it also represents measurement that marks where 25% of the data is below this point. The 3rd quartile (Q3) is the middle value between the median and the maximum; and also represents 75% of the data is below this point. Notice the column named ensgene is a discrete (character) variable and only provides the number of elements in that column. We can visualize these summaries using boxplots. The base package has the function boxplot(). #car data boxplot boxplot(mpg~cyl, data=cars_data, col=(c(&quot;#DA291CFF&quot;, &quot;#56A8CBFF&quot;, &quot;#53A567FF&quot;)), main=&quot;car data boxplots&quot;, xlab=&quot;cylinders&quot;) You can also visualize these summaries using boxplots for all variables using the R-package reshape2 and its function melt(). This will reshape the data frame to be more amenable for boxplots. # you might need to install R-package &#39;reshape2&#39; install.packages(&quot;reshape2&quot;) library(reshape2) # reduce the &quot;airway_data&quot; to only the continuous variables. airway_data_reduced2 &lt;- airway_data[,2:ncol(airway_data_very_high_low)] head(airway_data_reduced2) # SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 723 486 904 445 1170 1097 806 604 #2 0 0 0 0 0 0 0 0 #3 467 523 616 371 582 781 417 509 #4 347 258 364 237 318 447 330 324 #5 96 81 73 66 118 94 102 74 #6 0 0 1 0 2 0 0 0 airway_data_reduced2_melted &lt;- melt(airway_data_reduced2) #No id variables; using all as measure variables # we will change the names of the columns to reflect what they represent. names(airway_data_reduced2_melted) &lt;- c(&quot;sample&quot;, &quot;expression&quot;) head(airway_data_reduced2_melted) # sample expression #1 SRR1039508 723 #2 SRR1039508 0 #3 SRR1039508 467 #4 SRR1039508 347 #5 SRR1039508 96 #6 SRR1039508 0 #airway_data boxplots boxplot(expression~sample, data=airway_data_reduced2_melted, main=&quot;airway_data boxplots&quot;, xlab=&quot;samples&quot;) This figure is not very elegant (we will work with graph in that section). However, these boxplots are extremely skewed to low expression values. 3) We can discretize our data by creating groups based on criterium (e.g., threshold value). Such discrete groups can help us to define if a treatment or condition is reflected in our data or experimental groups. We will create a binary data frame for airway_data for the columns (variables) that represent the normalized gene expression for the samples that start withSRR. For this example, we will consider an arbitrary scale to divide into groups as: very high (x =&gt; 10000), high (10000 &gt; x =&gt; 100) and low (x &lt; 100) expression levels. This task can be perform using a specialized R-package: purrr and its function modify() with a conditional ifelse(). We already introduce purrr on previous sessions. Notice that when we close some of the commands we use complex punctuation (i.e., (, ), {, }) and many mistakes can happen if you forget to close such punctuation pairs (i.e., missing one is a very common error).. # we already installed R-package &#39;purrr&#39; library(purrr) # we create an alternative object, so we original raw data airway_data airway_data_very_high_low &lt;- airway_data # We will apply a the conditional airway_data_very_high_low &lt;- purrr::modify(airway_data_very_high_low,function(x) {ifelse(x &gt;= 10000, &quot;very_high&quot;, ifelse( x &gt;= 1000, &quot;high&quot;, &quot;low&quot;))}) head(airway_data_very_high_low) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 very_high low low low low high high low low #2 very_high low low low low low low low low #3 very_high low low low low low low low low #4 very_high low low low low low low low low #5 very_high low low low low low low low low #6 very_high low low low low low low low low #... During the process above, we have “lost” the values of our variable ensgene that contains the names of the loci. We can add it back from our original data frame airway_data as indicated next. airway_data_very_high_low$ensgene &lt;- airway_data$ensgene head(airway_data_very_high_low) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 ENSG00000000003 low low low low high high low low #2 ENSG00000000005 low low low low low low low low #3 ENSG00000000419 low low low low low low low low #4 ENSG00000000457 low low low low low low low low #5 ENSG00000000460 low low low low low low low low #6 ENSG00000000938 low low low low low low low low #... 4) With this discretized data frame airway_data_very_high_low, we can perform contingency tables. These are a type of table in a matrix format that displays the frequency distribution of the variables tested. We will exclude the first column ensgene because it has unique values. airway_data_reduced &lt;- airway_data_very_high_low[,2:ncol(airway_data_very_high_low)] # collect frequencies on samples summary_frequencies_airway_data &lt;- list() for(i in 1:ncol(airway_data_reduced)) { sample_name &lt;- names(airway_data_reduced[i]) one_table &lt;- table(airway_data_reduced[i]) # from table to vector one_contigency_vector &lt;- as.numeric(one_table) # add names of categories names(one_contigency_vector) &lt;- names(one_table) one_data_frame &lt;- as.data.frame(one_contigency_vector) # add name of sample names(one_data_frame) &lt;- sample_name # collect results in list summary_frequencies_airway_data[[i]] &lt;- one_data_frame } summary_frequencies_airway_data #[[1]] # SRR1039508 #high 3732 #low 34691 #very_high 271 #[[2]] # SRR1039509 #high 3444 #low 34986 #very_high 264 #... With the list summary_frequencies_airway_data, we can cover it to a data frame by binding each vector as a column using two functions do.call() and cbind(). final_summary_all_samples_df &lt;- do.call(cbind, summary_frequencies_airway_data) final_summary_all_samples_df # SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #high 3732 3444 4118 2607 4155 4703 3432 3683 #low 34691 34986 34216 35902 34196 33500 35024 34725 #very_high 271 264 360 185 343 491 238 286 5) With this data frame, we can test these discretized variables for independence using a chi-squared test with the function summary(). For example, we can compare two of the airway_data_reduced samples (i.e., columns). Notice that I use a ; command at the end of one line of commands, this allows you to paste several commands next to each other as if they were in new lines. sample_names &lt;- names(airway_data_reduced)[1:2] title_test &lt;- paste0(sample_names, collapse =&quot;..vs..&quot;) contigency_result &lt;- summary(table(airway_data_reduced[,1], airway_data_reduced[,2])) cat(title_test, &quot;\\n&quot;); print(contigency_result) #SRR1039508..vs..SRR1039509 #ENumber of cases in table: 38694 #Number of factors: 2 #Test for independence of all factors: # Chisq = 58193, df = 4, p-value = 0 # Chi-squared approximation may be incorrect These results indicate that we can conclude that these samples are likely independent, but the large number of cases 38694 might have inflated the Chisq statistic. Next we have an interesting loop for you to dissect, it will run all pairwise contingency tests, report the process, and add names of the pairs being compared. Notice the iterative variable counter and function break that breaks the loop when it has finished. counter &lt;- 0 collect_test_independence &lt;- list() for(i in 1:ncol(airway_data_reduced)) { # i &lt;- 1 names_sample_i &lt;- names(airway_data_reduced)[i] n &lt;- i + 1 upper_limit &lt;- ncol(airway_data_reduced)+1 if(n &lt; upper_limit) { cat(&quot;sample:&quot;,names_sample_i, &quot;..vs..&quot;) for(j in n:ncol(airway_data_reduced)) { counter &lt;- counter + 1 names_sample_j &lt;- names(airway_data_reduced)[j] cat(names_sample_j, &quot;....&quot;) list_name &lt;- paste0(names_sample_i,&quot;..vs..&quot;, names_sample_j) collect_test_independence[[counter]] &lt;- summary(table(airway_data_reduced[,i], airway_data_reduced[,j])) names(collect_test_independence)[counter] &lt;- list_name } cat(&quot;\\n&quot;) } else {break} } #sample: SRR1039508 ..vs..SRR1039509 ....SRR1039512 ....SRR1039513 ....SRR1039516 ....SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039509 ..vs..SRR1039512 ....SRR1039513 ....SRR1039516 ....SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039512 ..vs..SRR1039513 ....SRR1039516 ....SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039513 ..vs..SRR1039516 ....SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039516 ..vs..SRR1039517 ....SRR1039520 ....SRR1039521 .... #sample: SRR1039517 ..vs..SRR1039520 ....SRR1039521 .... #sample: SRR1039520 ..vs..SRR1039521 .... # if you call the collect_test_independence, you will get the statistics for independence collect_test_independence #$SRR1039508..vs..SRR1039509 #Number of cases in table: 38694 #Number of factors: 2 #Test for independence of all factors: # Chisq = 58193, df = 4, p-value = 0 # Chi-squared approximation may be incorrect # #$SRR1039508..vs..SRR1039512 #Number of cases in table: 38694 #Number of factors: 2 #Test for independence of all factors: # Chisq = 58274, df = 4, p-value = 0 # Chi-squared approximation may be incorrect 7.2 Normalizing your data 6) Normalizing or scaling your data is sometimes useful when you are trying to compare datasets that might have extremely different units (e.g., tiny fractions and others in the hundreds of thousands). This can be done with the function scale(). head(cars_data) # mpg cyl disp hp drat wt qsec vs am gear carb #Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 #Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 #Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 #Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 #Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 cars_data_scaled &lt;- purrr::modify(cars_data, scale) # head(cars_data_scaled) # mpg cyl disp hp drat wt qsec vs am gear carb #Mazda RX4 0.1508848 -0.1049878 -0.57061982 -0.5350928 0.5675137 -0.610399567 -0.7771651 -0.8680278 1.1899014 0.4235542 0.7352031 #Mazda RX4 Wag 0.1508848 -0.1049878 -0.57061982 -0.5350928 0.5675137 -0.349785269 -0.4637808 -0.8680278 1.1899014 0.4235542 0.7352031 #Datsun 710 0.4495434 -1.2248578 -0.99018209 -0.7830405 0.4739996 -0.917004624 0.4260068 1.1160357 1.1899014 0.4235542 -1.1221521 #Hornet 4 Drive 0.2172534 -0.1049878 0.22009369 -0.5350928 -0.9661175 -0.002299538 0.8904872 1.1160357 -0.8141431 -0.9318192 -1.1221521 #Hornet Sportabout -0.2307345 1.0148821 1.04308123 0.4129422 -0.8351978 0.227654255 -0.4637808 -0.8680278 -0.8141431 -0.9318192 -0.5030337 #Valiant -0.3302874 -0.1049878 -0.04616698 -0.6080186 -1.5646078 0.248094592 1.3269868 1.1160357 -0.8141431 -0.9318192 -1.1221521 7) Common statistical tests are parametric and most of them assume that your data has a normal (Gaussian) distribution, i.e., a typical bell-shaped curve. In other words, the distribution of values of variable that has a normal distribution will produce values (as they are being measured) with an equal chance to fall above and below the mean value of all measurements. The normal distribution is expected to have very close values for its mean (average of all values), median (mid-point value at the center of the distribution) and mode (the most frequent value). If a distribution is not normal, these three parameters will differ and the distribution is considered skewed (i.e., not normal). # Create a sequence of numbers between 0 and 100 by 1 x &lt;- seq(0, 100, by = 1) # Choose the mean as 50 and standard deviation as 17.5 y &lt;- dnorm(x, mean = 50, sd = 17.5) # Plot this distribution plot(x,y) 8) We can test for normal (Gaussian) distribution of our data using a normality test. We can use the Shapiro-Wilk’s test that determines if the null hypothesis (i.e., the sample has normal distribution) versus the alternative that it does not. If the test is significant, the distribution has a non-normal distribution. This test is sensitive to sample size, which means that large samples most often do not pass normality tests (i.e., show to be significant). Therefore, it’s important that you use visual inspection of your data distribution before you decide to transform your data. The Shapiro-Wilk’s test is a build-in function and can be call as shapiro.test(). However, this test cannot be applied to large vectors (i.e., &gt; 5000 entries). # we can perfome in a variable of a data frame applying it as vector shapiro.test(cars_data$hp) # Shapiro-Wilk normality test # #data: cars_data$hp #W = 0.93342, p-value = 0.04881 shapiro.test(cars_data$qsec) # Shapiro-Wilk normality test # #data: cars_data$qsec #W = 0.97325, p-value = 0.5935 Based on these results, we can conclude that the variable hp has not normal distribution, while qsec has normal one. If you want to apply the shapiro.test() to all you variables in your data frame use apply with 2 argument for columns. apply(cars_data,2,shapiro.test) 9) In most cases, you might like to apply a transformation to your data before you apply a parametric statistical test. One of the most frequent transformations for continuous data is to normalize them, this means that change the distribution of the data to a quasinormal or normal one. Such transformations will make our data more amenable for parametric tests. However, the interpretation of the results has to be framed using the units of the transformed data (i.e., if log-transformed, you have to interpret in log-transformed unit). To determine how if your data need to be normalized, it is a good idea to visualize its distribution. To exemplify this, we will describe this using the hp variable of the car_data. hp_vector &lt;- cars_data$hp hp_vector #[1] 110 110 93 110 175 105 245 62 95 123 123 180 180 180 205 215 230 66 52 65 97 150 150 245 175 66 91 113 264 175 335 109 # we test for normality shapiro.test(hp_vector) # Shapiro-Wilk normality test # #data: cars_data$hp #W = 0.93342, p-value = 0.04881 boxplot(hp_vector, xlab = &quot;hp&quot;, ylab = &quot;values&quot;) In this boxplot, we can see an outlier and it is skewed towards lower values of hp. Some of the most common transformations are to square root the data using the function sqrt() and to perform logarithms using log10() with base 10 or natural logarithms ‘log().’ hp_vector_sqrt &lt;- sqrt(hp_vector) shapiro.test(hp_vector_sqrt) #Shapiro-Wilk normality test # #data: hp_vector_sqrt #W = 0.96456, p-value = 0.3642 hp_vector_log10 &lt;- log10(hp_vector) shapiro.test(hp_vector_log10) # Shapiro-Wilk normality test # #data: hp_vector_log10 #W = 0.97026, p-value = 0.5065 boxplot(hp_vector_log10, xlab = &quot;hp&quot;, ylab = &quot;values_transformed_by_log10&quot;) Here is the resulting boxplot afterlog10() transformation. In some extreme cases, there are several R-packages that can help you to transform your data to have a normal distribution. One of these is bestNormalize and its function boxcox(). This provides a Box-Cox power transformation that can be useful for extremely skewed distributions. Likewise, this function returns, by default, transformed values that are also centered and scaled. # if you need to install the R-package bestNormalize install.packages(&quot;bestNormalize&quot;) # load library library(bestNormalize) # perform Box-Cox power tranformation. hp_vector_boxcox_list &lt;- bestNormalize::boxcox(hp_vector) hp_vector_boxcox_list #Standardized Box Cox Transformation with 32 nonmissing obs.: # Estimated statistics: # - lambda = 0.0940865 # - mean (before standardization) = 6.21272 # - sd (before standardization) = 0.7515468 str(hp_vector_boxcox_list) #List of 8 # $ x.t : num [1:32] -0.401 -0.401 -0.745 -0.401 0.582 ... #$ x : num [1:32] 110 110 93 110 175 105 245 62 95 123 ... #$ mean : num 6.21 #$ sd : num 0.752 #$ lambda : num 0.0941 #$ n : int 32 #$ norm_stat : num 1.4 #$ standardize: logi TRUE #- attr(*, &quot;class&quot;)= chr [1:2] &quot;boxcox&quot; &quot;list&quot; hp_vector_boxcox &lt;- hp_vector_boxcox_list$x.t #(1] -0.4005863 -0.4005863 -0.7454816 -0.4005863 0.5821478 -0.4967035 1.3216259 -1.5563448 -0.7020701 -0.1680641 -0.1680641 0.6431660 0.6431660 0.6431660 0.9269679 #[16] 1.0317736 1.1809841 -1.4333227 -1.8985905 -1.4634315 -0.6594788 0.2511055 0.2511055 1.3216259 0.5821478 -1.4333227 -0.7897473 -0.3447992 1.4889763 0.5821478 #[31] 2.0305623 -0.4194885 boxplot(hp_vector_boxcox, xlab = &quot;hp&quot;, ylab = &quot;values_transformed_by_boxcox&quot;) 7.3 What test should I use? The task of choosing the best statistical analyses given your dataset are one of the most important steps for any biological research. Your choice of methods depends on your understanding of your dataset and these include the following: (a) the number of dependent or outcome variables; (b) the number of your independent or predictor variables; (c) the characteristics of your dependent and independent variables, namely whether they are continuous (interval or ratio) variables (e.g., temperature, growth area, enzyme activity, dose amount, reaction rate, concentration, weight, length); (d) they are ordinal variables where order matters but differences between adjacent categories do not necessarily have the same meaning such as in treatment response (“low,” ”middle”, ”high”) or location (“understory,” “canopy”); (e) they are categorical variables if the categories that do not have a natural order or ranking such in colors (e.g., red, blue, green), populations (e.g., A, B, C, D); and (f) the type of distribution of the variables (it is normally distributed so parametric methods could be applied or not). The table of statistical choices has been modified form here. Dependents (Number) Dependents (Type) Independents (Number) Independents (Type) Test Section 1 &amp; one population continuous (normal) N/A N/A one-sample t-test Session 6 9 1 &amp; one population continuous (non-normal); ordinal N/A N/A one-sample median or Wilcoxon signed rank test Session 6 11 1 &amp; one population categorical (2 categories) N/A N/A binomial test Session 6 12 1 &amp; one population categorical (multiple categories) N/A N/A Chi-square goodness-of-fit Session 6 13 1 &amp; two populations continuous (normal) N/A N/A two-sample t-test Session 6 10 1 &amp; two populations continuous (non-normal); ordinal N/A N/A two-sample median or Wilcoxon signed rank test Session 6 11 1 &amp; two populations categorical (multiple categories) compare agreements N/A N/A Chi-square goodness-of-fit Session 6 14 1 &amp; two populations categorical (multiple categories) N/A N/A Fisher’s exact test Session 6 15 1 &amp; one population continuous (normal) 1 categorical (2 or more categories) one-way ANOVA Session 7 12 1 &amp; one population continuous (non-normal); ordinal 1 categorical (2 or more categories) one-way ANOVA Session 7 15 1 &amp; one population continuous (normal) 1 continuous (normal) correlation Session 7 12 1 &amp; one population continuous (normal) 1 continuous (normal) simple linear regression Session 7 1 1 &amp; one population continuous (normal) many continuous (normal) multiple linear regression Session 7 3 1 &amp; one population continuous 1 continuous or ordinal non-parametric correlation Session 7 10 1 &amp; one population ordinal 1 continuous or ordinal simple logistic regression Session 7 12 Here is another chart about selecting a statistial test for your project. The original source is in here. 7.4 Comparing means: t-test 9) We can perform one sample t-test on vectors using the function t.test(). For this test, you can test if its mean is statistically different from a given value specified by the mu argument equal such value. # we will test the mean of sample SRR1039508 equal to mu = 546.5 for names(cars_data)[7] [1] &quot;qsec&quot; t.test(cars_data$qsec, mu = 11) # # One Sample t-test # #data: cars_data$qsec #t = 21.681, df = 31, p-value &lt; 2.2e-16 #alternative hypothesis: true mean is not equal to 11 #95 percent confidence interval: # 17.20449 18.49301 #sample estimates: #mean of x #17.84875 # we will test the mean of sample SRR1039508 equal to mu = 546.5 for names(airway_data)[2] #[1] &quot;SRR1039508&quot; t.test(airway_data[,2], mu = 546.5) # # One Sample t-test # #data: airway_data[, 2] #t = -0.00053579, df = 38693, p-value = 0.9996 #alternative hypothesis: true mean is not equal to 546.5 #95 percent confidence interval: # 501.7696 591.2060 #sample estimates: #mean of x # 546.4878 These result indicate that the mean of qsec from the cars_data is statically different form 11, while the mean for sample SRR1039508 from the airway_data is not different from the value 546.6 (this value was derived from the summary see 2)). One sample t-test does not assume that the population is normally distributed. However, it is better to have a normalized distribution in our data when we apply parametric test like this one. The output of the function t.test() also provides 95% confidence interval of the mean value. In these examples, qsec has a mean 17.84875 with 95CI (17.20449, 18.49301) and the SRR1039508 sample mean is 546.4878 with 95CI (501.7696, 591.2060). Our interpretation is that the true value for the mean of the measurements of each of these variables is in the proposed range with a 95% confidence level. 10) For two samples, we can also perform t-tests with two vectors using the same function t.test(). In this case, you are testing if two samples each from different populations (e.g., control and experimental) have the same mean. If the test is significant, you can conclude that the difference might derive from a treatment (as long as you have controlled for other causal agents). These t-tests can be consider as paired (i.e., argument paired = TRUE) or unpaired (i.e., argument paired = FALSE). For paired, the t-test will compare the means of the same group under two separate conditions. In contrast, an unpaired t-test compares the means of two independent groups. In an unpaired t-test, the variance between groups is assumed to be equal. We will test this for airway_data data that have several samples from the same group the argument paired = TRUE. names_pairs &lt;- names(airway_data)[2:3] names_pairs &lt;- paste0(names_pairs, collapse = &quot;..vs..&quot;) #[1] &quot;SRR1039508..vs..SRR1039509&quot; out_put_ttest_paired &lt;- t.test(airway_data[,2], airway_data[,3], paired = TRUE) cat(names_pairs); print(out_put_ttest_paired) #SRR1039508..vs..SRR1039509 # Paired t-test # #data: airway_data[, 2] and airway_data[, 3] #t = 6.9876, df = 38693, p-value = 2.842e-12 #alternative hypothesis: true difference in means is not equal to 0 #95 percent confidence interval: # 32.66123 58.12758 #sample estimates: #mean of the differences # 45.3944 The above results suggest that there is strong difference between these two samples of the same group. Now, we will also consider the case of unpaired for comparison. out_put_ttest_unpaired &lt;- t.test(airway_data[,2], airway_data[,3], paired = FALSE) cat(names_pairs); print(out_put_ttest_unpaired) #SRR1039508..vs..SRR1039509 # Welch Two Sample t-test # #data: airway_data[, 2] and airway_data[, 3] #t = 1.5108, df = 75614, p-value = 0.1309 #alternative hypothesis: true difference in means is not equal to 0 #95 percent confidence interval: # -13.49803 104.28683 #sample estimates: #mean of x mean of y # 546.4878 501.0934 In this case, the result is the opposite and there is no evidence of a difference in the mean values if these samples were considered of different groups. Note that a careful determination of what parametrization of the t-test is determined by your experimental group and other considerations. Likewise, a t-test is invalid for small samples from non-normal distributions, but it is valid for large samples from non-normal distributions (like our example above with &gt;30,000 entries). These considerations are beyond this bioinformatics workshop and are the topic of a biostatistics course. 7.5 Beyond t-test: Wilcoxon signed rank test 11) Alternatives to the parametric t-test exist, one of the most common is the one/two-sample median test or the Wilcoxon signed rank test. This test is used to compare two unpaired and paired samples or repeated measurements on a single sample to assess whether their population mean ranks differ. The Wilcoxon signed rank test is implemented with the function wilcox.test() in a similar fashion as is in a t.test(). # test for mean of sample wilcox.test(airway_data[,2], mu = 546.5) # # Wilcoxon signed rank test with continuity correction # #data: airway_data[, 2] #V = 145919557, p-value &lt; 2.2e-16 #alternative hypothesis: true location is not equal to 546.5 # test for two samples of different group wilcox.test(airway_data[,2], airway_data[,3], paired = FALSE) # # Wilcoxon rank sum test with continuity correction # #data: airway_data[, 2] and airway_data[, 3] #W = 753579582, p-value = 0.08979 #alternative hypothesis: true location shift is not equal to 0 # test for two samples of same group, paired-test wilcox.test(airway_data[,2], airway_data[,3], paired = TRUE) # # Wilcoxon signed rank test with continuity correction # #data: airway_data[, 2] and airway_data[, 3] #V = 147963112, p-value &lt; 2.2e-16 #alternative hypothesis: true location shift is not equal to 0 7.6 Beyond t-test: Binomial test 12) The binomial test will determine the significance of deviations from expected distribution of observations into two groups of categorical variable. One common use of this test is when an experiment has two possible outcomes (i.e. success/failure) and you have an expectation (i.e., probability) success. For example, if you flip fair coin you have expectation of 50/50 chance of heads of tail every flip. You can apply a binomial test to a population with two categories using the function prop.test(). For example, we can test if the number of females in a population is p = 0.51 (i.e., the chance of being female is 51%). We will use function rbinom() to create such population of 1000 individuals. ## we create a binomial variable assigned to males of females. ## we assume that if female is 1 and 0 if male one_population &lt;- rbinom(n = 1000, size = 1, prob= 0.51) str(one_population) #int [1:1000] 0 0 0 0 1 0 1 0 0 1 ... prop.test(sum(one_population), length(one_population), p = 0.51) # # 1-sample proportions test with continuity correction # #data: sum(one_population) out of length(one_population), null probability 0.51 #X-squared = 0.04902, df = 1, p-value = 0.8248 #alternative hypothesis: true p is not equal to 0.51 #95 percent confidence interval: # 0.4745504 0.5374029 #sample estimates: # p #0.506 In the case above, we found a non-significant result p-value = 0.8248 and this population is likely to have 51% chance of females. However, less create a biased population towards males. ## we create a binomial variable assigned to males of females. ## we assume that if female is 1 and 0 if male other_population &lt;- rbinom(n = 1000, size = 1, prob= 0.40) str(other_population) #int [1:1000] 0 0 0 0 1 1 0 1 0 0 ... prop.test(sum(other_population), length(other_population), p = 0.51) # # 1-sample proportions test with continuity correction # #data: sum(other_population) out of length(other_population), null probability 0.51 #X-squared = 45.387, df = 1, p-value = 1.617e-11 #alternative hypothesis: true p is not equal to 0.51 #95 percent confidence interval: # 0.3725342 0.4342204 #sample estimates: # p #0.403 In the case above, we found a significant result p-value = 1.617e-11 and this population unlikely has 51% chance of females. 7.7 Beyond t-test: Chi-square goodness of fit 13) The Chi-square goodness of fit test* will determine the significance of deviations from expected probabilities of multiple categories in a population. For example, you might want to compare the observed distribution of categories of a population to an expected distribution of such categories. We will perform this test using a table object with multiple categories with the function chisq.test() and an argument of probabilities for each category p. For example, we can test if the proportion of nitrogen fixing bacteria in a population collected of dataset rhizobium from R package ade4. ## load rhizobium dataset from ade4 R package library(ade4) data(rhizobium) rhizobium_pop &lt;- as.character(rhizobium$pop) str(rhizobium_pop) #chr [1:215] &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; &quot;FTmdc&quot; ... ## create a table object rhizobium_pop_table &lt;- table(rhizobium_pop) rhizobium_pop_table #rhizobium_pop # FTmdc FTmlt TELmlt TETmdc TETmlt THLmlt THTmlt # 46 43 20 20 24 20 42 ## we have 7 categories. We will assume that they should be in equal proportion (i.e., 1/7 each). chisq.test(rhizobium_pop_table, p = c(1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7)) # # Chi-squared test for given probabilities # #data: rhizobium_pop_table #X-squared = 29.349, df = 6, p-value = 5.225e-05 14) You can also cross tabulate results of two measurements of population that result in two or more categories for each and determine if they are related using a Chi-square goodness of fit test. The goal is to create a contingency table where your compare the outcomes for two different measurements of the same population and determine if they tend to be in agreement or not. ## we can have a population and determine a categorical variable that result in two category (0, 1) like for example: species_A and species_B sample_species &lt;- rbinom(n = 100, size = 1, prob= 0.40) species_names &lt;- ifelse(sample_species == 1, &quot;species_B&quot;, &quot;species_A&quot;) names(sample_species) &lt;- species_names head(sample_species) #species_A species_A species_B species_B species_A species_A # 0 0 1 1 0 0 ## we can have the same population and determine a different categorical variable that result in two categories (0, 1) like for example: parasite and not_parasite sample_parasite &lt;- rbinom(n = 100, size = 1, prob= 0.40) parasite_names &lt;- ifelse(sample_parasite == 1, &quot;parasite&quot;, &quot;not_parasite&quot;) names(sample_parasite) &lt;- parasite_names head(sample_parasite) # parasite not_parasite not_parasite not_parasite parasite parasite # 1 0 0 0 1 1 We cross-tabulate these results and it is important to understand this table. The outcomes for sample_species and sample_parasite are 0 1 as indicated in the first row and column. The agreements are when the outcomes of both experiments are the same as each experiment was executed 0-0 and 1-1. All others are disagreements (e.g., 0-1, 1-0). cross_tab &lt;- table(sample_species, sample_parasite) cross_tab # sample_parasite #sample_species 0 1 # 0 26 33 # 1 27 14 We can now estimate if such agreements and disagreements are different using Chi-squared Test for Count Data with the function chisq.test(). chisq.test(cross_tab) # # Pearson&#39;s Chi-squared test with Yates&#39; continuity correction # #data: cross_tab #X-squared = 3.776, df = 1, p-value = 0.05199 In this case, we found that they were barely non-significant different. In this case, if the species is species_A this is more likely to not have a parasite. 7.8 Beyond t-test: McNemar’s test 15) This is a similar test to the Chi-square goodness of fit and it is used with paired nominal data in 2 × 2 contingency tables derived from dichotomous trait (i.e., YES or NO; Present or Absent), with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (see here). ## we can use the same cross tabulation example cross_tab &lt;- table(sample_species, sample_parasite) cross_tab # sample_parasite #sample_species 0 1 # 0 30 32 # 1 24 14 ## we apply the McNemar&#39;s test mcnemar.test(cross_tab) # # McNemar&#39;s Chi-squared test with continuity correction # #data: cross_tab #McNemar&#39;s chi-squared = 0.875, df = 1, p-value = 0.3496 In this case, we found that the same result as the Chi-squared test and McNemar-s test was also non-significant. In this case, if the species is species_A this is more likely to not have a parasite. 7.9 Beyond t-test: Fisher’s exact test 16) This is a similar test to the Chi-square goodness of fit and can be used with populations with small samples. The Fisher’s exact test will examine the significance of the association (contingency) between the two kinds of classification. The null hypothesis is that both classifications are dependent. We can use the same dataset as above with the function fisher.test(). fisher.test(cross_tab) # #Fisher&#39;s Exact Test for Count Data # #data: cross_tab #p-value = 0.04194 #alternative hypothesis: true odds ratio is not equal to 1 #95 percent confidence interval: # 0.1638957 1.0038597 #sample estimates: #odds ratio # 0.4122915 In this case, we found that they were barely significant different. In this case, if the species is species_A this is more likely to be a parasite. 7.10 Correlations 17) We can determine correlation between two variables using cor() and test for its significance using cor.test(). Correlation is the dependence or association between variables and it usually refers to the degree to which a pair of variables are linearly related. For practical uses, correlations can provide a predictive relationship that can derived from one variable on the other. However, the evidence of correlation does not mean a causal relationship (i.e., A does not necessarily cause B). Likewise, it is important to determine if such correlations are significant and these can be performed for normal or non-normal distributions (in this case use the use argument method = \"spearman\"). We will determine a correlation and test for its significance between two samples of the airway_data. cor(airway_data[,2], airway_data[,3]) #[1] 0.9646265 # we will assume that these are not normal cor.test(airway_data[,2], airway_data[,3], method = &quot;spearman&quot;) # # Spearman&#39;s rank correlation rho # #data: airway_data[, 2] and airway_data[, 3] #S = 4.8345e+11, p-value &lt; 2.2e-16 #alternative hypothesis: true rho is not equal to 0 #sample estimates: # rho #0.9499302 7.11 Comparing distributions 18) We can also determine if two variables come from the same distribution using Kolmogorov-Smirnov test ks.test(). The data does not require to be normal. We will determine if two samples of the airway_data have the same distribution. ks.test(airway_data[,2], airway_data[,3]) # # Two-sample Kolmogorov-Smirnov test # #data: airway_data[, 2] and airway_data[, 3] #D = 0.013413, p-value = 0.001896 #alternative hypothesis: two-sided # #Warning message: #In ks.test(airway_data[, 2], airway_data[, 3]) : # p-value will be approximate in the presence of ties We can conclude that these two samples do not have the same distribution (i.e., p-value = 0.001896). 19) We can compare two groups that have some individuals in such grops being exposed treatment verus control using from the R-package pROC using the functions roc() and roc.test(). This test will compare the AUC (area under the curve) of two ROC (receiver operating characteristic) curves. These are graphical plots useful in the diagnostic of a binary classifier (e.g., “control-0” or “exposed-1”) as its discrimination threshold is varied (i.e., what is the cutoff to consider a measurement as “exposed-1”). We will illustrate this test using two airway_data by assigning “control” and “exposed” to sample of rows. # you need to instal the package &quot;pROC&quot; install.packages(&quot;pROC&quot;) library(pROC) We can prepare two samples of airway_data and asign some of elements of each as “control” and “exposed.” ## set two samples sample_SRR1039508 &lt;- airway_data$SRR1039508 sample_SRR1039509 &lt;- airway_data$SRR1039509 ## get summaries of each summary_SRR1039508 &lt;- summary(sample_SRR1039508) summary_SRR1039508 # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.0 0.0 1.0 546.5 203.0 287372.0 summary_SRR1039509 &lt;- summary(sample_SRR1039509) summary_SRR1039509 # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.0 0.0 1.0 501.1 172.0 244921.0 ## assign &quot;control&quot; if less than 3rd quartile of each SRR1039508_binary &lt;- ifelse(sample_SRR1039508 &gt; summary_SRR1039508[5], &quot;exposed&quot;, &quot;control&quot;) ## assign &quot;control&quot; if more than 3rd quartile of each -- a reverse from last sample SRR1039509_binary &lt;- ifelse(sample_SRR1039509 &gt; summary_SRR1039509[5], &quot;control&quot;, &quot;exposed&quot;) ## create a roc object for each sample roc_SRR1039508 &lt;- roc(SRR1039508_binary, sample_SRR1039508, direction = &quot;&lt;&quot;) #Setting levels: control = control, case = exposed roc_SRR1039509 &lt;- roc(SRR1039509_binary, sample_SRR1039509, direction = &quot;&lt;&quot;) #Setting levels: control = control, case = exposed ## perform the roc.test and increase boot.n for a more precise p-value: roc.test(roc_SRR1039508, roc_SRR1039509, method=&quot;bootstrap&quot;, boot.n=1000) # |=========================================================================================================================================================================| 100% # # Bootstrap test for two ROC curves # #data: roc_SRR1039508 and roc_SRR1039509 #D = Inf, boot.n = 1000, boot.stratified = 1, p-value &lt; 2.2e-16 #alternative hypothesis: true difference in AUC is not equal to 0 #sample estimates: #AUC of roc1 AUC of roc2 # 1 0 We can conclude that for SRR1039508 and SRR1039509 are different in their distribution in reference to outcomes for “control” versus “exposed.” "],["regressions-and-anova-in-r.html", "Session 8 – Regressions and ANOVA in R 8.1 Linear regressions 8.2 Selecting predictors 8.3 Logistic regressions 8.4 Non-linear regressions 8.5 Toy datasets for regressions 8.6 ANOVA 8.7 One-way ANOVA 8.8 Two-way ANOVA 8.9 Post-hoc tests 8.10 Beyond ANOVA: Kruskal-Wallis Test 8.11 Toy datasets for ANOVA", " Session 8 – Regressions and ANOVA in R An important aspect of any statistical analyses is determining the relationship between variables and developing predictive models. A simple linear regression is a model of the relationship between a dependent (response) with a predictor variable and some error associated with it. This model can be expanded to include more than one predictor variable, which is known as a multivariate linear regression. These can be easily implemented in R. As mentioned, this workshop is not a biostatistics class and you might need to consult your notes for biostatistics. However, as mentioned by Long and Teetor (2019) here are some important considerations when you evaluate the usefulness of your regression models. This is provided by the function summary() on your model. a) Is the model statistically significant? Check the F statistic at the bottom of the summary. b) Are the regression coefficients significant? Check the coefficient t statistic and the p-value associate with such statistic. c) Is the model useful? Check the R2 and its significance. d) Does the model fit the provided data well? Plot the residuals and check regression diagnostics. e) Does the data satisfy the assumptions behind linear regression? Check whether the diagnostics confirm that a linear model is reasonable for your data. As happens in many cases, biological data are more complex and not necessarily follow a linear model. Many alternative and non-linear models might be more useful when dealing with such complexity. We will revise some non-parametric models. This can be visualized in the Anscombes quartet. Jan Vanhove What data patterns can lie behind a correlation coefficient?. 8.1 Linear regressions 1) The most basic form is a simple linear regression that can be performed directly in R using the function lm(). In this case, we have to two vectors with paired responses where a measurement of one predictor usually represented as x is corresponded with one dependent response represented as y. For this example, we will use the R-package MASS and its dataset mammals that contains brain and body weights for 62 species of land mammals. ## we can install MASS and load the dataset for this example install.packages(&quot;MASS&quot;) library(MASS) # assign mammals dataset to an object mammals_raw &lt;- mammals head(mammals_raw) # body brain #Arctic fox 3.385 44.5 #Owl monkey 0.480 15.5 #Mountain beaver 1.350 8.1 #Cow 465.000 423.0 #Grey wolf 36.330 119.5 #Goat 27.660 115.0 We can explore the relationship of bodyas predictor of brain by plotting their one-to-one relationship. Notice the formula brain ~ body inside the regression function lm() that correspond brain (y-axis) and body (x-axis). plot(x = mammals_raw$body, y = mammals_raw$brain) mtext(side=3, line=0.5, adj=0, &quot;raw data from mammal -- body vs brain&quot;) abline(lm(brain ~ body, data = mammals_raw), col = &quot;blue&quot;) # let&#39;s save this model to an object raw_brain_body &lt;- lm(brain ~ body, data = mammals_raw) This plot suggest that this might not be quite linear relationship. The reason is that well-known logistic relationship. For this transformation, we can use the function modify() of the package purrr # we have already downloaded this function before library(purrr) mammals_log &lt;- modify(mammals_raw,log10) head(mammals_log) # body brain #Arctic fox 0.5295587 1.648360 #Owl monkey -0.3187588 1.190332 #Mountain beaver 0.1303338 0.908485 #Cow 2.6674530 2.626340 #Grey wolf 1.5602654 2.077368 #Goat 1.4418522 2.060698 # now we can plot this transformed data plot(x = mammals_log$body, y = mammals_log$brain) mtext(side=3, line=0.5, adj=0, &quot;Log10 data from mammal -- body vs brain&quot;) abline(lm(brain ~ body, data = mammals_log), col = &quot;blue&quot;) # let&#39;s save this model to an object log_brain_body &lt;- lm(brain ~ body, data = mammals_log) # class of this regression object is &quot;lm&quot;. class(log_brain_body) #[1] &quot;lm&quot; This is a better plot and it is easier to visualize the linear trend in the log10 transformed data for body and brain size. 2) We can summarize the results of these simple linear regressions with the function summary(). First, we need to understand significance values and symbols (i.e., the asterisks *) next to sections of the output Symbol p-value range Interpretation *** less than 0.001 strongly significant and very likely to reject null (e.g., coefficient different than 0) ** 0.001 &lt; p-value &lt; 0.01 significant and most likely to reject null (e.g., coefficient different than 0) * 0.05 &lt; p-value &lt; 0.1 barely significant and likely to reject null (e.g., coefficient different than 0) . p-value &gt; 0.05 likely not significant and might not reject null (e.g., coefficient different than 0) no- asterisks p-value &gt; 0.05 does not reject null (e.g., coefficient is not different than 0) ## summary from untransformed data summary(raw_brain_body) #Call: #lm(formula = brain ~ body, data = mammals_raw) # #Residuals: # Min 1Q Median 3Q Max #-810.07 -88.52 -79.64 -13.02 2050.33 # #Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) #(Intercept) 91.00440 43.55258 2.09 0.0409 * #body 0.96650 0.04766 20.28 &lt;2e-16 *** #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # #Residual standard error: 334.7 on 60 degrees of freedom #Multiple R-squared: 0.8727, Adjusted R-squared: 0.8705 #F-statistic: 411.2 on 1 and 60 DF, p-value: &lt; 2.2e-16 ## summary from log10 transformed data summary(log_brain_body) #Call: #lm(formula = brain ~ body, data = mammals_log) # #Residuals: # Min 1Q Median 3Q Max #-0.74503 -0.21380 -0.02676 0.18934 0.84613 # #Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) #(Intercept) 0.92713 0.04171 22.23 &lt;2e-16 *** #body 0.75169 0.02846 26.41 &lt;2e-16 *** #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # #Residual standard error: 0.3015 on 60 degrees of freedom #Multiple R-squared: 0.9208, Adjusted R-squared: 0.9195 #F-statistic: 697.4 on 1 and 60 DF, p-value: &lt; 2.2e-16 Some of the highlights of the raw_brain_body output can be as follows. Coefficient name Coefficient value Statistic name Statistic value p-value Interpretation F 411.2 &lt; 2.2e-16 *** the model is strongly significant Intercept 91.00440 t-value 2.09 0.0409 * Intercept coefficient is barely significant Body 0.96650 t-value 20.28 &lt; 2.2e-16 *** Body coefficient is strongly significant R-squared 0.8727 the value is close to 1.0 and model is useful Some of the highlights of the log_brain_body output can be as follows. Coefficient name Coefficient value Statistic name Statistic value p-value Interpretation F 697.4 &lt; 2.2e-16 *** the model is strongly significant Intercept 0.92713 t-value 22.23 &lt; 2.2e-16 *** Intercept coefficient is strongly significant Body 0.75169 t-value 26.41 &lt; 2.2e-16 *** body coefficient is strongly significant R-squared 0.9208 the value is close to 1.0 and model is useful We can conclude that both models are significant and useful, but the log10 transformed data seems to fit better a linear model (e.g., see intercept row). 3) We can expand our regression models to more than one predictor (i.e., multiple linear regression) that can be performed again using the function lm(). In this case, we have to multiple vectors: one response and many predictors. For this example, we will use the dataset crabs that has morphological measurements on Leptograpsus crabs from R-package MASS. The variables that included: sp is species - “B” or “O” for blue or orange, sex, index is index 1:50 within each of the four groups, FL is frontal lobe size (mm), RW is rear width (mm), CL is carapace length (mm), CW is carapace width (mm) and BD is body depth (mm). ## we have already downloaded MASS and we assign the crabs dataset to an object library(MASS) crab_dataset &lt;- crabs head(crab_dataset) # sp sex index FL RW CL CW BD #1 B M 1 8.1 6.7 16.1 19.0 7.0 #2 B M 2 8.8 7.7 18.1 20.8 7.4 #3 B M 3 9.2 7.8 19.0 22.4 7.7 #4 B M 4 9.6 7.9 20.1 23.1 8.2 #5 B M 5 9.8 8.0 20.3 23.0 8.2 #6 B M 6 10.8 9.0 23.0 26.5 9.8 ## we will use BD as dependent and FL, RW, CL and CW as predictors. crab_multiple_lm &lt;- lm(formula = BD ~ FL+RW+CL+CW, data = crab_dataset) class(crab_multiple_lm) #[1] &quot;lm&quot; summary(crab_multiple_lm) #Call: #lm(formula = BD ~ FL + RW + CL + CW, data = crab_dataset) # #Residuals: # Min 1Q Median 3Q Max #-1.3518 -0.2291 0.0112 0.2664 1.0965 # #Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) #(Intercept) -0.97616 0.15296 -6.382 1.25e-09 *** #FL 0.42022 0.05585 7.524 1.91e-12 *** #RW 0.03768 0.03209 1.174 0.242 #CL 0.59511 0.06825 8.720 1.22e-15 *** #CW -0.30560 0.04935 -6.193 3.43e-09 *** #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # #Residual standard error: 0.4208 on 195 degrees of freedom #Multiple R-squared: 0.9852, Adjusted R-squared: 0.9849 #F-statistic: 3247 on 4 and 195 DF, p-value: &lt; 2.2e-16 Based on these statistics, multiple regression is significant (F-statistic), the model seems to be useful (Multiple R-squared is very close to 1), all coefficients with the exception for CL (carapace length) were significant. 4) We can perform some diagnostic plots to checks for heteroscedasticity, normality, and influential observerations. # diagnostic plots layout(matrix(c(1,2,3,4),2,2)) plot(crab_multiple_lm) To understand these plots, we need to look for indications that data might not follow normality or has bias. These include: (a) the Residual vs Fitted should have a random distribution without any pattern (our plot shows that); (b) the Normal Q-Q show fall into oblique dashed line that supports relative multivariate normality (i.e., our multiple linear method is valid) and (c) the plots Scale-Location and Residuals vs Leverage should fall close to the center (i.e., these plots are more or less OK) 5) You can also get a list of influential measurements using functions influence.measures() and influence(). These measurements might be outliers or errors in data input (i.e., miss typing, incorrect instrument calibration, contamination, etc.) ## Notice values with an asterisk (*) influence_values &lt;- influence.measures(crab_multiple_lm) #Influence measures of # lm(formula = BD ~ FL + RW + CL + CW, data = crab_dataset) : # # dfb.1_ dfb.FL dfb.RW dfb.CL dfb.CW dffit cov.r cook.d hat inf #1 0.230383 1.06e-01 -1.27e-01 -1.15e-01 9.97e-02 0.272172 1.022 1.48e-02 0.04052 #2 -0.009954 -4.12e-05 1.55e-03 -6.81e-05 5.21e-04 -0.010550 1.053 2.24e-05 0.02556 #3 0.018286 6.27e-03 -9.58e-03 -9.30e-03 9.08e-03 0.022461 1.056 1.01e-04 0.02866 #... #46 0.076014 -8.84e-02 2.99e-01 1.25e-01 -1.87e-01 -0.426499 0.914 3.55e-02 0.03051 * #47 0.037663 -1.85e-02 3.83e-02 4.76e-02 -6.15e-02 -0.088701 1.057 1.58e-03 0.03509 #48 -0.027395 8.70e-03 -7.23e-02 -3.70e-02 6.03e-02 0.115458 1.077 2.68e-03 0.05319 * #49 -0.017943 1.88e-02 -4.78e-02 -3.53e-02 4.67e-02 0.071147 1.077 1.02e-03 0.04985 * #50 -0.023874 7.47e-05 -3.10e-02 -2.11e-02 3.51e-02 0.063645 1.110 8.14e-04 0.07675 * #51 0.127156 8.62e-03 -2.53e-02 -8.99e-03 2.53e-03 0.133549 1.052 3.58e-03 0.03666 #32 -0.020620 -6.29e-02 -1.96e-02 -3.45e-03 4.20e-02 0.145671 1.028 4.25e-03 0.02396 #... ## now we can get the identity of these values organized in a vector by focusing in column &#39;hat&#39; influence_df &lt;- influence(crab_multiple_lm) head(influence_df) # we can the 10 most influential value order(influence_df$hat, decreasing = TRUE)[1:10] #[1] 50 145 199 195 98 200 99 144 186 97 6) We can further explore our simple and multiple regression examples by calling different regression statistics. We have two regression objects log_brain_body and crab_multiple_lm. ANOVA table, we will explore this later in this session. We use the function anova(). anova(log_brain_body) #Analysis of Variance Table # #Response: brain # Df Sum Sq Mean Sq F value Pr(&gt;F) #body 1 63.409 63.409 697.42 &lt; 2.2e-16 *** #Residuals 60 5.455 0.091 #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 anova(crab_multiple_lm) #Analysis of Variance Table # #Response: BD # Df Sum Sq Mean Sq F value Pr(&gt;F) #FL 1 2276.68 2276.68 12858.0018 &lt; 2.2e-16 *** #RW 1 0.57 0.57 3.1984 0.07526 . #CL 1 15.52 15.52 87.6332 &lt; 2.2e-16 *** #CW 1 6.79 6.79 38.3516 3.43e-09 *** #Residuals 195 34.53 0.18 #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Get the model coefficients using the function coefficients() or coef(). coefficients(log_brain_body) #(Intercept) body # 0.9271269 0.7516859 coef(crab_multiple_lm) #(Intercept) FL RW CL CW #-0.97615668 0.42022057 0.03768281 0.59511478 -0.30559587 Get the model confidence intervals for regression coefficients (i.e., range that contains the estimate regression coefficients) using the function confint(). confint(log_brain_body) 2.5 % 97.5 % #(Intercept) 0.8436923 1.0105616 #body 0.6947503 0.8086215 confint(crab_multiple_lm) # 2.5 % 97.5 % #(Intercept) -1.27783361 -0.6744798 #FL 0.31007030 0.5303708 #RW -0.02560597 0.1009716 #CL 0.46051803 0.7297115 #CW -0.40291710 -0.2082746 Notice the confidence interval for RW is (-0.02560597, 0.1009716) and this range contains 0.0 (i.e., zero). This is consistent with the summary(crab_multiple_lm) where RW was not significant, see 3). 8.2 Selecting predictors 7) We can further improve our regression by selecting the best predictors and removing those that do not contribute to the regression. The function step() can help you to determine what predictors to keep . We will illustrate this using our model crab_multiple_lm. ## we will assign this as our full model full_model &lt;- crab_multiple_lm full_model #Call: #lm(formula = BD ~ FL + RW + CL + CW, data = crab_dataset) # #Coefficients: #(Intercept) FL RW CL CW # -0.97616 0.42022 0.03768 0.59511 -0.30560 We can do this using backward or both approaches (they usually reach the same conclusion). reduced_model &lt;- step(full_model, direction = &quot;backward&quot;) #Start: AIC=-341.31 #BD ~ FL + RW + CL + CW # # Df Sum of Sq RSS AIC #- RW 1 0.2442 34.772 -341.90 #&lt;none&gt; 34.527 -341.31 #- CW 1 6.7907 41.318 -307.40 #- FL 1 10.0234 44.551 -292.34 #- CL 1 13.4637 47.991 -277.46 #Step: AIC=-341.9 #BD ~ FL + CL + CW # # Df Sum of Sq RSS AIC #&lt;none&gt; 34.772 -341.90 #- CW 1 7.5018 42.273 -304.83 #- CL 1 15.2099 49.981 -271.33 #- FL 1 17.0716 51.843 -264.02 reduced_model # #Call: #lm(formula = BD ~ FL + CL + CW, data = crab_dataset) # #Coefficients: #(Intercept) FL CL CW We can notice that the predictor RW has been dropped from the reduced model and we can get some statistics on this improved model. summary(reduced_model) #Call: #lm(formula = BD ~ FL + CL + CW, data = crab_dataset) # #Residuals: # Min 1Q Median 3Q Max #-1.34264 -0.23109 0.00893 0.26523 1.05895 # #Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) #(Intercept) -0.92444 0.14663 -6.305 1.87e-09 *** #FL 0.45656 0.04654 9.810 &lt; 2e-16 *** #CL 0.55718 0.06018 9.259 &lt; 2e-16 *** #CW -0.27594 0.04243 -6.503 6.41e-10 *** #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # #Residual standard error: 0.4212 on 196 degrees of freedom #Multiple R-squared: 0.9851, Adjusted R-squared: 0.9849 #F-statistic: 4320 on 3 and 196 DF, p-value: &lt; 2.2e-16 8) One of the most important uses of regressions is predictions of the dependent variable. For example, given a set of measurements on your predictors, can you predict an outcome. This can be done using the function predict(). As an example, we will predict brain size of some mammals that are not in the mammals_raw dataset. ## recall the regression and the predictor name for the model: log_brain_body log_brain_body #Call: #lm(formula = brain ~ body, data = mammals_log) # #Coefficients: #(Intercept) body # 0.9271 0.7517 We cab revise species in mammals_raw. head(mammals_raw) # body brain #Arctic fox 3.385 44.5 #Owl monkey 0.480 15.5 #Mountain beaver 1.350 8.1 #Cow 465.000 423.0 #Grey wolf 36.330 119.5 #Goat 27.660 115.0 rownames(mammals_raw) #[1] &quot;Arctic fox&quot; &quot;Owl monkey&quot; &quot;Mountain beaver&quot; &quot;Cow&quot; # [5] &quot;Grey wolf&quot; &quot;Goat&quot; &quot;Roe deer&quot; &quot;Guinea pig&quot; # [9] &quot;Verbet&quot; &quot;Chinchilla&quot; &quot;Ground squirrel&quot; &quot;Arctic ground squirrel&quot; #[13] &quot;African giant pouched rat&quot; &quot;Lesser short-tailed shrew&quot; &quot;Star-nosed mole&quot; &quot;Nine-banded armadillo&quot; #[17] &quot;Tree hyrax&quot; &quot;N.A. opossum&quot; &quot;Asian elephant&quot; &quot;Big brown bat&quot; #[21] &quot;Donkey&quot; &quot;Horse&quot; &quot;European hedgehog&quot; &quot;Patas monkey&quot; #[25] &quot;Cat&quot; &quot;Galago&quot; &quot;Genet&quot; &quot;Giraffe&quot; #[29] &quot;Gorilla&quot; &quot;Grey seal&quot; &quot;Rock hyrax-a&quot; &quot;Human&quot; #[33] &quot;African elephant&quot; &quot;Water opossum&quot; &quot;Rhesus monkey&quot; &quot;Kangaroo&quot; #[37] &quot;Yellow-bellied marmot&quot; &quot;Golden hamster&quot; &quot;Mouse&quot; &quot;Little brown bat&quot; #[41] &quot;Slow loris&quot; &quot;Okapi&quot; &quot;Rabbit&quot; &quot;Sheep&quot; #[45] &quot;Jaguar&quot; &quot;Chimpanzee&quot; &quot;Baboon&quot; &quot;Desert hedgehog&quot; #[49] &quot;Giant armadillo&quot; &quot;Rock hyrax-b&quot; &quot;Raccoon&quot; &quot;Rat&quot; #[53] &quot;E. American mole&quot; &quot;Mole rat&quot; &quot;Musk shrew&quot; &quot;Pig&quot; #[57] &quot;Echidna&quot; &quot;Brazilian tapir&quot; &quot;Tenrec&quot; &quot;Phalanger&quot; #[61] &quot;Tree shrew&quot; &quot;Red fox&quot; We will add three more mammals body weight in kg and estimate their brain weight in g by create a new data frame with the new body sizes. new_body &lt;- data.frame(body = c(272, 495, 160, 3.13), stringsAsFactors = FALSE) rownames(new_body) &lt;- c(&quot;grizzly_bear&quot;, &quot;bactrian_camel&quot;, &quot;llama&quot;, &quot;capuchin_monkey&quot;) new_body # body #grizzly_bear 272.00 #bactrian_camel 495.00 #llama 160.00 #capuchin_monkey 3.13 Remember that the model is for log10 transformed data, so we need to transform accordingly. library(purrr) new_body_log &lt;- purrr::modify(new_body,log10) new_body_log # body #grizzly_bear 2.4345689 #bactrian_camel 2.6946052 #llama 2.2041200 #capuchin_monkey 0.4955443 We can estimate their brain weight in g of these mammals using the function predict(). new_brain_log &lt;- predict(log_brain_body, new_body_log) new_brain_log # grizzly_bear bactrian_camel llama capuchin_monkey # 2.757158 2.952624 2.583933 1.299621 We can transform this log10 weight brains back by exponentiation with base 10. new_brain &lt;- 10^new_brain_log new_brain # grizzly_bear bactrian_camel llama capuchin_monkey # 571.6868 896.6517 383.6480 19.9352 8.3 Logistic regressions 9) We can use a logistic regression when our dependent variable is binary (i.e., 1 or 0; yes or no; A or B; male or female) from a set of continuous predictor variables that determine such outcome. We can implement such regressions using the function glm() for a generalized linear model (GLM). We will dataset catsthat has anatomical data from domestic cats including sex and two continuous variables (body and heart weight). This dataset is part of the R-package MASS. ## load MASS package and ‘cats’ dataset library(MASS) cats_data &lt;- cats # check structure str(cats_data) #&#39;data.frame&#39;: 144 obs. of 3 variables: # $ Sex: Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... # $ Bwt: num 2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ... # $ Hwt: num 7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ... We can now run the model with the formula with Sex as a binary dependent and the two continuous as predictors. Notice the argument for type or family of distribution family = binomial(). cats_data_logistic_fit &lt;- glm(Sex ~ Bwt + Hwt, data = cats_data, family = binomial()) summary(cats_data_logistic_fit) #Call: #glm(formula = Sex ~ Bwt + Hwt, family = binomial(), data = cats_data) # #Deviance Residuals: # Min 1Q Median 3Q Max #-2.1727 -0.7815 0.3141 0.7476 1.8189 # #Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) #(Intercept) -8.69896 1.69982 -5.118 3.09e-07 *** #Bwt 3.55391 0.88208 4.029 5.60e-05 *** #Hwt 0.02295 0.16146 0.142 0.887 #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # #(Dispersion parameter for binomial family taken to be 1) # # Null deviance: 181.90 on 143 degrees of freedom #Residual deviance: 132.24 on 141 degrees of freedom #AIC: 138.24 # #Number of Fisher Scoring iterations: 5 For the summary, we can that the intercept and body weight are strongly significant, while heart weight is not. This suggest that the sex of the cat has a strong association of the weight of the cat, while not its heart weight. For body weight, its positive coefficient (3.55391) suggests that heavier cats are most likely males while controlling for heart weight. Now, we can plot this logistic regression. cats_data_newdata &lt;- expand.grid(Bwt = pretty(cats_data$Bwt, 20), Hwt = pretty(cats_data$Hwt, 5)) cats_data_newdata$Sex &lt;- predict(cats_data_logistic_fit, newdata = cats_data_newdata, type = &quot;response&quot;) ## we require library ggplot2 library(ggplot2) ggplot(cats_data_newdata, aes(x = Bwt, y = Sex)) + geom_line() + facet_wrap(~Hwt) 8.4 Non-linear regressions 10) We can have a dataset that is not adjust to a linear model (as in many cases in biological experiment outcomes), so we have to use non-parametric models. Several methods can perform such complex regression include those of the R-packages drc. We will illustrate some of these. ## we need to install the &#39;drc&#39; package install.packages(&quot;drc&quot;) library(drc) ## we install a library with examples of exponential growth install.packages(&quot;growthrates&quot;) library(growthrates) We can determine estimate a regression that fit an exponential curve like bacterial growth ## load an example data(bactgrowth) head(bactgrowth) # strain replicate conc time value #1 T 2 0 0 0.013 #2 T 2 0 1 0.014 #3 T 2 0 2 0.017 #4 T 2 0 3 0.022 #5 T 2 0 4 0.030 #6 T 2 0 5 0.039 Now, we subset this data for strain T and replicate 2 (i.e., two conditions) by using &amp;. bactgrowth_strain_T &lt;- subset(bactgrowth, strain == &quot;T&quot; &amp; replicate == 2 &amp; conc == 31.25) head(bactgrowth_strain_T) # strain replicate conc time value #249 T 2 31.25 0 0.011 #250 T 2 31.25 1 0.010 #251 T 2 31.25 2 0.010 #252 T 2 31.25 3 0.010 #253 T 2 31.25 4 0.010 #254 T 2 31.25 5 0.011 # visualize this graph plot(value~time, data = bactgrowth_strain_T) We can fit an exponential curve using the function drm() of the package drc. Notice that the type of curve will depend on its parametrization as indicated by the argument fct =. In this case is fct = G.4() or , for more information check drc manual and this page. ## fct for exponential growth model_bactgrowth_strain_T &lt;- drc::drm(value ~ time, fct = G.4(), data = bactgrowth_strain_T) summary(model_bactgrowth_strain_T) #Model fitted: Gompertz (4 parms) # #Parameter estimates: # # Estimate Std. Error t-value p-value #b:(Intercept) -0.1799145 0.0061157 -29.419 &lt; 2.2e-16 *** #c:(Intercept) 0.0105954 0.0003694 28.683 &lt; 2.2e-16 *** #d:(Intercept) 0.1008913 0.0013430 75.122 &lt; 2.2e-16 *** #e:(Intercept) 16.6860271 0.1201873 138.833 &lt; 2.2e-16 *** #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # #Residual standard error: # #0.001105757 (27 degrees of freedom) ## we can plot the fit of this function plot(model_bactgrowth_strain_T) We can also estimate new values of growth of the bacteria strain T (dependent) for new time (predictor) using the function predict(). new_time &lt;- data.frame(time = c(31,50,100,1000)) rownames(new_time) &lt;- c(&quot;time_31&quot;, &quot;time_50&quot;, &quot;time_100&quot;, &quot;time_1000&quot;) new_values_growth &lt;- predict(model_bactgrowth_strain_T, new_time) names(new_values_growth) &lt;- c(&quot;time_31&quot;, &quot;time_50&quot;, &quot;time_100&quot;, &quot;time_1000&quot;) new_values_growth # time_31 time_50 time_100 time_1000 #0.0942721 0.1006664 0.1008913 0.1008913 11) Like most non-linear models, it requires some parametrization determined by the curve that you observe by plotting your data. Another useful function is nls(). We can fit a exponential growth to our bactgrowth_strain_T using nls(). model_bactgrowth_strain_T_nls &lt;- nls(value ~ a*exp(r*time), data = bactgrowth_strain_T,start = list(a = 0.5, r = 0.2)) summary(model_bactgrowth_strain_T_nls) #Formula: value ~ a * exp(r * time) # #Parameters: # Estimate Std. Error t value Pr(&gt;|t|) #a 0.011133 0.001356 8.213 4.69e-09 *** #r 0.076416 0.004887 15.636 1.15e-15 *** #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 # #Residual standard error: 0.008326 on 29 degrees of freedom # #Number of iterations to convergence: 11 #Achieved convergence tolerance: 3.249e-06 coef(model_bactgrowth_strain_T_nls) # a r #0.01113342 0.07641632 model_coeff &lt;- coef(model_bactgrowth_strain_T_nls) # plot this graph plot(value ~ time, data = bactgrowth_strain_T, main = &quot;bactgrowth_strain_T&quot;) lines(bactgrowth_strain_T$time, model_coeff[1]*exp(model_coeff[2]*bactgrowth_strain_T$time), col = &quot;orange2&quot;, lwd = 2) # this is the same graph #plot(value ~ time, data = bactgrowth_strain_T, main = &quot;bactgrowth_strain_T&quot;) #lines(bactgrowth_strain_T$time, 0.01113342*exp(0.07641632*bactgrowth_strain_T$time), col = &quot;orange2&quot;, lwd = 2) We can also estimate new values of growth of the bacteria strain T (dependent) for new time (predictor) using the function predict(). new_time &lt;- data.frame(time = c(31,50,100,1000)) rownames(new_time) &lt;- c(&quot;time_31&quot;, &quot;time_50&quot;, &quot;time_100&quot;, &quot;time_1000&quot;) new_values_growth_nls &lt;- predict(model_bactgrowth_strain_T_nls, new_time) names(new_values_growth_nls) &lt;- c(&quot;time_31&quot;, &quot;time_50&quot;, &quot;time_100&quot;, &quot;time_1000&quot;) new_values_growth_nls # time_31 time_50 time_100 time_1000 #1.189684e-01 5.081454e-01 2.319249e+01 1.713226e+31 8.5 Toy datasets for regressions Now that you have an illustrated example of regressions, you can choose among the following examples to do one of each exercise (e.g., linear regression, selecting predictors, logistic regressions, and non-linear regressions): a) R-package ade4 dataset aravo for distribution of Alpine plants in Aravo (Valloire, France). install.packages(&quot;ade4&quot;) library(ade4) data(aravo) aravo_traits &lt;- aravo$traits head(aravo_traits) # Height Spread Angle Area Thick SLA N_mass Seed #Agro.rupe 6 10 80 60.0 0.12 8.1 218.70 0.08 #Alop.alpi 5 20 20 190.9 0.20 15.1 203.85 0.21 #Anth.nipp 15 5 50 280.0 0.08 18.0 219.60 0.54 #Heli.sede 0 30 80 600.0 0.20 10.6 233.20 1.72 #Aven.vers 12 30 60 420.0 0.14 12.5 156.25 1.17 #Care.rosa 30 20 80 180.0 0.40 6.5 208.65 1.68 b) R-package ade4 dataset bacteria for total number of the 21 amino acids in 43 species of bacteria. install.packages(&quot;ade4&quot;) library(ade4) data(bacteria) bacteria_espaa &lt;- bacteria$espaa head(bacteria_espaa) # Ala Arg Asn Asp Cys Gln Glu Gly His Ile Leu Lys Met Phe Pro Ser Stp Thr Trp Tyr Val #AERPECG 60384 48823 12907 24567 5896 12058 41892 54161 12189 32902 72081 22335 12476 17397 40897 47682 2619 29697 8324 21272 55421 #AQUAECG 28326 23565 17294 20754 3770 9791 46216 32425 7430 35203 50946 45092 8979 24736 19580 23036 1490 20221 4495 19887 38211 #ARCFUCG 50661 36713 20664 31358 7358 11448 56642 46562 9733 46172 60810 43489 16109 29467 24835 35350 2088 26833 6639 23426 55527 #BACHDCG 85555 55731 41785 58649 8393 47325 90358 81279 27972 80022 115667 67022 30743 51345 44287 64903 3558 64274 13156 39079 86451 #BACSUCG 91276 48698 46474 61327 9340 45257 85399 82164 26892 87050 114745 82768 31755 53186 43835 74444 3628 64270 12238 41133 80173 #BORBUCG 12555 8898 20318 14526 1822 6305 18931 14559 3403 29993 29020 28498 5008 17514 7044 20862 773 11018 1404 11814 14980 c) R-package ade4 dataset carni19 for carnivores body size and range. install.packages(&quot;ade4&quot;) library(ade4) data(carni70) carni70_size_range &lt;- carni70$tab head(carni70_size_range) # size range #Puma_concolor 37.01 36.00 #Herpailurus_yaguaroundi 2.59 24.00 #Leopardus_wiedii 3.20 21.75 #Leopardus_pardalis 7.90 24.50 #Oreailurus_jacobita 3.99 1.75 #Oncifelis_colocolo 2.99 6.25 d) R-package ade4 dataset cnc2003 for a data frame about cinema attendanace in France. This data frame contains the following variables, popuis the population department in million inhabitants. entris the number of movie theater visitors in million. receis the takings from ticket offices. seanis the number of proposed shows in thousands. commis the number of equipped communes in movie theaters (units). etabis the number of active movie theaters (units). salleis the number of active screens. fautis the number of proposed seats. artesis the number of movie theaters offering Art and Essay movies. multiis the number of active multiplexes. departis the name of the department. regis the administrative region of the department. install.packages(&quot;ade4&quot;) library(ade4) data(cnc2003) cnc2003_data &lt;- cnc2003 head(cnc2003_data) # popu entr rece sean comm etab salle faut artes multi depart reg #D1 0.515 0.769 4.065 26 16 20 35 6288 12 0 Ain Rhone-Alpes #D2 0.536 0.731 3.942 28 14 15 38 7403 8 0 Aisne Picardie #D3 0.345 0.499 2.789 17 7 11 28 3956 4 0 Allier Auvergne #D4 0.140 0.453 2.262 17 13 15 23 3480 7 0 Alpes de Hautes Provence Provence-Alpes-Coted&#39;Azur #D5 0.121 0.522 2.908 21 19 23 35 6053 5 0 Hautes Alpes Provence-Alpes-Coted&#39;Azur #D6 1.011 3.520 21.731 111 23 42 94 16764 8 1 Alpes Maritimes Provence-Alpes-Coted&#39;Azur e) R-package mlbench dataset PimaIndiansDiabetes2 for a data frame with 768 observations on 10 variables for the Pima Indians Diabetes Database: pregnant Number of times pregnant; glucose Plasma glucose concentration (glucose tolerance test); pressure Diastolic blood pressure (mm Hg); triceps Triceps skin fold thickness (mm); insulin 2-Hour serum insulin (mu U/ml); mass Body mass index (weight in kg/(height in m)^2); pedigree Diabetes pedigree function; age Age (years); diabetes Class variable (test for diabetes); prob_diabetes probability of diabetes (1 = yes, 0 = no). install.packages(&quot;mlbench&quot;) library(mlbench) data(PimaIndiansDiabetes2) PimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2) PimaIndiansDiabetes2$prob_diabetes &lt;- ifelse(PimaIndiansDiabetes2$diabetes == &quot;pos&quot;, 1, 0) head(PimaIndiansDiabetes2) # pregnant glucose pressure triceps insulin mass pedigree age diabetes prob_diabetes #4 1 89 66 23 94 28.1 0.167 21 neg 0 #5 0 137 40 35 168 43.1 2.288 33 pos 1 #7 3 78 50 32 88 31.0 0.248 26 pos 1 #9 2 197 70 45 543 30.5 0.158 53 pos 1 #14 1 189 60 23 846 30.1 0.398 59 pos 1 #15 5 166 72 19 175 25.8 0.587 51 pos 1 f) R-package mlbench dataset BostonHousing for Housing data for 506 census tracts of Boston from the 1970 census. Variables include crim per capita crime rate by town; zn proportion of residential land zoned for lots over 25,000 sq.ft; indus proportion of non-retail business acres per town; chas Charles River dummy variable (= 1 if tract bounds river; 0 otherwise); nox nitric oxides concentration (parts per 10 million)rmaverage number of rooms per dwelling; age proportion of owner-occupied units built prior to 1940; dis weighted distances to five Boston employment centres; rad index of accessibility to radial highways; tax full-value property-tax rate per USD 10,000; ptratio pupil-teacher ratio by town; b 1000(B−0.63)2whereBis the proportion of blacks by town; lstat percentage of lower status of the population; medv median value of owner-occupied homes in USD 1000’s. install.packages(&quot;mlbench&quot;) library(mlbench) data(BostonHousing) BostonHousing_data &lt;- BostonHousing head(BostonHousing_data) # crim zn indus chas nox rm age dis rad tax ptratio b lstat medv #1 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 24.0 #2 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 21.6 #3 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.7 #4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4 #5 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 36.2 #6 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 28.7 8.6 ANOVA An analysis of variance (ANOVA) will help you to determine if a dependent variable change according to the levels of one or more categorical independent variables. As part of this test, ANOVA will determine if a particular group has a different mean from the overall mean value of the data. This is determined by checking the variance of each of these individual group against the overall variance of all the data. A significant result is reported if one or more groups falls outside the range of variation predicted by the null hypothesis (all group means are equal). ANOVA will test a null hypothesis (H0) that indicates a no difference in means versus the alternative (Ha) that indicates that the means are different between groups. 8.7 One-way ANOVA 12) The most common types of ANOVA include a one-way ANOVA with one independent variable and a two-way ANOVA with two independent variables. In order to perform an ANOVA, we use the function aov(). For an example of one-way ANOVA, we will continue using the crab_dataset ## we have already downloaded MASS and we assign the crabs dataset to an object library(MASS) crab_dataset &lt;- crabs head(crab_dataset) # sp sex index FL RW CL CW BD #1 B M 1 8.1 6.7 16.1 19.0 7.0 #2 B M 2 8.8 7.7 18.1 20.8 7.4 #3 B M 3 9.2 7.8 19.0 22.4 7.7 #4 B M 4 9.6 7.9 20.1 23.1 8.2 #5 B M 5 9.8 8.0 20.3 23.0 8.2 #6 B M 6 10.8 9.0 23.0 26.5 9.8 We notice two categorical variables sp for species as “B” or “O” for blue or orange and sex for “M” males and “F” females. For one-way ANOVA, we will determine if species variable sp that includes two categories or groups “B” or “O” have different means for the continuous variable CL or the carapace length (mm). We use functions aov() and summary() CL_anova_one_way &lt;- aov(CL ~ sp, data = crab_dataset) CL_anova_one_way #Call: # aov(formula = CL ~ sp, data = crab_dataset) # #Terms: # sp Residuals #Sum of Squares 838.451 9246.853 #Deg. of Freedom 1 198 # #Residual standard error: 6.833833 #Estimated effects may be unbalanced summary(CL_anova_one_way) # Df Sum Sq Mean Sq F value Pr(&gt;F) #sp 1 838 838.5 17.95 3.47e-05 *** #Residuals 198 9247 46.7 #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Overall, the Pr(&gt;F) column provides the p-value (significance) of the F-statistic, which is very low (p &lt; 0.001) and suggest that the we find litter support for the null hypothesis of no difference among group means of sp (i.e., species “B” or “O”) based on CL or the carapace length (mm). Therefore, the measurements of carapace length seem to be different between the two species of crab. 8.8 Two-way ANOVA 13) We can perform a two-way ANOVA by including two independent variables. For a two-way ANOVA, we will both categorical variables sp for species as “B” or “O” for blue or orange and sex for “M” males and “F” females. As a dependent, we will use again the continuous variable CL or the carapace length (mm). We use functions aov() and summary() CL_anova_two_way &lt;- aov(CL ~ sp + sex, data = crab_dataset) CL_anova_two_way #Call: # aov(formula = CL ~ sp + sex, data = crab_dataset) # #Terms: # sp sex Residuals #Sum of Squares 838.451 111.154 9135.699 #Deg. of Freedom 1 1 197 # #Residual standard error: 6.809854 #Estimated effects may be unbalanced summary(CL_anova_two_way) # Df Sum Sq Mean Sq F value Pr(&gt;F) #sp 1 838 838.5 18.080 3.27e-05 *** #sex 1 111 111.2 2.397 0.123 #Residuals 197 9136 46.4 #--- #Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Overall, the Pr(&gt;F) column provides two p-values for the corresponding F-statistic and, again, only the species type shows evidence of difference among group means of their CL or the carapace length (mm). In contrast, sex does not explain differences in the measurements of carapace length. 8.9 Post-hoc tests 14) We can perform a post-hoc test to find the differences among group means. One of such tests is Tukey’s Honestly Significant Difference (Tukey’s HSD) for pairwise comparisons. We use the function TukeyHSD() on the resulting ANOVA object. ## one-way ANOVA TukeyHSD(CL_anova_one_way) # Tukey multiple comparisons of means # 95% family-wise confidence level # #Fit: aov(formula = CL ~ sp, data = crab_dataset) # #$sp # diff lwr upr p adj #O-B 4.095 2.189144 6.000856 3.47e-05 ## two-way ANOVA TukeyHSD(CL_anova_two_way) # Tukey multiple comparisons of means # 95% family-wise confidence level # #Fit: aov(formula = CL ~ sp + sex, data = crab_dataset) # #$sp # diff lwr upr p adj #O-B 4.095 2.195772 5.994228 3.27e-05 # #$sex # diff lwr upr p adj #M-F 1.491 -0.4082279 3.390228 0.123181 Again, these results support that sp for species as “B” or “O” is important when comparing means of CL or the carapace length (mm) of these crabs. 8.10 Beyond ANOVA: Kruskal-Wallis Test 15) Some samples that you want to compare might have a non-normal or unknown distributions. In this case, you can use non-parametric methods like Kruskal-Wallis Test for to determine if samples have different medians. The null hypothesis is that the medians of all groups are all equal, while the alternative hypothesis is that at least one sample median is different from the median of at least one other group or sample. We can reuse the crabs dataset from the package ‘MASS’ – this test does not assume normality. ## we can get crabs dataset library(MASS) crab_dataset &lt;- crabs head(crab_dataset) # sp sex index FL RW CL CW BD #1 B M 1 8.1 6.7 16.1 19.0 7.0 #2 B M 2 8.8 7.7 18.1 20.8 7.4 #3 B M 3 9.2 7.8 19.0 22.4 7.7 #4 B M 4 9.6 7.9 20.1 23.1 8.2 #5 B M 5 9.8 8.0 20.3 23.0 8.2 #6 B M 6 10.8 9.0 23.0 26.5 9.8 We can implement this Kruskal-Wallis Test using the function kruskal.test() for CL carapace length (mm) as dependent and grouping by sex or species. ## for sex as grouping kruskal.test(CL ~ sex, data = crab_dataset) # Kruskal-Wallis rank sum test # #data: CL by sex #Kruskal-Wallis chi-squared = 1.8424, df = 1, p-value = 0.1747 ## for species as grouping kruskal.test(CL ~ sp, data = crab_dataset) # Kruskal-Wallis rank sum test # #data: CL by sp #Kruskal-Wallis chi-squared = 15.698, df = 1, p-value = 7.431e-05 These results agree with the one-way ANOVA results indicated on 12). 8.11 Toy datasets for ANOVA Now that you have examples of ANOVA, you can choose among the following examples for further examples a) R-package agridat dataset fisher.barley for results of trials of 5 varieties of barley were conducted at 6 stations in Minnesota during the years 1931-1932. This result in a data frame with 60 observations on the following 4 variables: yield yield, bu/ac; gen genotype/variety, 5 levels; env environment/location, 2 levels; year year, 1931/1932. install.packages(&quot;agridat&quot;) library(agridat) data(fisher.barley) fisher_barley &lt;- fisher.barley head(fisher_barley) # yield gen env year #1 81.0 Manchuria UniversityFarm 1931 #2 80.7 Manchuria UniversityFarm 1932 #3 146.6 Manchuria Waseca 1931 #4 100.4 Manchuria Waseca 1932 #5 82.3 Manchuria Morris 1931 #6 103.1 Manchuria Morris 1932 b) R-package agridat dataset gumpertz.pepper for results with 800 observations on the following 6 variables: field field factor, 2 levels; row x ordinate; quadrat y ordinate; disease presence (Y) or absence (N) of disease; water soil moisture percent; leaf leaf assay count. Each field is 20 rows by 20 quadrates, with 2 to 3 bell pepper plants per plot. If any plant was wilted, dead, or had lesions, the Phytophthora disease was considered to be present in the plot. The soil pathogen load was assayed as the number of leaf disks colonized out of five. In field 2, the pattern of disease presence appears to follow soil water content. In field 1, no obvious trends were present. install.packages(&quot;agridat&quot;) library(agridat) data(gumpertz.pepper) gumpertz_pepper &lt;- gumpertz.pepper head(gumpertz_pepper) # field row quadrat disease water leaf #1 F1 1 1 N 15.05 5 #2 F1 1 2 N 14.32 2 #3 F1 1 3 N 13.99 3 #4 F1 1 4 N 13.85 0 #5 F1 1 5 N 13.16 5 #6 F1 1 6 N 11.81 5 "],["exploring-multidimensional-data-in-r.html", "Session 9 – Exploring Multidimensional Data in R 9.1 Principal Component Analysis 9.2 Multidimensional scaling 9.3 Cluster analysis: DFA 9.4 Cluster analysis: k-means (unsupervised)", " Session 9 – Exploring Multidimensional Data in R Many datasets derive from multiple variables measured from hundreds of individuals and many of such parameters are obtained simultaneously (i.e., multidimensional) and together are often difficult to interpret. Most researchers aim to reduce such complexity, identify groups or increase its interpretability using diverse methods like principal component analysis (PCA), multidimensional scaling (MDS), cluster analyses among others. We will explore some of these methods. 9.1 Principal Component Analysis 1) PCA is a common approach to reduce dimensionality in a dataset (i.e., reduce many variables to few summary components). PCA aims to reduce the dimensionality of a dataset, while preserving much of the statistical information. During the analysis, new variables are found that are linear functions of those in the original dataset, that successively maximize variance and that are uncorrelated with each other, see Jolliffe and Cadima 2016. PCA is recommended when you have large dataset of variables (e.g., &gt; 4) and individuals with measurements of such variables. For this example, we will use the env data frame of the doubs dataset from the R-package ade4. The env is a data frame with 30 rows (sites) and 11 environmental variables. ## we can install ade4 and load the dataset for this example install.packages(&quot;ade4&quot;) library(ade4) ##load the doubs dataset to the R environment data(doubs) env_raw &lt;- doubs$env head(env_raw) # dfs alt slo flo pH har pho nit amm oxy bdo #1 3 934 6.176 84 79 45 1 20 0 122 27 #2 22 932 3.434 100 80 40 2 20 10 103 19 #3 102 914 3.638 180 83 52 5 22 5 105 35 #4 185 854 3.497 253 80 72 10 21 0 110 13 #5 215 849 3.178 264 81 84 38 52 20 80 62 #6 324 846 3.497 286 79 60 20 15 0 102 53 We can start by getting a pairwise bivariate plots of these 11 variables. ## we can install ade4 and load the dataset for this example pairs(env_raw, pch = 19, cex=0.5) We notice a relative variability in these pairwise plots. Some show a linear relationship, others do not. Likewise, some scales differences are evident (some are in the hundreds, others much less). ## we can use the function modify() form the R-package **purrr** to standardize this dataset library(purrr) env_log &lt;- purrr::modify(env_raw, log) head(env_log) # dfs alt slo flo pH har pho nit amm oxy bdo #1 1.098612 6.839476 1.820671 4.430817 4.369448 3.806662 0.0000000 2.995732 -Inf 4.804021 3.295837 #2 3.091042 6.837333 1.233726 4.605170 4.382027 3.688879 0.6931472 2.995732 2.302585 4.634729 2.944439 #3 4.624973 6.817831 1.291434 5.192957 4.418841 3.951244 1.6094379 3.091042 1.609438 4.653960 3.555348 #4 5.220356 6.749931 1.251905 5.533389 4.382027 4.276666 2.3025851 3.044522 -Inf 4.700480 2.564949 #5 5.370638 6.744059 1.156252 5.575949 4.394449 4.430817 3.6375862 3.951244 2.995732 4.382027 4.127134 #6 5.780744 6.740519 1.251905 5.655992 4.369448 4.094345 2.9957323 2.708050 -Inf 4.624973 3.970292 ## Notice some -Inf value, we will remove that column 9 that corresponds to ‘amm’ env_log_red &lt;- env_log[,-9] head(env_log_red) # dfs alt slo flo pH har pho nit oxy bdo #1 1.098612 6.839476 1.820671 4.430817 4.369448 3.806662 0.0000000 2.995732 4.804021 3.295837 #2 3.091042 6.837333 1.233726 4.605170 4.382027 3.688879 0.6931472 2.995732 4.634729 2.944439 #3 4.624973 6.817831 1.291434 5.192957 4.418841 3.951244 1.6094379 3.091042 4.653960 3.555348 #4 5.220356 6.749931 1.251905 5.533389 4.382027 4.276666 2.3025851 3.044522 4.700480 2.564949 #5 5.370638 6.744059 1.156252 5.575949 4.394449 4.430817 3.6375862 3.951244 4.382027 4.127134 #6 5.780744 6.740519 1.251905 5.655992 4.369448 4.094345 2.9957323 2.708050 4.624973 3.970292 ## we can install ade4 and load the modified dataset for this example pairs(env_log_red, pch = 19, cex=0.5) 2) We can run PCA using the preloaded function in R and with several specialized packages. We will start with the functions prcomp() and summary(). ## apply PCA - scale. = TRUE is highly advisable, but default is FALSE. env_prcomp &lt;- prcomp(env_log_red, center = TRUE, scale. = TRUE) env_prcomp #Standard deviations (1, .., p=10): # [1] 2.5031260 1.2651443 0.9875811 0.6860815 0.5095091 0.3946341 0.3212448 0.2824013 0.2580574 0.1512203 # #Rotation (n x k) = (10 x 10): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 #dfs 0.36486258 -0.18617302 0.1482270 -0.27425018 0.070856291 -0.06822936 0.375972370 0.04774989 -0.73277048 0.211581601 #alt -0.36451449 0.12764399 -0.1690422 -0.36872511 0.165563399 0.30103670 0.159545574 0.45954368 -0.16927250 -0.550436218 #slo -0.33426066 0.15461463 0.2062596 -0.41599181 -0.694152941 0.08059586 -0.008314283 -0.38539136 -0.10748520 0.016710642 #flo 0.35536702 -0.26645120 0.2469480 0.07962110 -0.140470255 -0.21095156 0.148040510 -0.22735329 0.11111159 -0.764991160 #pH 0.02686455 -0.43046956 -0.8301590 -0.07333534 -0.270021833 -0.14147519 -0.099820286 -0.07667608 -0.09705901 -0.035234427 #har 0.32775459 -0.15043755 0.1025784 -0.71869778 0.192865111 -0.02065425 -0.450542106 0.06200716 0.30538254 0.071982806 #pho 0.35978479 0.15025127 -0.2384279 -0.09780973 0.004998182 0.59283974 0.493290882 -0.25929815 0.33522306 0.084468872 #nit 0.36509191 -0.03139547 0.1053406 0.23840435 -0.511528294 0.41078684 -0.313263865 0.51006660 -0.09509725 -0.006021563 #oxy -0.25192228 -0.56073462 0.2173434 -0.06473107 -0.174757541 -0.08405657 0.438140938 0.36832297 0.39110186 0.229102408 #bdo 0.24827761 0.55517993 -0.1762335 -0.13016095 -0.252295130 -0.55318468 0.254587830 0.33694829 0.18286003 0.026082994 ## see its structure #str(env_prcomp) #List of 5 # $ sdev : num [1:10] 2.503 1.265 0.988 0.686 0.51 ... #$ rotation: num [1:10, 1:10] 0.3649 -0.3645 -0.3343 0.3554 0.0269 ... # ..- attr(*, &quot;dimnames&quot;)=List of 2 # .. ..$ : chr [1:10] &quot;dfs&quot; &quot;alt&quot; &quot;slo&quot; &quot;flo&quot; ... # .. ..$ : chr [1:10] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ... #$ center : Named num [1:10] 6.887 6.019 0.943 7.153 4.388 ... # ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;dfs&quot; &quot;alt&quot; &quot;slo&quot; &quot;flo&quot; ... #$ scale : Named num [1:10] 1.6786 0.5747 0.3863 1.3041 0.0214 ... # ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;dfs&quot; &quot;alt&quot; &quot;slo&quot; &quot;flo&quot; ... #$ x : num [1:30, 1:10] -6 -4.96 -3.6 -3.18 -1.24 ... # ..- attr(*, &quot;dimnames&quot;)=List of 2 # .. ..$ : chr [1:30] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... # .. ..$ : chr [1:10] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ... # - attr(*, &quot;class&quot;)= chr &quot;prcomp&quot; ## we can get a summary summary(env_prcomp) #Importance of components: # PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 #Standard deviation 2.5031 1.2651 0.98758 0.68608 0.50951 0.39463 0.32124 0.28240 0.25806 0.15122 #Proportion of Variance 0.6266 0.1601 0.09753 0.04707 0.02596 0.01557 0.01032 0.00798 0.00666 0.00229 #Cumulative Proportion 0.6266 0.7866 0.88415 0.93123 0.95719 0.97276 0.98308 0.99105 0.99771 1.00000 The output of env_prcomp includes the standard deviation of each of the 10 principal components (PCs). You should expect as many PCs as variables in your dataset. The values under Rotation (n x k) are the rotation (or loadings) of each of variables in the original dataset that have been maximize its relevance on each components. The higher the absolute value of the loading, the more that variable loads (i.e., is better represented) on each specific PC. Thesummary(env_prcomp) output describe the importance of each PC. The proportion of variance indicates the proportion of the total variance in the data explained by each PC. We can notice the values of the second row are progressively added resulting in a cumulative proportion of explained variance. In most cases, the first PC accounts for more most of the variance explain (i.e., 62.66%). If we consider the two firsts PCs, then the total variance explained by these 2 PCs is 78.66%. The rest of PCs are progressively less and less relevant to explain the variance of the data. 3) For most purposes, the first two PCs are used for plots and visualizing dimensionality reduction. However, we can determine how many PCs to retain using a scree plot and other references using the R-package factoextra. We can get an scree plot with the function fviz_eig(). ## install package, if not present. It also requires the R-package ggplot2. install.packages(&quot;factoextra&quot;) install.packages(&quot;ggplot2&quot;) library(factoextra) #Loading required package: ggplot2 #Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa factoextra::fviz_eig(env_prcomp) This scree plot shows the eigenvalues that provide information on the percentage of variances explained by each PC and support our intuition that the first two PCs are the most useful to summarize the variability of our dataset. For instance, we see a marked drop in the percentage of explained after PC2, and we can ignore those PCs (3-10) that explain relatively little variation. Thus, we should focus primarily on PC1 and PC2. 4) We can graph our samples (i.e., individuals on our dataset) with their PC1 and PC2 coordinates using the function fviz_pca_ind(). factoextra::fviz_pca_ind(env_prcomp, col.ind = &quot;cos2&quot;, gradient.cols = c(&quot;firebrick4&quot;, &quot;gray&quot;, &quot;dodgerblue4&quot;), repel = TRUE) Individuals with a similar profile tend to group together and color cos2 refers to the quality of representation. Notice that you can choose the color gradient that you want with the argument gradient.cols, it might need 3 colors. ### NOTE: Sometimes you get this error. #Error in UseMethod(&quot;depth&quot;) : # no applicable method for &#39;depth&#39; applied to an object of class &quot;NULL&quot; ### Try to resubmit the fviz_pca_ind() after closing the graph window. 5) We can also graph of variables that we used in our PCA using the function fviz_pca_var(). factoextra::fviz_pca_var(env_prcomp, col.var = &quot;contrib&quot;, gradient.cols = c(&quot;dodgerblue4&quot;, &quot;#E7B800&quot;, &quot;firebrick4&quot;), repel = TRUE) We can see that positive correlated variables point to the same side of the plot, while negative correlated variables point to opposite sides of the graph. Notice that you can choose the color gradient that you want with the argument gradient.cols, it might need 3 colors. ### NOTE: Sometimes you get this error. #Error in UseMethod(&quot;depth&quot;) : # no applicable method for &#39;depth&#39; applied to an object of class &quot;NULL&quot; ### Try to resubmit the fviz_pca_ind() after closing the graph window. 6) We can also graph plot both individuals and variables at the same time using the function fviz_pca_biplot(). factoextra::fviz_pca_biplot(env_prcomp, col.var = &quot;gray&quot;, # Variables color col.ind = &quot;dodgerblue4&quot;, # Individuals color repel = TRUE) 7) We can extract other metrics of our PCA with several functions of factoextra. The eigenvalues can be printed with the function get_eigenvalue()describe the importance of each PC. Similar to summary(env_prcomp). ## Eigenvalues eig_val_env_prcomp &lt;- factoextra::get_eigenvalue(env_prcomp) eig_val_env_prcomp # eigenvalue variance.percent cumulative.variance.percent #Dim.1 6.26563999 62.6563999 62.65640 #Dim.2 1.60059015 16.0059015 78.66230 #Dim.3 0.97531642 9.7531642 88.41547 #Dim.4 0.47070789 4.7070789 93.12254 #Dim.5 0.25959955 2.5959955 95.71854 #Dim.6 0.15573609 1.5573609 97.27590 #Dim.7 0.10319822 1.0319822 98.30788 #Dim.8 0.07975052 0.7975052 99.10539 #Dim.9 0.06659361 0.6659361 99.77132 #Dim.10 0.02286757 0.2286757 100.00000 The variable results can be printed with the function get_pca_var(). ## Results for Variables res_var_env_prcomp &lt;- factoextra::get_pca_var(env_prcomp) # Coordinates res_var_env_prcomp$coord # Contributions to the PCs res_var_env_prcomp$contrib # Quality of representation res_var_env_prcomp$cos2 The individual (i.e., samples) associated results can be printed with the function get_pca_ind(). ## Results for individuals res_ind_env_prcomp &lt;- factoextra::get_pca_ind(env_prcomp) # Coordinates -- this is same as env_prcomp$x res_ind_env_prcomp$coord # Contributions to the PCs res_ind_env_prcomp$contrib # Quality of representation res_ind_env_prcomp$cos2 9.2 Multidimensional scaling 8) We use multidimensional scaling helps to visualize similarities of individual cases or samples of a dataset. In this analysis, we reduce dimensions to be able to visualize or represent data points in a more friendly manner. We start with a set of samples or individuals measured on several numeric variables. After the analysis, we get a representation of the distances among these samples or individuals in a lower dimensional space (e.g., 2 dimensions), so we can infer the relative relationship (distance) between them. In other words, individuals or samples that are closer are more similar. We will use our example `env_log_red, but we need to add names to the sites for visualization. ## We can name our samples by assigned a unique row name. env_log_red_MDS &lt;- env_log_red rownames(env_log_red_MDS) &lt;- paste0(&quot;site_&quot;, 1:nrow(env_log_red_MDS)) head(env_log_red_MDS) # dfs alt slo flo pH har pho nit oxy bdo #site_1 1.098612 6.839476 1.820671 4.430817 4.369448 3.806662 0.0000000 2.995732 4.804021 3.295837 #site_2 3.091042 6.837333 1.233726 4.605170 4.382027 3.688879 0.6931472 2.995732 4.634729 2.944439 #site_3 4.624973 6.817831 1.291434 5.192957 4.418841 3.951244 1.6094379 3.091042 4.653960 3.555348 #site_4 5.220356 6.749931 1.251905 5.533389 4.382027 4.276666 2.3025851 3.044522 4.700480 2.564949 #site_5 5.370638 6.744059 1.156252 5.575949 4.394449 4.430817 3.6375862 3.951244 4.382027 4.127134 #site_6 5.780744 6.740519 1.251905 5.655992 4.369448 4.094345 2.9957323 2.708050 4.624973 3.970292 We can perform MDS using the function dis() and cmdscale()for metric or classical MDS with two dimensions k = 2. ## Classical MDS: N rows (objects) x p columns (variables) # get Euclidean distances between the rows env_log_red_MDS_d &lt;- dist(env_log_red_MDS) # k is the number of dimensions env_log_red_MDS_fit &lt;- cmdscale(env_log_red_MDS_d,eig=TRUE, k=2) str(env_log_red_MDS_fit) #List of 5 # $ points: num [1:30, 1:2] -7.21 -5.58 -3.84 -3.11 -1.83 ... # ..- attr(*, &quot;dimnames&quot;)=List of 2 # .. ..$ : chr [1:30] &quot;site_1&quot; &quot;site_2&quot; &quot;site_3&quot; &quot;site_4&quot; ... # .. ..$ : NULL #$ eig : num [1:30] 203.13 25.14 9.43 4.54 3.22 ... #$ x : NULL #$ ac : num 0 #$ GOF : num [1:2] 0.92 0.92 We can plot this samples using the x-y coordinates in env_log_red_MDS_fit$points. ## plot solution x &lt;- env_log_red_MDS_fit$points[,1] y &lt;- env_log_red_MDS_fit$points[,2] plot(x, y, xlab= &quot;Coordinate 1 - MDS&quot;, ylab=&quot;Coordinate 2 - MDS&quot;, main=&quot;Metric MDS for env_log_red&quot;, type=&quot;n&quot;) text(x, y, labels = row.names(env_log_red_MDS), cex=.7) 9) We use nonmetric multidimensional scaling to visualizing the function isoMDS() of the R-package MASS. ## For nonmetric MDS, we load MASS (we installed this package before) library(MASS) env_log_red_MDS_d &lt;- dist(env_log_red_MDS) env_log_red_NMDS &lt;- MASS::isoMDS(env_log_red_MDS_d, k=2) str(env_log_red_NMDS) #List of 2 # $ points: num [1:30, 1:2] -7.64 -5.6 -3.74 -3.08 -1.72 ... # ..- attr(*, &quot;dimnames&quot;)=List of 2 # .. ..$ : chr [1:30] &quot;site_1&quot; &quot;site_2&quot; &quot;site_3&quot; &quot;site_4&quot; ... # .. ..$ : NULL # $ stress: num 2.49 ## plot solution NMDS solution x &lt;- env_log_red_NMDS$points[,1] y &lt;- env_log_red_NMDS$points[,2] plot(x, y, xlab= &quot;Coordinate 1 - NMDS&quot;, ylab=&quot;Coordinate 2 - NMDS&quot;, main=&quot;Nonmetric MDS for env_log_red&quot;, type=&quot;n&quot;) text(x, y, labels = row.names(env_log_red_MDS), cex=.7) 9.3 Cluster analysis: DFA Many biological datasets might have an underlying structure were groups or clusters that can be outcome of a treatment. Finding those set of objects that are more similar might help to identify groups or clusters that can used to classification of other samples. 10) We use a classic example known as the Fisher’s or Anderson’s Iris data set, which is a dataset of multiple measurements introduced by Ronald Fisher to illustrate taxonomic problems as an example of linear discriminant analysis. This dataset consists of 50 samples of three species of flowers: Iris setosa, Iris virginica and Iris versicolor. The dataset includes four features of the flower measured in centimeters. This dataset can be loaded from the R-package [datasets]. install.packages(&quot;datasets&quot;) library(datasets) data(iris) iris_dataset &lt;- iris summary(iris_dataset) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species #Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 #1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 #Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 #Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 #3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 #Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 head(iris_dataset) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species #1 5.1 3.5 1.4 0.2 setosa #2 4.9 3.0 1.4 0.2 setosa #3 4.7 3.2 1.3 0.2 setosa #4 4.6 3.1 1.5 0.2 setosa #5 5.0 3.6 1.4 0.2 setosa #6 5.4 3.9 1.7 0.4 setosa 11) We can perform discriminant function analysis (DFA) to find a function (e.g., linear classifier) to separate samples in to two or more classes. For this analysis, we will the function lda() from the package [MASS]. ## we have used the packages MASS and ggplot2. library(MASS) library(ggplot2) ## we get the names of predictors (i.e., measurements) and target group str(iris_dataset) #&#39;data.frame&#39;: 150 obs. of 5 variables: #$ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... #$ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... #$ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... #$ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... #$ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... We see that the grouping variable is Species with the three species and four continuous measurements associated with each individual in the dataset. In this case, we input a formula Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width. ## we fit the lda function iris_lda_model &lt;- lda(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris_dataset) iris_lda_model #Call: #lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, # data = iris_dataset) # #Prior probabilities of groups: # setosa versicolor virginica # 0.3333333 0.3333333 0.3333333 # #Group means: # Sepal.Length Sepal.Width Petal.Length Petal.Width #setosa 5.006 3.428 1.462 0.246 #versicolor 5.936 2.770 4.260 1.326 #virginica 6.588 2.974 5.552 2.026 # #Coefficients of linear discriminants: # LD1 LD2 #Sepal.Length 0.8293776 0.02410215 #Sepal.Width 1.5344731 2.16452123 #Petal.Length -2.2012117 -0.93192121 #Petal.Width -2.8104603 2.83918785 # #Proportion of trace: # LD1 LD2 #0.9912 0.0088 11) We can now evaluate how good LDA can discriminate our dataset. We need to install two R-packages caret and e1071. ## we install these packages install.packages(&quot;caret&quot;) install.packages(&quot;e1071&quot;) library(caret) library(e1071) We need to fit predictive model over our data using the function train() and obtain the confusion matrix confusionMatrix() that is table that determines how good was the classification using LDA. iris_lda_predict &lt;- caret::train(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, method = &quot;lda&quot;, data = iris_dataset) iris_lda_predict #Linear Discriminant Analysis # #150 samples # 4 predictor # 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; # #No pre-processing #Resampling: Bootstrapped (25 reps) #Summary of sample sizes: 150, 150, 150, 150, 150, 150, ... #Resampling results: # # Accuracy Kappa # 0.9697408 0.9542207 iris_lda_confusion_matrix &lt;- confusionMatrix(iris_dataset$Species, predict(iris_lda_predict, iris_dataset)) iris_lda_confusion_matrix #Confusion Matrix and Statistics # # Reference #Prediction setosa versicolor virginica # setosa 50 0 0 # versicolor 0 48 2 # virginica 0 1 49 # #Overall Statistics # # Accuracy : 0.98 # 95% CI : (0.9427, 0.9959) # No Information Rate : 0.34 # P-Value [Acc &gt; NIR] : &lt; 2.2e-16 # # Kappa : 0.97 # # Mcnemar&#39;s Test P-Value : NA # #Statistics by Class: # # Class: setosa Class: versicolor Class: virginica #Sensitivity 1.0000 0.9796 0.9608 #Specificity 1.0000 0.9802 0.9899 #Pos Pred Value 1.0000 0.9600 0.9800 #Neg Pred Value 1.0000 0.9900 0.9800 #Prevalence 0.3333 0.3267 0.3400 #Detection Rate 0.3333 0.3200 0.3267 #Detection Prevalence 0.3333 0.3333 0.3333 #Balanced Accuracy 1.0000 0.9799 0.9753 We can see the confusion matrix and values that not consider between Reference and Prediction are discrepancies between what we assigned and what LDA determined as a part of a group. We can add such predictions to the iris_dataset using the function predict(). iris_lda_predicton &lt;- predict (iris_lda_predict, iris_dataset) iris_dataset$lda_species_prediction &lt;- iris_lda_predicton iris_dataset # Sepal.Length Sepal.Width Petal.Length Petal.Width Species lda_species_prediction #1 5.1 3.5 1.4 0.2 setosa setosa #2 4.9 3.0 1.4 0.2 setosa setosa #3 4.7 3.2 1.3 0.2 setosa setosa #... #69 6.2 2.2 4.5 1.5 versicolor versicolor #70 5.6 2.5 3.9 1.1 versicolor versicolor #71 5.9 3.2 4.8 1.8 versicolor virginica #72 6.1 2.8 4.0 1.3 versicolor versicolor #... #83 5.8 2.7 3.9 1.2 versicolor versicolor #84 6.0 2.7 5.1 1.6 versicolor virginica #85 5.4 3.0 4.5 1.5 versicolor versicolor #... #132 7.9 3.8 6.4 2.0 virginica virginica #133 6.4 2.8 5.6 2.2 virginica virginica #134 6.3 2.8 5.1 1.5 virginica versicolor #... You can see above that discrepancies in the classification assigned using LDA, these discrepancies should agree with the confusion table. ## we can add the LD coordinates iris_dataset &lt;- cbind(iris_dataset, predict(iris_lda_model)$x) head(iris_dataset) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species lda_species_prediction LD1 LD2 #1 5.1 3.5 1.4 0.2 setosa setosa 8.061800 0.3004206 #2 4.9 3.0 1.4 0.2 setosa setosa 7.128688 -0.7866604 #3 4.7 3.2 1.3 0.2 setosa setosa 7.489828 -0.2653845 #4 4.6 3.1 1.5 0.2 setosa setosa 6.813201 -0.6706311 #5 5.0 3.6 1.4 0.2 setosa setosa 8.132309 0.5144625 #6 5.4 3.9 1.7 0.4 setosa setosa 7.701947 1.4617210 ## we subset the iris_dataset to the original classification iris_dataset_original &lt;- subset(iris_dataset, select = c(Species, LD1,LD2)) head(iris_dataset_original) # Species LD1 LD2 #1 setosa 8.061800 0.3004206 #2 setosa 7.128688 -0.7866604 #3 setosa 7.489828 -0.2653845 #4 setosa 6.813201 -0.6706311 #5 setosa 8.132309 0.5144625 #6 setosa 7.701947 1.4617210 ## show original groups ggplot(iris_dataset, aes(LD1, LD2)) + geom_point(aes(color = Species)) + ggtitle(&quot;Iris Original Groups&quot;) ## show LDA predicted groups ggplot(iris_dataset, aes(LD1, LD2)) + geom_point(aes(color = lda_species_prediction)) + ggtitle(&quot;Iris LDA Predicted Groups&quot;) NOTE sometimes the ggplot() might need to run twice without closing the plot window to produce a correct plot Can you find the changes on the classification between original and LDA predicted? 9.4 Cluster analysis: k-means (unsupervised) 12) We can use our own data without defining groups and ask if an algorithm used to find homogeneous subgroups in our data. One of such approaches is k-means clustering and we can use the preloaded function kmeans(). We will use the iris_dataset and assume that there are 3 groups (as we know about such dataset and its three putative species). ## Let&#39;s check again this dataset iris_dataset &lt;- iris head(iris_dataset) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species #1 5.1 3.5 1.4 0.2 setosa #2 4.9 3.0 1.4 0.2 setosa #3 4.7 3.2 1.3 0.2 setosa #4 4.6 3.1 1.5 0.2 setosa #5 5.0 3.6 1.4 0.2 setosa #6 5.4 3.9 1.7 0.4 setosa ## We need to remove the column with species assignments iris_dataset_num &lt;- iris_dataset[,-which(names(iris_dataset) %in% c(&quot;Species&quot;))] head(iris_dataset_num) # Sepal.Length Sepal.Width Petal.Length Petal.Width #1 5.1 3.5 1.4 0.2 #2 4.9 3.0 1.4 0.2 #3 4.7 3.2 1.3 0.2 #4 4.6 3.1 1.5 0.2 #5 5.0 3.6 1.4 0.2 #6 5.4 3.9 1.7 0.4 Notice how I removed the “Species” column, this is one of many forms to remove columns by name. Now we determine the 3 groups with centers = 3 by repeating 30 times using the argument nstart = 30 with the function kmeans(). iris_dataset_kmeans &lt;- kmeans(iris_dataset_num, centers = 3, nstart = 30) #K-means clustering with 3 clusters of sizes 50, 62, 38 # #Cluster means: # Sepal.Length Sepal.Width Petal.Length Petal.Width #1 5.006000 3.428000 1.462000 0.246000 #2 5.901613 2.748387 4.393548 1.433871 #3 6.850000 3.073684 5.742105 2.071053 # #Clustering vector: # [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # [70] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 #[139] 2 3 3 3 2 3 3 3 2 3 3 2 # #Within cluster sum of squares by cluster: #[1] 15.15100 39.82097 23.87947 # (between_SS / total_SS = 88.4 %) # #Available components: # #[1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; We can add this kmean groups and compare those with the original species names. ## add the cluster vector to iris_dataset iris_dataset_kmeans$cluster # [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # [70] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 #[139] 2 3 3 3 2 3 3 3 2 3 3 2 iris_dataset_original$kmeans_cluster &lt;- factor(iris_dataset_kmeans$cluster) head(iris_dataset_original) # Species LD1 LD2 kmeans_cluster #1 setosa 8.061800 0.3004206 1 #2 setosa 7.128688 -0.7866604 1 #3 setosa 7.489828 -0.2653845 1 #4 setosa 6.813201 -0.6706311 1 #5 setosa 8.132309 0.5144625 1 #6 setosa 7.701947 1.4617210 1 ## we can plot these groups using the lda coordinates to visualize. ggplot(iris_dataset_original, aes(LD1, LD2)) + geom_point(aes(color = kmeans_cluster)) + ggtitle(&quot;Iris kmeans with 3 Groups&quot;) "],["graphics-and-colors.html", "Session 10 – Graphics and Colors 10.1 Avoid bad graphics 10.2 Graphics in R – ggplot2 10.3 Preparing your dataset 10.4 Color palettes online 10.5 ggplot2 default palette 10.6 Heatmaps or tiled charts 10.7 R-packages about color 10.8 Making your own palette", " Session 10 – Graphics and Colors Plots, graphs and other visual representations of data and central to bioinformatics. Such illustrations summarize year/months of data collection and complex statistical analyzes. The main goal of such visual representations is to improve the observer, reader or researcher his/her understanding of the data and his/her abilities to compare with other datasets and reveal trends in the data collected, compiled or analyzed. However, you might have seen good, bad and ugly graphs. Yet, an effective graph is easier to decode or understand than others, by virtue of this, it is likely to be beautiful. 10.1 Avoid bad graphics I think it is a good idea to review about effective graphics. Here is a list of suggestions, mostly summarized, from the graphics section by Jenny Bryan STAT 545 class at UBC. A) No 3D or overdone plots: Our current format to visualize data is paper or screen (a 2D surfaces) by adding another dimension makes information difficult to visualize. Likewise, eliminating clutter and de-emphasizing supporting elements make reduce the attention exhaustion of the reader (i.e., every aspect of a figure should add if needed). Suggestions: Try to keep graphics simple and make most of them as 2D, no shadows or excess ornamentation. Here is a visual animation that transforms an overdone bar chart into one that is just right with the information that it wants to communicate. Source https://www.darkhorseanalytics.com. B) No Pie Charts: This type of infographic is probably the least effective when encoding quantitative data (e.g., percentages or fractions) because pie charts use angles and areas, which are very hard for visualize or compare. Suggestions: Use bar charts and dotplot for proportions and percentages. These graphs make comparisons easier by positioning objects along a common scale. Source https://www.darkhorseanalytics.com. C) Careful about scale of things: Some data need to be scaled or transformed (e.g., logarithms) before plotting to allow the reader to visualize the data distribution. For example, some variables show a multiplicative or exponential scales that jumps from tens to hundreds or thousands. ** Suggestions:** Explore you data first and determine if it needs transformation. ## load mammal dataset from &#39;MASS&#39; and transform them with modify() from &#39;purrr&#39; library(MASS) library(purrr) mammals_raw &lt;- mammals mammals_log &lt;- modify(mammals_raw,log10) ## plot them side to side into a 1*2 array par(mfrow=c(1,2)) plot(x = mammals_raw$body, y = mammals_raw$brain) mtext(side=3, line=0.5, adj=0, &quot;raw data from mammal -- body vs brain&quot;) abline(lm(brain ~ body, data = mammals_raw), col = &quot;blue&quot;) plot(x = mammals_log$body, y = mammals_log$brain) mtext(side=3, line=0.5, adj=0, &quot;Log10 data from mammal -- body vs brain&quot;) abline(lm(brain ~ body, data = mammals_log), col = &quot;blue&quot;) D) Avoid Microsoft Excel default plots: Most people use Excel at some point to store their data, then comes the temptation to use their reduced offer of plots and charts. In general, these tend to be plain and less customizable than those derived from R graphics. ** Suggestions:** You already are taking this workshop. E) Careful with the figure legend: Most graphics need a legend when they are in paper, reports or other divulgation source as these images should stand alone from the main text of the article. Then, the accompanying legend should not contradict what the graphic is showing (i.e., self-contradiction). This is a common mistake and can be used against your paper and thesis by reviewers that will consider that your work is careless and sloppy. ** Suggestions:** Always proofread your manuscripts. F) Make simple tables: We received a lot of attention for our Data Looks Better Naked post. People got bored on Christmas Eve and some interesting searches for Star Trek somehow landed them on our page. Now their charts look better. The principles outlined in that article aren’t just for charts, though. You can apply them to your data tables with similar improvements in readability and aesthetics. To paraphrase Edward Tufte, too often when we create a data table, we imprison our data behind a wall of grid lines. Instead we can let the data itself form the structure that aids readability by making better use of alignment and whitespace. In the gif below we start with a table formatted similar to one of Excel’s many styling options which, much like the chart styles, do nothing to improve the table. Progressive deletions and some reorganization deliver a clearer and more compelling picture. As with charts, rather than dressing up our data we should be stripping it down. For more information on table design, you can read Chapter 8 of Stephen Few’s Show Me the Numbers. My apologies to any true fans of 80’s wrestling, the stats below, much like the ring rivalries, are entirely fabricated. F) Other Plots to avoid:: Radar charts that make comparisons of data points very difficult as they are placed in a circle, the middle portion stacks too many contrasts and there is a disproportionate emphasis of high numbers (i.e., middle or low values are less visible). Use stack bars or bar charts. 10.2 Graphics in R – ggplot2 The preloaded graphics package of R helps the easy access plots, bar charts, boxplots and other. However, with the development of the ggplot2 package that is part of the tidyverse collection. This package and its growing collection of add-ons, which include at least 81, has made ggplot2 a dominant, flexible, and beautiful R graphics package. Therefore, we will explore graphics in R using this package. Like most visualizations, it would be ideal to start with installing ggplot2 and datasets that will serve as examples. 1) If you have not installed ggplot2, you should follow these instructions before we explore graphics and plots. install.packages(&quot;ggplot2&quot;) library(ggplot2) 10.3 Preparing your dataset 2) We also need some reference datasets to use and plot for our graphics. We will use cars and the airway_scaledcounts.csv datasets. ## we will load &#39;ade4&#39; package to get the bacteria dataset library(ade4) data(bacteria, package = &quot;ade4&quot;) ## we get the espaa dataframe with number of the 21 amino acids present in 43 species of bacteria bacteria_espaa &lt;- bacteria$espaa We then scale this dataset using the function modify() from the package purrr. # load purrr package and scale the values require(purrr) bacteria_espaa_scaled &lt;- purrr::modify(bacteria_espaa, scale) head(bacteria_espaa_scaled) # Ala Arg Asn Asp Cys Gln Glu Gly His Ile Leu #AERPECG 0.3285668 0.7317790 -0.57189998 -0.1194444 0.1648422 -0.3942439 0.2871196 0.4766228 0.1461376 -0.04383607 0.39172834 #AQUAECG -0.4008694 -0.2034641 -0.22356528 -0.3025407 -0.3950725 -0.5262848 0.4634441 -0.1820602 -0.3947798 0.06415029 -0.09928685 #ARCFUCG 0.1073331 0.2833748 0.04401802 0.2066524 0.5498823 -0.4297732 0.8885965 0.2463444 -0.1330163 0.57892747 0.12987680 #BACHDCG 0.9012986 0.9875656 1.72105862 1.5171379 0.8224654 1.6598743 2.2634710 1.2984007 1.9400648 2.16751418 1.40433248 #BACSUCG 1.0314722 0.7271505 2.09337260 1.6457327 1.0718724 1.5394241 2.0612524 1.3252195 1.8173099 2.49733951 1.38291228 #BORBUCG -0.7597170 -0.7465478 0.01654507 -0.6016028 -0.9081081 -0.7293261 -0.6491864 -0.7234675 -0.8524967 -0.18035597 -0.60867882 # Lys Met Phe Pro Ser Stp Thr Trp Tyr Val #AERPECG -0.36413421 0.09017282 -0.3275198 0.89114307 0.7319805 0.7113529 0.152094962 0.38406553 0.4369524 0.61086466 #AQUAECG 0.88477099 -0.28951403 0.1851469 -0.17796712 -0.4145196 -0.2123879 -0.323609531 -0.26598780 0.3076040 0.01812197 #ARCFUCG 0.79679827 0.48462587 0.5156315 0.08558657 0.1583118 0.2768920 0.008319353 0.09800132 0.6381195 0.61451548 #BACHDCG 2.08829038 2.07351272 2.0439218 1.06116153 1.5330792 1.4796369 1.887894271 1.20439921 2.0999898 1.67959246 #BACSUCG 2.95243158 2.18339064 2.1725251 1.03849240 1.9769142 1.5369105 1.887693467 1.04854940 2.2918176 1.46336710 #BORBUCG -0.02590849 -0.72066544 -0.3193467 -0.80668435 -0.5156512 -0.7990328 -0.785609156 -0.79075009 -0.4463524 -0.78199455 We can also create a more easy to map plot with using the function melt() of the package reshape2. ## load reshape library(reshape2) # use melt() to transform bacteria_espaa_for_heatmap &lt;- melt(as.matrix(bacteria_espaa_scaled)) # transform factor variables to character factor_columns &lt;- sapply(bacteria_espaa_for_heatmap, is.factor) bacteria_espaa_for_heatmap[factor_columns] &lt;- lapply(bacteria_espaa_for_heatmap[factor_columns], as.character) ## we can rename the columns in this data frame names(bacteria_espaa_for_heatmap) &lt;- c(&quot;bacteria_sp&quot;, &quot;Amino_acid&quot;, &quot;scaled_value&quot;) head(bacteria_espaa_for_heatmap) # bacteria_sp Amino_acid scaled_value #1 AERPECG Ala 0.3285668 #2 AQUAECG Ala -0.4008694 #3 ARCFUCG Ala 0.1073331 #4 BACHDCG Ala 0.9012986 #5 BACSUCG Ala 1.0314722 #6 BORBUCG Ala -0.7597170 3) A more complex dataset for color schemes, we will use the airway_scaledcounts.csv that has expression analysis data for several RNAseq experiments. This dataset is in our class GitHub repository. ## NOTE: remember to update the path to file with the dataset where you downloaded in your computer -- THIS IS EXCLUSIVE TO YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW ## load get &#39;airway_scaledcounts.csv&#39; dataset airway_data &lt;- read.table(&quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/airway_scaledcounts.csv&quot;, header = TRUE, sep = &quot;,&quot;, stringsAsFactors = FALSE) head(airway_data) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 ENSG00000000003 723 486 904 445 1170 1097 806 604 #2 ENSG00000000005 0 0 0 0 0 0 0 0 #3 ENSG00000000419 467 523 616 371 582 781 417 509 #4 ENSG00000000457 347 258 364 237 318 447 330 324 #5 ENSG00000000460 96 81 73 66 118 94 102 74 #6 ENSG00000000938 0 0 1 0 2 0 0 0 str(airway_data) #&#39;data.frame&#39;: 38694 obs. of 9 variables: # $ ensgene : chr &quot;ENSG00000000003&quot; &quot;ENSG00000000005&quot; &quot;ENSG00000000419&quot; &quot;ENSG00000000457&quot; ... # $ SRR1039508: num 723 0 467 347 96 ... # $ SRR1039509: num 486 0 523 258 81 ... # $ SRR1039512: num 904 0 616 364 73 1 6000 2640 692 531 ... # $ SRR1039513: num 445 0 371 237 66 ... # $ SRR1039516: num 1170 0 582 318 118 ... # $ SRR1039517: num 1097 0 781 447 94 ... # $ SRR1039520: num 806 0 417 330 102 ... # $ SRR1039521: num 604 0 509 324 74 ... We then scale this dataset using the function modify() from the package purrr. We needed to remove and add again the names of the genes in column ensgene. ensgene &lt;- airway_data[,1] airway_data_numeric &lt;- airway_data[,-1] # load purrr package require(purrr) airway_data_scaled &lt;- purrr::modify(airway_data_numeric, scale) # cbind the name of the genes from vector &#39;ensgene&#39; airway_data_scaled &lt;- cbind(ensgene, airway_data_scaled, stringsAsFactors = FALSE) head(airway_data_scaled) # ensgene SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #1 ENSG00000000003 0.03933056 -0.003924270 0.036555187 0.011021618 0.09101424 0.041361372 0.06313763 0.007896859 #2 ENSG00000000005 -0.12176875 -0.130284028 -0.106887175 -0.112547703 -0.11339060 -0.123910392 -0.10822243 -0.116658738 #3 ENSG00000000419 -0.01771152 0.005695712 -0.009143265 -0.009526988 -0.01171230 -0.006246538 -0.01956592 -0.011693839 #4 ENSG00000000457 -0.04444999 -0.063204156 -0.049129410 -0.046736626 -0.05783441 -0.056566291 -0.03806261 -0.049844146 #5 ENSG00000000460 -0.10037797 -0.109224069 -0.095303887 -0.094220568 -0.09277541 -0.109748545 -0.08653667 -0.101398615 #6 ENSG00000000938 -0.12176875 -0.130284028 -0.106728500 -0.112547703 -0.11304119 -0.123910392 -0.10822243 -0.116658738 We can also create a more easy to map plot with using the function melt() of the package reshape2. ## load reshape library(reshape2) # prepare other data frame airway_data_scaled2 &lt;- purrr::modify(airway_data_numeric, scale) # add ensgene as rownames rownames(airway_data_scaled2) &lt;- ensgene head(airway_data_scaled2) # SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #ENSG00000000003 0.03933056 -0.003924270 0.036555187 0.011021618 0.09101424 0.041361372 0.06313763 0.007896859 #ENSG00000000005 -0.12176875 -0.130284028 -0.106887175 -0.112547703 -0.11339060 -0.123910392 -0.10822243 -0.116658738 #ENSG00000000419 -0.01771152 0.005695712 -0.009143265 -0.009526988 -0.01171230 -0.006246538 -0.01956592 -0.011693839 #ENSG00000000457 -0.04444999 -0.063204156 -0.049129410 -0.046736626 -0.05783441 -0.056566291 -0.03806261 -0.049844146 #ENSG00000000460 -0.10037797 -0.109224069 -0.095303887 -0.094220568 -0.09277541 -0.109748545 -0.08653667 -0.101398615 #ENSG00000000938 -0.12176875 -0.130284028 -0.106728500 -0.112547703 -0.11304119 -0.123910392 -0.10822243 -0.116658738 ## get matrix dimensions dim(airway_data_scaled2) #[1] 38694 8 As indicated by the output of the function dim() this is a very large matrix (38,694 rows). This is a huge table; however, most genes will have similar levels of expression across SRR samples. We will search for the ensgenes that have the most dispersion around the mean expression by estimating the coefficient of variation CV. This statistic represents the ratio of the standard deviation to the mean. Then, we will extract those ensgenes with the 100 highest values. ## Create a function to valculate coefficient of variation co.var &lt;- function(x) ( 100*sd(x)/mean(x) ) ## we estimate row-wise coefficient of variation airway_data_scaled2$coVar&lt;-apply(airway_data_scaled2[,names(airway_data_scaled2)],1,co.var) head(airway_data_scaled2) # SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 coVar #ENSG00000000003 0.03933056 -0.003924270 0.036555187 0.011021618 0.09101424 0.041361372 0.06313763 0.007896859 87.129966 #ENSG00000000005 -0.12176875 -0.130284028 -0.106887175 -0.112547703 -0.11339060 -0.123910392 -0.10822243 -0.116658738 -6.935013 #ENSG00000000419 -0.01771152 0.005695712 -0.009143265 -0.009526988 -0.01171230 -0.006246538 -0.01956592 -0.011693839 -77.383502 #ENSG00000000457 -0.04444999 -0.063204156 -0.049129410 -0.046736626 -0.05783441 -0.056566291 -0.03806261 -0.049844146 -15.968492 #ENSG00000000460 -0.10037797 -0.109224069 -0.095303887 -0.094220568 -0.09277541 -0.109748545 -0.08653667 -0.101398615 -8.191639 #ENSG00000000938 -0.12176875 -0.130284028 -0.106728500 -0.112547703 -0.11304119 -0.123910392 -0.10822243 -0.116658738 -6.980658 ## descresing order by coVar airway_data_scaled2_order &lt;- airway_data_scaled2[order(airway_data_scaled2$coVar, decreasing = TRUE),] head(airway_data_scaled2_order) # SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 coVar #ENSG00000126709 0.0313090216 -0.0005442761 -0.006445787 0.049897360 -0.012760526 -0.030050493 -0.019778529 -0.011075185 39115.82 #ENSG00000278270 0.1077364958 0.1148955034 -0.015648947 -0.035906955 -0.042285668 -0.052950501 -0.036574367 -0.037471074 30921.30 #ENSG00000041802 -0.0043422784 0.0150556941 -0.007239162 -0.015080665 0.012746403 0.017406759 -0.008723041 -0.009425442 25810.60 #ENSG00000086712 -0.0116953590 0.0098557040 -0.021678604 -0.008138569 0.004535268 -0.034570232 0.038475388 0.023981854 25111.84 #ENSG00000188603 -0.0001086865 0.0116757006 -0.020567877 0.002413418 -0.009441131 -0.004739959 -0.004258325 0.025631597 18384.05 #ENSG00000125398 0.0698569896 -0.0096442587 0.058928387 -0.045348207 -0.017652265 -0.093326829 0.090138534 -0.049637928 15830.54 ## retain the first 100 genes airway_data_scaled_red &lt;- airway_data_scaled2_order[1:100,] ## remove sd column airway_data_scaled_red &lt;- airway_data_scaled_red[,-length(airway_data_scaled_red)] head(airway_data_scaled_red) # SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 #ENSG00000126709 0.0313090216 -0.0005442761 -0.006445787 0.049897360 -0.012760526 -0.030050493 -0.019778529 -0.011075185 #ENSG00000278270 0.1077364958 0.1148955034 -0.015648947 -0.035906955 -0.042285668 -0.052950501 -0.036574367 -0.037471074 #ENSG00000041802 -0.0043422784 0.0150556941 -0.007239162 -0.015080665 0.012746403 0.017406759 -0.008723041 -0.009425442 #ENSG00000086712 -0.0116953590 0.0098557040 -0.021678604 -0.008138569 0.004535268 -0.034570232 0.038475388 0.023981854 #ENSG00000188603 -0.0001086865 0.0116757006 -0.020567877 0.002413418 -0.009441131 -0.004739959 -0.004258325 0.025631597 #ENSG00000125398 0.0698569896 -0.0096442587 0.058928387 -0.045348207 -0.017652265 -0.093326829 0.090138534 -0.049637928 ## get dimensions dim(airway_data_scaled_red) #[1] 100 8 With this reduced matrix, we can use melt() to create a heatmap amenable data frame. airway_data_for_heatmap &lt;- melt(as.matrix(airway_data_scaled_red)) # transform factor variables to character factor_columns &lt;- sapply(airway_data_for_heatmap, is.factor) airway_data_for_heatmap[factor_columns] &lt;- lapply(airway_data_for_heatmap[factor_columns], as.character) str(airway_data_for_heatmap) #&#39;data.frame&#39;: 800 obs. of 3 variables: # $ Var1 : chr &quot;ENSG00000126709&quot; &quot;ENSG00000278270&quot; &quot;ENSG00000041802&quot; &quot;ENSG00000086712&quot; ... # $ Var2 : chr &quot;SRR1039508&quot; &quot;SRR1039508&quot; &quot;SRR1039508&quot; &quot;SRR1039508&quot; ... # $ value: num 0.031309 0.107736 -0.004342 -0.011695 -0.000109 ... ## we can rename the columns in this data frame names(airway_data_for_heatmap) &lt;- c(&quot;ensgene&quot;, &quot;SRR&quot;, &quot;expression&quot;) head(airway_data_for_heatmap) # ensgene SRR expression #1 ENSG00000126709 SRR1039508 0.0313090216 #2 ENSG00000278270 SRR1039508 0.1077364958 #3 ENSG00000041802 SRR1039508 -0.0043422784 #4 ENSG00000086712 SRR1039508 -0.0116953590 #5 ENSG00000188603 SRR1039508 -0.0001086865 #6 ENSG00000125398 SRR1039508 0.0698569896 10.4 Color palettes online 4) One of the “hardest” parts before or after doing graphics is choosing colors. Many colors can be called by name (e.g., firebrick4) and I have added a file named Rcolor.pdf in this class GitHub repository. Any color also can be included by their hexadecimal color codes (e.g., #005f3f). These codes and the colors can be found in many online color palette websites like following: https://www.color-hex.com https://www.colourlovers.com https://colorbrewer2.org http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/ https://carto.com/carto-colors/ Thankfully, there are some R base color palettes (rainbow, heat.colors, cm.color). These are called as functions, e.g., rainbow() and it will return in hexadecimal color code the number of colors that you want. ## get 5 colors of rainbow() rainbow(5) #[1] &quot;#FF0000FF&quot; &quot;#CCFF00FF&quot; &quot;#00FF66FF&quot; &quot;#0066FFFF&quot; &quot;#CC00FFFF&quot; Yet, there are several R-packages can also help you in choosing colors. I will list some of those that I found useful. 10.5 ggplot2 default palette 5) The package ggplot2 has its own base palette. These can be called when the function ggplot() is called. We can visualize the default colors of ggplot2 usin the package scales that provide scale functions for visualization. Using function hue_pal() to generate a hue palette and show_col() to generate a plot to show colours. ## insall and load &#39;scales&#39; R package -- you will get an error but do worry install.packages(&quot;scales&quot;) library(scales) #Error in value[[3L]](cond) : # Package ‘scales’ version 1.1.0 cannot be unloaded: # Error in unloadNamespace(package) : namespace ‘scales’ is imported by ‘ggplot2’, ‘ggsci’ so cannot be unloaded ## load ggplot2 library(ggplot2) ## Identify hex codes in ggplot2 defult for 10 colors hex_codes1 &lt;- scales::hue_pal()(10) hex_codes1 # [1] &quot;#F8766D&quot; &quot;#D89000&quot; &quot;#A3A500&quot; &quot;#39B600&quot; &quot;#00BF7D&quot; &quot;#00BFC4&quot; &quot;#00B0F6&quot; &quot;#9590FF&quot; &quot;#E76BF3&quot; &quot;#FF62BC&quot; ## Plot hex codes/colors scales::show_col(hex_codes1) 10.6 Heatmaps or tiled charts 6) I will illustrate the diversity of colors using tile plots. These are color matrices (x and y coordinates) that can be plot and they are also heatmaps (tiled charts) where each tile is a continuous value that can be trnaslated to a color scale. To get these plots, we use the function geom_tile(). We make sure that we have a dataset bacteria from package ade4 is in the correc format and ready to use for color exploration. ## we will load packages library(ade4) library(purrr) library(reshape2) library(ggplot2) ## load data and assign to data frame data(bacteria, package = &quot;ade4&quot;) bacteria_espaa &lt;- bacteria$espaa bacteria_espaa_scaled &lt;- purrr::modify(bacteria_espaa, scale) ## reshape dataframe bacteria_espaa_for_heatmap &lt;- melt(as.matrix(bacteria_espaa_scaled)) factor_columns &lt;- sapply(bacteria_espaa_for_heatmap, is.factor) bacteria_espaa_for_heatmap[factor_columns] &lt;- lapply(bacteria_espaa_for_heatmap[factor_columns], as.character) ## we can rename the columns in this data frame names(bacteria_espaa_for_heatmap) &lt;- c(&quot;bacteria_sp&quot;, &quot;Amino_acid&quot;, &quot;scaled_value&quot;) head(bacteria_espaa_for_heatmap) # bacteria_sp Amino_acid scaled_value #1 AERPECG Ala 0.3285668 #2 AQUAECG Ala -0.4008694 #3 ARCFUCG Ala 0.1073331 #4 BACHDCG Ala 0.9012986 #5 BACSUCG Ala 1.0314722 #6 BORBUCG Ala -0.7597170 We can explore some of the colors in ggplot2. ## checking if ggplot2 is loaded require(ggplot2) ## we can plot a heatmap with the expression values heatmap_bacteria_base &lt;- ggplot(bacteria_espaa_for_heatmap,aes(x=Amino_acid,y=bacteria_sp,fill=scaled_value))+ geom_tile()+ theme_bw()+ ggtitle(&quot;My First Heatmap -- Base Color&quot;) + theme(axis.text.y = element_text(size=rel(0.5)), axis.text.x = element_text(size=rel(0.7),angle = 90, vjust = 0.5, hjust=1), legend.title = element_text(color = &quot;black&quot;, size = 7), legend.text = element_text(color = &quot;black&quot;, size = 7)) heatmap_bacteria_base Here is the base colors for this tile plot. 10.7 R-packages about color 10.7.1 viridis package 6) The package viridis can create color scales including palettes for those persons with colorblindness. This package has a vignette that help you with more examples. ## We need to install R-package &#39;viridis&#39; install.packages(&quot;viridis&quot;) library(viridis) #Loading required package: viridisLite ## checking if ggplot2 is loaded require(ggplot2) ## we can plot a heatmap with the expression values heatmap_viridis_bac_magma &lt;- ggplot(bacteria_espaa_for_heatmap,aes(x=Amino_acid,y=bacteria_sp,fill=scaled_value))+ geom_tile()+ scale_fill_viridis(option=&quot;A&quot;)+ theme_bw()+ theme(axis.text.y = element_text(size=rel(0.5)), axis.text.x = element_text(size=rel(0.7),angle = 90, vjust = 0.5, hjust=1)) heatmap_viridis_bac_magma ## NOTE: you might need to run the plot &#39;heatmap_viridis_bac_magma&#39;, if it gives you an error We can also run this color example with airway_data_for_heatmap. ## checking if ggplot2 is loaded require(ggplot2) ## we can plot a heatmap with the expression values heatmap_viridis &lt;- ggplot(airway_data_for_heatmap,aes(x=SRR,y=ensgene,fill=expression))+ geom_tile()+ scale_fill_viridis()+ theme_bw()+ theme(axis.text.y = element_text(size=rel(0.5)), axis.text.x = element_text(size=rel(0.7),angle = 90, vjust = 0.5, hjust=1)) ## NOTE: this will require time to load (if it is huge) -- you might not want to run this plot heatmap_viridis 10.7.2 RColorBrewer package 7) The package RColorBrewer. This package is a collection of color palettes that can be easily incorporated in plots. ## We need to install R-package &#39;RColorBrewer&#39; install.packages(&quot;RColorBrewer&quot;) library(RColorBrewer) We can display available color palettes. Notice that names on the row can be call when using ggplot2. display.brewer.all() This image provides three types of palettes: diverging, qualitative, and sequential color. The top group are colors for gradient quantitative and sequential data that progresses from low to higher values (e.g., temperature measurements). The middle group are colors for discrete or categorical data that do not imply magnitude differences between values or states in the data (e.g., types of fruits). The lower group are colors for quantitative and diverting data that increases in magnitude toward opposites or extremes (e.g., above and below sea level). 8) The package [RColorBrewer] also provides colorblind-friendly palettes and these can be call with the argument colorblindFriendly = TRUE. display.brewer.all(colorblindFriendly = TRUE) 9) For most practical uses, you will need only few colors based on your requirements and RColorBrewer allows to select specific palettes and the number of colors that you want. ## we will load &#39;ade4&#39; and the ‘ichtyo’ dataset library(ade4) data(ichtyo) ichtyo_data &lt;- ichtyo$tab head(ichtyo_data) # HOT VAN CHE SPI GOU BAF GAR ABL PER #1 57 47 60 11 27 17 10 9 21 #2 9 25 19 6 2 5 0 4 6 #3 48 60 52 16 25 24 17 8 12 #4 26 50 32 8 6 11 2 10 5 #5 26 43 31 14 9 18 13 14 2 #6 62 43 47 16 11 9 15 16 3 ## we use the function &#39;melt()&#39;&#39; of the package &#39;reshape2&#39; to arrange on the &#39;ggplot2&#39; format library(reshape2) ichtyo_data_ggplot &lt;- melt(ichtyo_data) #No id variables; using all as measure variables names(ichtyo_data_ggplot) &lt;- c(&quot;faunistic_array&quot;, &quot;counts&quot;) str(ichtyo_data_ggplot) #&#39;data.frame&#39;: 288 obs. of 2 variables: # $ faunistic_array: Factor w/ 9 levels &quot;HOT&quot;,&quot;VAN&quot;,&quot;CHE&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... # $ counts : num 57 9 48 26 26 62 17 18 9 12 ... ## View a single RColorBrewer palette by specifying its name (e.g., &#39;Paired&#39;) and 9 colors for each faunistic_array display.brewer.pal(n = 9, name = &#39;Paired&#39;) # you can also request the hexadecimal color code brewer.pal(n = 9, name = &#39;Paired&#39;) #[1] &quot;#A6CEE3&quot; &quot;#1F78B4&quot; &quot;#B2DF8A&quot; &quot;#33A02C&quot; &quot;#FB9A99&quot; &quot;#E31A1C&quot; &quot;#FDBF6F&quot; &quot;#FF7F00&quot; &quot;#CAB2D6&quot; my_paired_palette &lt;- brewer.pal(n = 9, name = &#39;Paired&#39;) 10) To implement the color palette selected, we then can call if from the ggplot2 function ggplot(). However, ggplot2 has an standard palette so we will start with that for comparison. This is the boxplot with ggplot2 with its default colors. ## check that ggplot2 has been loaded require(&quot;ggplot2&quot;) ## Box plot with counts on y-axis and faunistic_array on x-axis and color by faunistic_array my_ichtyo_boxplot &lt;- ggplot(data = ichtyo_data_ggplot, mapping = aes(faunistic_array, counts)) + geom_boxplot(aes(fill = faunistic_array)) + theme_minimal() + theme(legend.position = &quot;&quot;) my_ichtyo_boxplot This is the boxplot with ggplot2 with our RColorBrewer Paired color palette using the function scale_fill_brewer(). my_ichtyo_boxplot2 &lt;- my_ichtyo_boxplot + scale_fill_brewer(palette = &#39;Paired&#39;) my_ichtyo_boxplot2 This is a scatter plot with ggplot2 with its default colors. ## check that ggplot2 has been loaded require(&quot;ggplot2&quot;) ## Scatter plot with counts on y-axis and faunistic_array on x-axis and color by faunistic_array my_ichtyo_scatter &lt;- ggplot(data = ichtyo_data_ggplot, mapping = aes(faunistic_array, counts)) + geom_violin() + geom_point(aes(color = faunistic_array)) + theme_minimal() + theme(legend.position = &quot;&quot;) my_ichtyo_scatter ## Notice the violin shapes under the scatter plot. class(my_ichtyo_scatter) #[1] &quot;gg&quot; &quot;ggplot&quot; This is the scatter plot with ggplot2 with 9 colors form RColorBrewer Paired and using scale_colour_manual(). my_paired_palette &lt;- brewer.pal(n = 9, name = &#39;Paired&#39;) my_paired_palette #[1] &quot;#A6CEE3&quot; &quot;#1F78B4&quot; &quot;#B2DF8A&quot; &quot;#33A02C&quot; &quot;#FB9A99&quot; &quot;#E31A1C&quot; &quot;#FDBF6F&quot; &quot;#FF7F00&quot; &quot;#CAB2D6&quot; my_ichtyo_scatter2 &lt;- my_ichtyo_scatter + scale_colour_manual(values = my_paired_palette) my_ichtyo_scatter2 class(my_ichtyo_scatter2) #[1] &quot;gg&quot; &quot;ggplot&quot; Notice how easy is to change colors from existing ggplot object, in this case my_ichtyo_scatter. We will explore this further as we progress on this session. 10.7.3 ggsci package 11) The package ggsci is general collection of scientific journal and sci-fi themed color palettes for ggplot2. ## We need to install R-package &#39;ggsci&#39; install.packages(&quot;ggsci&quot;) library(ggsci) The ggsci has its own vignette. The palettes in this package are diverse and here is a list from their vignette. Name Scales Palette Types Palette Generator NPG scale_color_npg() scale_fill_npg() “nrc” pal_npg() AAAS scale_color_aaas() scale_fill_aaas() “default” pal_aaas() NEJM scale_color_nejm() scale_fill_nejm() “default” pal_nejm() Lancet scale_color_lancet() scale_fill_lancet() “lanonc” pal_lancet() JAMA scale_color_jama() scale_fill_jama() “default” pal_jama() JCO scale_color_jco() scale_fill_jco() “default” pal_jco() UCSCGB scale_color_ucscgb() scale_fill_ucscgb() “default” pal_ucscgb() D3 scale_color_d3() scale_fill_d3() “category10” “category20” “category20b” “category20c” pal_d3() LocusZoom scale_color_locuszoom() scale_fill_locuszoom() “default” pal_locuszoom() IGV scale_color_igv() scale_fill_igv() “default”“alternating” pal_igv() UChicago scale_color_uchicago() scale_fill_uchicago() “default”“light” “dark” pal_uchicago() Star Trek scale_color_startrek() scale_fill_startrek() “uniform” pal_startrek() Tron Legacy scale_color_tron() scale_fill_tron() “legacy” pal_tron() Futurama scale_color_futurama() scale_fill_futurama() “planetexpress” pal_futurama() Rick and Morty scale_color_rickandmorty() scale_fill_rickandmorty() “schwifty” pal_rickandmorty() The Simpsons scale_color_simpsons() scale_fill_simpsons() “springfield” pal_simpsons() GSEA scale_color_gsea() scale_fill_gsea() “default” pal_gsea() Material Design scale_color_material() scale_fill_material() “red” “pink” “purple” “deep-purple” “indigo” “blue” “light-blue” “cyan” “teal” “green” “light-green” “lime” “yellow” “amber” “orange” “deep-orange” “brown” “grey” “blue-grey” pal_material() 12) You apply this palettes as we did in 10) using scale_color_XXX() or scale_fill_XXX(). For example, we will use my_ichtyo_boxplot the aaas palette. my_ichtyo_ggsci_aaas &lt;- my_ichtyo_boxplot + scale_fill_aaas() my_ichtyo_ggsci_aaas We can use my_ichtyo_scatter the uchicago palette. my_ichtyo_ggsci_uchicago &lt;- my_ichtyo_scatter + scale_fill_uchicago() my_ichtyo_ggsci_uchicago 10.7.4 wesanderson package 13) The package wesanderson is a collection of palettes generated mostly from ‘Wes Anderson’ movies that can be incorporated in ggplot2. ## We need to install R-package &#39;wesanderson&#39; install.packages(&quot;wesanderson&quot;) library(wesanderson) The list of the palettes present include: BottleRocket1,BottleRocket2,Rushmore1,Royal1,Royal2,Zissou1,Darjeeling1,Darjeeling2,Chevalier1,FantasticFox1,Moonrise1,Moonrise2,Moonrise3,Cavalcanti1,GrandBudapest1,GrandBudapest2,IsleofDogs1,IsleofDogs2. # I will need 9 colors in a discrete scale for &#39;my_ichtyo_boxplot&#39; my_wes_palette &lt;- wes_palette(&quot;IsleofDogs2&quot;, 9, type = &quot;continuous&quot;) my_wes_palette str(my_wes_palette) #&#39;palette&#39; chr [1:9] &quot;#EAD3BF&quot; &quot;#CAB3A2&quot; &quot;#AA9486&quot; &quot;#B08C69&quot; &quot;#B6854D&quot; &quot;#775B3E&quot; &quot;#39312F&quot; &quot;#2A2423&quot; &quot;#1C1718&quot; # - attr(*, &quot;name&quot;)= chr &quot;IsleofDogs2&quot; We can apply the my_ichtyo_scatter with ggplot2 with these 9 colors using scale_colour_manual(). my_ichtyo_scatter_wes &lt;- my_ichtyo_scatter + scale_colour_manual(values = my_wes_palette) my_ichtyo_scatter_wes 10.7.5 rcartocolor package 14) The package rcartocolor is a collection of color schemes for maps and other graphicsdesigned by ‘CARTO’ as described at https://carto.com/carto-colors/. It includes four types of palettes: aggregation, diverging, qualitative,and quantitative that can be incorporated in ggplot2. ## We need to install R-package &#39;rcartocolor&#39; install.packages(&quot;rcartocolor&quot;) library(rcartocolor) ## diplay all colors display_carto_all() The list of 34 palettes presented can be select for a number specific of colors and if you want them to be colorblind_friendly = TRUE. ## check information about how to change colors in palette ?scale_fill_carto_d ## color the boxplot my_ichtyo_boxplot_carto &lt;- my_ichtyo_boxplot + scale_fill_carto_d(type = &quot;qualitative&quot;, palette = &quot;Vivid&quot;, direction = -1) my_ichtyo_boxplot_carto 10.7.6 colorspace package 15) Exploring and selecting your own colors and palettes can be performed with colorspace that includes an extensive manual. To use colorspace, we can do as follow. ## install and load package install.packages(&quot;colorspace&quot;) library(colorspace) ## palettes in this package, type and their names hcl_palettes() #HCL palettes # #Type: Qualitative #Names: Pastel 1, Dark 2, Dark 3, Set 2, Set 3, Warm, Cold, Harmonic, Dynamic # #Type: Sequential (single-hue) #Names: Grays, Light Grays, Blues 2, Blues 3, Purples 2, Purples 3, Reds 2, Reds 3, Greens 2, Greens 3, Oslo # #Type: Sequential (multi-hue) #Names: Purple-Blue, Red-Purple, Red-Blue, Purple-Orange, Purple-Yellow, Blue-Yellow, Green-Yellow, Red-Yellow, Heat, Heat 2, Terrain, # Terrain 2, Viridis, Plasma, Inferno, Dark Mint, Mint, BluGrn, Teal, TealGrn, Emrld, BluYl, ag_GrnYl, Peach, PinkYl, # Burg, BurgYl, RedOr, OrYel, Purp, PurpOr, Sunset, Magenta, SunsetDark, ag_Sunset, BrwnYl, YlOrRd, YlOrBr, OrRd, Oranges, # YlGn, YlGnBu, Reds, RdPu, PuRd, Purples, PuBuGn, PuBu, Greens, BuGn, GnBu, BuPu, Blues, Lajolla, Turku, Hawaii, Batlow # #Type: Diverging #Names: Blue-Red, Blue-Red 2, Blue-Red 3, Red-Green, Purple-Green, Purple-Brown, Green-Brown, Blue-Yellow 2, Blue-Yellow 3, # Green-Orange, Cyan-Magenta, Tropic, Broc, Cork, Vik, Berlin, Lisbon, Tofino hcl_palettes(plot = TRUE) You then can select the palette that you want and request the number of colors for your plot. These color will be returned as hexadecimal codes. ## you want 5 qualitative colors from palette my_qualitative &lt;- qualitative_hcl(5, palette = &quot;Harmonic&quot;) my_qualitative #[1] &quot;#C7A76C&quot; &quot;#99B56B&quot; &quot;#5CBD92&quot; &quot;#3BBCBF&quot; &quot;#7DB0DD&quot; ## you want 5 sequential colors from palette my_sequential &lt;- sequential_hcl(5, palette = &quot;Batlow&quot;) my_sequential #[1] &quot;#201158&quot; &quot;#005E5E&quot; &quot;#578B21&quot; &quot;#E89E6B&quot; &quot;#FFCEF4&quot; ## you want 5 diverging colors from palette my_diverging &lt;- diverging_hcl(5, palette = &quot;Berlin&quot;) my_diverging #[1] &quot;#7FBFF5&quot; &quot;#004B73&quot; &quot;#111111&quot; &quot;#713430&quot; &quot;#F8A29E&quot; This R packate provides with some demos to visualize how it will look like. ## my_qualitative visualization specplot(my_qualitative, type = &quot;o&quot;) ## my_sequential visualization specplot(my_sequential, type = &quot;o&quot;) ## my_diverging visualization specplot(my_diverging, type = &quot;o&quot;) 10.8 Making your own palette 16) In many instances, you want your own palette and this can be done with the base function colorRampPalette(). You have to pick your colors by name as in the Rcolor.pdf. Alternatively, you can get the names by colors(). We also want to be able to the colors, so we use the function show_col() from the R package scales. ## get a vector of all colors by name colors() # [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; &quot;antiquewhite1&quot; &quot;antiquewhite2&quot; &quot;antiquewhite3&quot; # [7] &quot;antiquewhite4&quot; &quot;aquamarine&quot; &quot;aquamarine1&quot; &quot;aquamarine2&quot; &quot;aquamarine3&quot; &quot;aquamarine4&quot; #[13] &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; &quot;azure3&quot; &quot;azure4&quot; &quot;beige&quot; #[19] &quot;bisque&quot; &quot;bisque1&quot; &quot;bisque2&quot; &quot;bisque3&quot; &quot;bisque4&quot; &quot;black&quot; # ... ## the total number of colors by name length(colors()) #[1] 657 ## for our example, we will get three colors by name in a vector my_colors_by_name &lt;- c(&quot;firebrick4&quot;, &quot;gray89&quot;, &quot;navyblue&quot;) scales::show_col(my_colors_by_name) Likewise, you can get some hexadecimal colors codes for those that you want, e.g., #EEBD91, #F5A2A2, #40CB63, #4682B4 and #862185. ## for our example, we will get these 5 hex colors in a vector my_colors_by_hex &lt;- c(&quot;#EEBD91&quot;, &quot;#F5A2A2&quot;, &quot;#40CB63&quot;, &quot;#4682B4&quot;, &quot;#862185&quot;) scales::show_col(my_colors_by_hex) To generate your palette, you use the function colorRampPalette(). ##NOTE: the vector of colors that you want to generate in this case 9 and this number is in parenthesis, so it is (9) my_colors_by_name_ramp &lt;- colorRampPalette(my_colors_by_name)(9) my_colors_by_name_ramp #[1] &quot;#8B1A1A&quot; &quot;#A14C4C&quot; &quot;#B67E7E&quot; &quot;#CCB0B0&quot; &quot;#E3E3E3&quot; &quot;#AAAACA&quot; &quot;#7171B1&quot; &quot;#383898&quot; &quot;#000080&quot; my_colors_by_hex_ramp &lt;- colorRampPalette(my_colors_by_hex)(9) my_colors_by_hex_ramp #[1] &quot;#EEBD91&quot; &quot;#F1AF99&quot; &quot;#F5A2A2&quot; &quot;#9AB682&quot; &quot;#40CB63&quot; &quot;#43A68B&quot; &quot;#4682B4&quot; &quot;#66519C&quot; &quot;#862185&quot; Finally, we can apply these color ramps to my_ichtyo_scatter with ggplot2 using scale_colour_manual(). my_ichtyo_scatter_by_name &lt;- my_ichtyo_scatter + scale_colour_manual(values = my_colors_by_name_ramp) my_ichtyo_scatter_by_name my_ichtyo_scatter_by_hex &lt;- my_ichtyo_scatter + scale_colour_manual(values = my_colors_by_hex_ramp) my_ichtyo_scatter_by_hex 17) You can extract colors for an image that you like (jpg) from an url (website) or a photo using a clever combination of functions of the R packages:magick, imager, dplyr. Other packages like scales and ggplot2 were previously installes. ## Load up packages I&#39;m going to use first. install.packages(&quot;magick&quot;) install.packages(&quot;imager&quot;) install.packages(&quot;dplyr&quot;) ## required libraries library(ggplot2) library(magick) library(scales) library(imager) library(dplyr) ## read image to extract color ## NOTE: this has to be your own image, I will not post this jpg. my_image &lt;- &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/figures_emer/samurai_champloo.jpg&quot; ## Function to get your image processed, n number of colors out of your image, and cs as colour space. get_colorPal_form_image &lt;- function(image_file_input, n=8, cs=&quot;RGB&quot;, show_image_row_user = TRUE){ #require libraries require(ggplot2) require(magick) require(scales) require(imager) require(dplyr) cat(&quot;\\n\\n *** code modified from: https://www.r-bloggers.com/2019/01/extracting-colours-from-your-images-with-image-quantization/ *** \\n\\n&quot;) image_file_input_pros &lt;- image_file_input show_image_row &lt;- show_image_row_user raw_image &lt;- magick::image_read(image_file_input_pros) tmp &lt;- magick::image_resize(image = raw_image , &quot;100&quot;) # reducing colours! different colorspace gives you different result tmp &lt;- magick::image_quantize(tmp, max=n, colorspace=cs) # converting, becauase I want to use as.data.frame function in imager package. tmp &lt;- imager::magick2cimg(tmp) # sorting colour by hue rather than RGB (red green blue) tmp &lt;- imager::RGBtoHSV(tmp) # making it wide makes it easier to output hex colour tmp &lt;- as.data.frame(tmp, wide=&quot;c&quot;) tmp &lt;- dplyr::mutate(tmp, hex = hsv(rescale(c.1, from=c(0,360)),c.2,c.3), hue = c.1, sat = c.2, value = c.3) tmp &lt;- dplyr::count(tmp, hex, hue, sat,value, sort=T) tmp &lt;- dplyr::mutate(tmp, colorspace = cs) # out colors palette color_hex &lt;- tmp$hex color_df &lt;- tmp # plot colors if(show_image_row) {magick::image_browse(raw_image)} scales::show_col(color_hex) # return colors return(list(color_hex, color_df)) } ## running the function and asking for 8 colors samuari_champloo_color_hex &lt;- get_colorPal_form_image(my_image, n= 8) # hex codes samuari_champloo_color_hex[[1]] #[1] &quot;#FFE39D&quot; &quot;#D93907&quot; &quot;#2F1E16&quot; &quot;#F86413&quot; &quot;#A56343&quot; &quot;#F8D894&quot; &quot;#E9A264&quot; scales::show_col(samuari_champloo_color_hex[[1]]) Here is an image that is free in its URL to apply the function get_colorPal_form_image(). ## from URL from jpg. my_image &lt;- &quot;https://upload.wikimedia.org/wikipedia/commons/a/a4/Piet_Mondriaan%2C_1930_-_Mondrian_Composition_II_in_Red%2C_Blue%2C_and_Yellow.jpg&quot; Mondriaan_color_hex &lt;- get_colorPal_form_image(my_image, n= 10) Mondriaan_color_hex #[[1]] # [1] &quot;#DB2920&quot; &quot;#E3312C&quot; &quot;#EAE9E9&quot; &quot;#E5E4DE&quot; &quot;#2E2B2F&quot; &quot;#617098&quot; &quot;#DCD6CD&quot; &quot;#DFCCA4&quot; &quot;#E3DDD8&quot; &quot;#E2AB9C&quot; # #[[2]] # hex hue sat value n colorspace #1 #DB2920 2.887701 0.853881279 0.8588235 2669 RGB #2 #E3312C 1.639344 0.806167401 0.8901961 2286 RGB #3 #EAE9E9 0.000000 0.004273504 0.9176471 1636 RGB #4 #E5E4DE 51.428571 0.030567686 0.8980392 1197 RGB #5 #2E2B2F 285.000000 0.085106383 0.1843137 724 RGB #6 #617098 223.636364 0.361842105 0.5960784 670 RGB #7 #DCD6CD 36.000000 0.068181818 0.8627451 412 RGB #8 #DFCCA4 40.677966 0.264573991 0.8745098 203 RGB #9 #E3DDD8 27.272727 0.048458150 0.8901961 161 RGB #10 #E2AB9C 12.857143 0.309734513 0.8862745 142 RGB "],["basic-graphics.html", "Session 11 – Basic Graphics 11.1 Bar charts 11.2 Stacked bar charts 11.3 Histograms 11.4 Multiple histograms 11.5 Density plots 11.6 Box plots 11.7 Violin plots 11.8 Scatterplots 11.9 Lines and Scatterplots 11.10 Multiple graphs at once with cowplot 11.11 Save your plots as PDF or PNG 11.12 Other resources", " Session 11 – Basic Graphics 11.1 Bar charts One of the most basic graphs is those that illustrate counts or mean values of discrete variables or ranges of values as groups in a continuous variable. For counts, the x-axis includes discrete variables and the y-axis are the counts assigned to each these variables (this is a bar graph). For a continuous variable, values on this variable can be divided into groups or ranges (e.g., 0-5, 6-10, 11-15, 16-20, etc) in the x-axis and then the number of times that individual values of the continuous variable fall in the ranges are represented in the y-axis (this is a histogram). 17) To do a bar chart, we need a dataset of discrete variables that include count and in the correct format to ggplot2. We will use the ‘ichtyo’ dataset of the package ‘ade4.’ ## we will load &#39;ade4&#39; and the ‘ichtyo’ dataset library(ade4) data(ichtyo) ichtyo_data &lt;- ichtyo$tab head(ichtyo_data) # HOT VAN CHE SPI GOU BAF GAR ABL PER #1 57 47 60 11 27 17 10 9 21 #2 9 25 19 6 2 5 0 4 6 #3 48 60 52 16 25 24 17 8 12 #4 26 50 32 8 6 11 2 10 5 #5 26 43 31 14 9 18 13 14 2 #6 62 43 47 16 11 9 15 16 3 str(ichtyo_data) #&#39;data.frame&#39;: 32 obs. of 9 variables: # $ HOT: num 57 9 48 26 26 62 17 18 9 12 ... # $ VAN: num 47 25 60 50 43 43 34 22 14 9 ... # $ CHE: num 60 19 52 32 31 47 28 18 8 9 ... # $ SPI: num 11 6 16 8 14 16 13 9 5 10 ... # $ GOU: num 27 2 25 6 9 11 8 3 8 2 ... # $ BAF: num 17 5 24 11 18 9 13 7 8 2 ... # $ GAR: num 10 0 17 2 13 15 14 6 1 10 ... # $ ABL: num 9 4 8 10 14 16 5 8 1 5 ... # $ PER: num 21 6 12 5 2 3 4 4 0 0 ... For a bar chart, we need total counts. Therefore, we need to get the total sums per each column using colSums() and as a data frame using as.data.frame(). ## colum sum as data frame ichtyo_sum_df &lt;- as.data.frame(colSums (ichtyo_data, na.rm = TRUE)) ichtyo_sum_df # colSums(ichtyo_data, na.rm = TRUE) #HOT 936 #VAN 995 #CHE 1741 #SPI 491 #GOU 951 #BAF 658 #GAR 1007 #ABL 337 #PER 625 ## build a nicer data frame and add column of site_names ichtyo_sum_df$site_names &lt;- rownames(ichtyo_sum_df) ## upgrade column names names(ichtyo_sum_df) &lt;- c(&quot;counts&quot;, &quot;site_names&quot;) ichtyo_sum_df # counts site_names #HOT 936 HOT #VAN 995 VAN #CHE 1741 CHE #SPI 491 SPI #GOU 951 GOU #BAF 658 BAF #GAR 1007 GAR #ABL 337 ABL #PER 625 PER ## add count labels for later use ichtyo_sum_df$count_label &lt;- as.character(ichtyo_sum_df$counts) str(ichtyo_sum_df) #&#39;data.frame&#39;: 9 obs. of 3 variables: # $ counts : num 936 995 1741 491 951 ... # $ site_names : chr &quot;HOT&quot; &quot;VAN&quot; &quot;CHE&quot; &quot;SPI&quot; ... # $ count_label: chr &quot;936&quot; &quot;995&quot; &quot;1741&quot; &quot;491&quot; ... We can now plot our bar chart with site_names in the x-axis and counts in the y-axis. A series the options to the ggplot are added: ggplot() is the main scaffold of the plot and includes the argument data = ichtyo_sum_df as source of data,aes() defines axis in the 2D ggplot, x is the x-axis and we defined site_names as the variable for that axis,y is the y-axis and we defined counts as the variable for that axis, fill is the variable that will define groups to color the bars in the plot.geom_bar() is the option for bar charts, the argument colour = black is for black countour of the bars, the argument stat = \"identity\" basically says that we want the values of the y-axis (i.e., counts) as the heights of the bars.guides(fill = FALSE) indicagtes that we do not want legends. theme_minimal() is a present style of the plot that is simple. xlab(), ylab() and ggtitle() are labels for the axis and title, you can put in there whatever you want. ## checking if ggplot2 and ggsci are loaded require(ggplot2) require(ggsci) # base plot ichtyo_barchart &lt;- ggplot(data=ichtyo_sum_df, aes(x=site_names, y=counts, fill = site_names)) + geom_bar(colour=&quot;black&quot;, stat=&quot;identity&quot;) + guides(fill=FALSE) + theme_minimal() + xlab(&quot;Sites on a place&quot;) + ylab(&quot;Total counts&quot;) + ggtitle(&quot;My First Bar Chart&quot;) ichtyo_barchart We can simplify this plot with adding the counts on top of the bars with the count_label of the ichtyo_sum_df and remove the background grids. A series the options to the ggplot are added: geom_text() will add the text of the count_label using the site_names and counts as x and y coordinates and the argument vjust=-1 will move it a bit above the x/y coordinate.theme() will remove some unwanted elements by assigning them as element_blank() these include the y-axis text and all the grids. ichtyo_barchart &lt;- ggplot(data=ichtyo_sum_df, aes(x=site_names, y=counts, fill = site_names)) + geom_bar(colour=&quot;black&quot;, stat=&quot;identity&quot;) + guides(fill=FALSE) + theme_minimal() + xlab(&quot;Sites on a place&quot;) + ylab(&quot;Total counts&quot;) + ggtitle(&quot;My First Bar Chart&quot;) + geom_text(data=ichtyo_sum_df,aes(x=site_names,y=counts,label=count_label),vjust=-1) + theme(axis.text.y = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) ichtyo_barchart The base colors of this plot are becoming cliché as the enormous popularity of ggplot2, so we can change it to something more original using the color packages that we presented on previous section. # plot with futurama fill colors ichtyo_barchart &lt;- ichtyo_barchart + scale_fill_futurama() ichtyo_barchart 11.2 Stacked bar charts 18) Some people argue against stacked bar charts, yet they are much better than pie charts to show relationships between proportions per sample. We will use the ‘geom_col()‘ with the same dataset ‘ecomor’ of the package ‘ade4.’ Notice that is a rather complex processing, yet you should be able to dissect the processing from input data to data frame ready for ggplot2. ## we will load &#39;ade4&#39; and the ‘ecomor’ dataset library(ade4) data(ecomor) #taxois a data frame with 129 species and 3 factors: Genus, Family and Order. It is a data frame of class&#39;taxo&#39;: the variables are factors giving nested classifications. ecomor_data &lt;- ecomor$taxo #categ is a data frame with 129 species, 2 factors : ’forsub’ summarizing the feeding place and ’diet’the diet type. ecomor_data &lt;- cbind(ecomor_data,ecomor$categ) str(ecomor_data) #&#39;data.frame&#39;: 129 obs. of 5 variables: # $ Genus : Factor w/ 86 levels &quot;Acanithis&quot;,&quot;Aegithalos&quot;,..: 11 12 12 49 70 21 21 73 85 85 ... # $ Family: Factor w/ 35 levels &quot;Aegithalidae&quot;,..: 31 31 31 31 31 6 6 6 6 6 ... # $ Ordre : Factor w/ 7 levels &quot;Apodiformes&quot;,..: 1 1 1 1 1 2 2 2 2 2 ... # $ forsub: Factor w/ 7 levels &quot;A&quot;,&quot;B&quot;,&quot;F&quot;,&quot;G&quot;,..: 5 5 5 1 3 5 5 4 4 4 ... # $ diet : Factor w/ 8 levels &quot;F&quot;,&quot;G&quot;,&quot;H&quot;,&quot;I&quot;,..: 7 7 7 7 7 2 6 2 2 2 ... ## we subset to families and diet ecomor_data_set &lt;- subset(ecomor_data, select = c(Family, diet)) head(ecomor_data_set) # Family diet #E033 Trochilidae N #E034 Trochilidae N #E035 Trochilidae N #E070 Trochilidae N #E071 Trochilidae N #E001 Columbidae G ## we split by family list_of_families &lt;- split(ecomor_data_set, ecomor_data_set$Family) list_of_families #$Aegithalidae # Family diet #E121 Aegithalidae I #E048 Aegithalidae I # #$Alaudidae # Family diet #E104 Alaudidae H # #$Cardinalidae # Family diet #E057 Cardinalidae G #E056 Cardinalidae G #... ## we can calculate the frequency of diet types by each family of these birds collect_families_processed &lt;- list() for(i in 1:length(list_of_families)) { # i &lt;- 1 one_family &lt;- list_of_families[[i]] name_family &lt;- unique(one_family[,1]) one_family_df &lt;- as.data.frame(table(one_family [,2]), stringsAsFactors = FALSE) one_family_df_sum &lt;- sum(one_family_df$Freq) one_family_df$Freq &lt;- one_family_df$Freq/one_family_df_sum names(one_family_df) &lt;- c(&quot;type_diet&quot;,&quot;Freq&quot;) one_family_df$Family &lt;- name_family collect_families_processed [[i]] &lt;- one_family_df } ## we put all of these in a single data frame collect_families_processed_df &lt;- do.call(rbind, collect_families_processed) head(collect_families_processed_df) # type_diet Freq Family #1 F 0 Aegithalidae #2 G 0 Aegithalidae #3 H 0 Aegithalidae #4 I 1 Aegithalidae #5 J 0 Aegithalidae #6 K 0 Aegithalidae str(collect_families_processed_df) #&#39;data.frame&#39;: 280 obs. of 3 variables: # $ type_diet: chr &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; ... # $ Freq : num 0 0 0 1 0 0 0 0 0 0 ... # $ Family : Factor w/ 35 levels &quot;Aegithalidae&quot;,..: 1 1 1 1 1 1 1 1 2 2 ... We can now plot a stacked bar plot. Notice we need 3 variables (columns): 2 categorical and 1 numeric. In this case the two categorical are type_of_diet and Family, and the one continuous is Freq. ## checking if ggplot2 and ggsci are loaded require(ggplot2) require(ggsci) ## the plot diet_family_stackedbar &lt;- ggplot(collect_families_processed_df) + geom_col(aes(x = Family, y = Freq, fill = type_diet), color = &quot;black&quot;)+ labs(title=&quot;Stacked Bar Graph -- Diet and Families Birds&quot;, x= &quot;Family&quot; , y=&quot;Frequency&quot;) + coord_flip() + theme_minimal() + theme(axis.text.x = element_text(size=rel(0.7),angle = 90, vjust = 0.5, hjust=1)) diet_family_stackedbar &lt;- diet_family_stackedbar + scale_fill_nejm() diet_family_stackedbar 11.3 Histograms 19) To do a histogram, we need a dataset with continuous variables and we will divide into ranges and format them into ggplot2. We will use two samples of the airway_scaledcounts.csv dataset that indicates gene expression. ## NOTE: remember to update the path to file with the dataset where you downloaded in your computer -- THIS IS EXCLUSIVE TO YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW ## load get &#39;airway_scaledcounts.csv&#39; dataset airway_data &lt;- read.table(&quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/ref_files_pdfs/airway_scaledcounts.csv&quot;, header = TRUE, sep = &quot;,&quot;, stringsAsFactors = FALSE) # remove &#39;ensgene&#39; variable airway_data_red &lt;- airway_data[,-1] # select two sample: &quot;SRR1039509&quot; and &quot;SRR1039517&quot; airway_data_red &lt;- subset(airway_data_red, select = c(SRR1039509,SRR1039517)) str(airway_data_red) #&#39;data.frame&#39;: 38694 obs. of 2 variables: #$ SRR1039509: num 486 0 523 258 81 ... #$ SRR1039517: num 1097 0 781 447 94 ... We estimate a histogram for sample SRR1039509, for this we need to explore this with summary(). ## summary summary(airway_data_red$SRR1039509) # Min. 1st Qu. Median Mean 3rd Qu. Max. # 0.0 0.0 1.0 501.1 172.0 244921.0 For a regular histogram in ggplot2 we want geom_histogram() with the airway_data_red data frame that includes sample SRR1039509. Next, we need to define some parameters: (1) how often we want the bins to be separated with the argument breaks(), it usually will be a sequence called by function seq() and in this case from a minimum (i.e., 0) and maximum (i.e., 244921 - 250000) and width argument by= for example 1000. (2) you can choose the color of the boundary of the histogram with col= like red, its fill color fill= like green and transparency alpha where 0 is transparent and 1 completely opaque. ## checking if ggplot2 is loaded require(ggplot2) SRR1039509_histogram &lt;- ggplot(data=airway_data_red, aes(x=SRR1039509)) + geom_histogram(breaks=seq(0, 250000, by=1000), col=&quot;red&quot;, fill=&quot;green&quot;, alpha = .2) + labs(title=&quot;Histogram for SRR1039509&quot;, x=&quot;Expression range&quot;, y=&quot;Count&quot;) + xlim(c(0,250000))+ theme_minimal() SRR1039509_histogram This histogram is quite skewed to the right and we notice the maximum of the sample (i.e., 244921) is ~500 x mean value. Some genes are highly overexpressed and most of these are rare (low count). We can modify the breaks() argument to series of individual breaks every 100 (e.g., 100,200,..,2000), in this last break it we want to include all genes +2000 expression units. SRR1039509_histogram &lt;- ggplot(data=airway_data_red, aes(x=SRR1039509)) + geom_histogram(breaks=seq(0, 2000, by=100), col=&quot;red&quot;, fill=&quot;green&quot;, alpha = .2) + labs(title=&quot;Histogram for SRR1039509&quot;, x=&quot;Expression range&quot;, y=&quot;Count&quot;) + xlim(c(0,2000))+ theme_minimal() SRR1039509_histogram This histogram is better, yet not ideal. First, it still is right skewed and count values &gt;2000 were excluded from the graph. This suggest that we need to simplify our data and bin all values more 2000 to that maximum. We can do this by creating an small function that we create and named cap_2000_fun that uses ifelse(). ## create a small function to cap values to 2000 cap_2000_fun &lt;- function(x) {cap &lt;- ifelse(x&gt;2000,2000, x) return(cap)} ## we can test our function cap_2000_fun(1) #[1] 1 cap_2000_fun(10000) #[1] 2000 Now, we can apply this function to airway_data_red using the modify() from the package purrr. ## modify values with function so all values &gt;2000 will be cap to 2000 airway_data_red_capped &lt;- purrr::modify(airway_data_red, cap_2000_fun) ## plot histogram SRR1039509_histogram_capped &lt;- ggplot(data=airway_data_red_capped, aes(x=SRR1039509)) + geom_histogram(breaks=seq(0, 2000, by=100), col=&quot;red&quot;, fill=&quot;green&quot;, alpha = .2) + labs(title=&quot;Histogram for SRR1039509&quot;, x=&quot;Expression range with capped values to 2000 if more than this value&quot;, y=&quot;Count&quot;) + xlim(c(0,2000))+ theme_minimal() SRR1039509_histogram_capped This histogram still looks skewed to the right. We can try bin all values above 500 by creating a new function and use shorter breaks every 10 on ggplot2. ## create a small function to cap values to 500 cap_500_fun &lt;- function(x) {cap &lt;- ifelse(x&gt;500,500, x) return(cap)} ## modify values with function so all values &gt;500 will be cap to 500 airway_data_red_capped &lt;- purrr::modify(airway_data_red, cap_500_fun) ## plot histogram and modify breaks SRR1039509_histogram_capped &lt;- ggplot(data=airway_data_red_capped, aes(x=SRR1039509)) + geom_histogram(breaks=seq(0, 500, by=10), col=&quot;red&quot;, fill=&quot;green&quot;, alpha = .2) + labs(title=&quot;Histogram for SRR1039509&quot;, x=&quot;Expression range with capped&quot;, y=&quot;Count&quot;) + xlim(c(0,500))+ theme_minimal() SRR1039509_histogram_capped We can also remove some low expressed genes (e.g., values less than 100) and over expressed genes (e.g., values more than 500). This can be done just by defining the argument xlim() between 100 and 400. ## plot histogram and modify breaks SRR1039509_histogram_capped &lt;- ggplot(data=airway_data_red_capped, aes(x=SRR1039509)) + geom_histogram(breaks=seq(0, 500, by=10), col=&quot;red&quot;, fill=&quot;green&quot;, alpha = .2) + labs(title=&quot;Histogram for SRR1039509&quot;, x=&quot;Expression range with capped -- yet be dropped less 100 and more 400 counts&quot;, y=&quot;Count&quot;) + xlim(c(100,400))+ theme_minimal() ## NOTICE: the warning about dropping rows outside the xlim(). SRR1039509_histogram_capped #Warning messages: #1: Removed 34710 rows containing non-finite values (stat_bin). #2: Removed 20 rows containing missing values (geom_bar). 11.4 Multiple histograms 20) To plot multiple histogram, we need two or more samples or populations with measurements on the same variable. For this example, we will use the crabs database of the package MASS. We will compare the two species based on their CL carapace length (mm). ## check is MASS is loaded and get &#39;crabs&#39; database require(MASS) data(crabs) crabs_data &lt;- crabs str(crabs_data) #&#39;data.frame&#39;: 200 obs. of 8 variables: # $ sp : Factor w/ 2 levels &quot;B&quot;,&quot;O&quot;: 1 1 1 1 1 1 1 1 1 1 ... # $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ... # $ index: int 1 2 3 4 5 6 7 8 9 10 ... # $ FL : num 8.1 8.8 9.2 9.6 9.8 10.8 11.1 11.6 11.8 11.8 ... # $ RW : num 6.7 7.7 7.8 7.9 8 9 9.9 9.1 9.6 10.5 ... # $ CL : num 16.1 18.1 19 20.1 20.3 23 23.8 24.5 24.2 25.2 ... # $ CW : num 19 20.8 22.4 23.1 23 26.5 27.1 28.4 27.8 29.3 ... # $ BD : num 7 7.4 7.7 8.2 8.2 9.8 9.8 10.4 9.7 10.3 ... We can plot overlaid histograms with its x-axis as x = CL and grouped by species as fill = sp and using geom_histogram(). You can control the width of the bins of the histogram with the argument binwidth and we selected 2.5. ## Overlaid histograms crabs_sp_CL_plot &lt;- ggplot(crabs_data, aes(x=CL, fill=sp)) + geom_histogram(binwidth=1, alpha=.4, position=&quot;identity&quot;) + ggtitle(&quot;Crab species CL overlap -- histograms&quot;) + theme_minimal() crabs_sp_CL_plot You can add a line to indicate the mean of each species, but you have to determine such values in a separated data frame. We use function split() and indicate that is by species with crabs_data_CL$sp. This will create a list with two data frames each one per species. ## get mean per species crabs_data_CL &lt;- subset(crabs_data, select = c(sp, CL)) crabs_data_CL_sp_list &lt;-split(crabs_data_CL, crabs_data_CL$sp) str(crabs_data_CL_sp_list) #List of 2 # $ B:&#39;data.frame&#39;: 100 obs. of 2 variables: # ..$ sp: Factor w/ 2 levels &quot;B&quot;,&quot;O&quot;: 1 1 1 1 1 1 1 1 1 1 ... # ..$ CL: num [1:100] 16.1 18.1 19 20.1 20.3 23 23.8 24.5 24.2 25.2 ... # $ O:&#39;data.frame&#39;: 100 obs. of 2 variables: # ..$ sp: Factor w/ 2 levels &quot;B&quot;,&quot;O&quot;: 2 2 2 2 2 2 2 2 2 2 ... # ..$ CL: num [1:100] 16.7 20.2 20.7 22.7 23.2 24.2 26 27.1 26.6 27.5 ... Here are two forms that we get the mean of each species. ## first form means_out_list &lt;- list() for(i in 1:length(crabs_data_CL_sp_list)) { one_species_name &lt;- names(crabs_data_CL_sp_list[i]) one_species_mean &lt;- mean(crabs_data_CL_sp_list[[i]][,2], na.rm = TRUE) one_sp_out &lt;- data.frame(sp = one_species_name, sp_mean = one_species_mean, stringsAsFactors = FALSE) means_out_list[[i]] &lt;- one_sp_out } means_out_df &lt;- do.call(rbind, means_out_list) means_out_df # sp sp_mean #1 B 30.058 #2 O 34.153 ## second form crab_removed_sp_column_list &lt;- lapply(crabs_data_CL_sp_list, function(x) { x[,1] &lt;- NULL; x }) crab_sp_means &lt;- lapply(crab_removed_sp_column_list, colMeans, na.rm = TRUE) means_out_df &lt;- do.call(rbind, crab_sp_means) means_out_df &lt;- as.data.frame(means_out_df) means_out_df$sp &lt;- rownames(means_out_df) names(means_out_df) &lt;- c(&quot;sp_mean&quot;, &quot;sp&quot;) means_out_df # sp_mean sp #B 30.058 B #O 34.153 O We can add a line to indicate the location of the mean of the histogram of each species with geom_vline(). ## add mean line crabs_sp_CL_plot &lt;- crabs_sp_CL_plot + geom_vline(data=means_out_df, aes(xintercept=sp_mean, colour=sp), linetype=&quot;dashed&quot;, size=0.5) crabs_sp_CL_plot 11.5 Density plots 21) These graphics are similar to histograms, but they are called geom_density(). We will use the same crab dataset to illustrate this example. ## check is MASS is loaded and get &#39;crabs&#39; database require(MASS) data(crabs) crabs_data &lt;- crabs str(crabs_data) #&#39;data.frame&#39;: 200 obs. of 8 variables: # $ sp : Factor w/ 2 levels &quot;B&quot;,&quot;O&quot;: 1 1 1 1 1 1 1 1 1 1 ... # $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ... # $ index: int 1 2 3 4 5 6 7 8 9 10 ... # $ FL : num 8.1 8.8 9.2 9.6 9.8 10.8 11.1 11.6 11.8 11.8 ... # $ RW : num 6.7 7.7 7.8 7.9 8 9 9.9 9.1 9.6 10.5 ... # $ CL : num 16.1 18.1 19 20.1 20.3 23 23.8 24.5 24.2 25.2 ... # $ CW : num 19 20.8 22.4 23.1 23 26.5 27.1 28.4 27.8 29.3 ... # $ BD : num 7 7.4 7.7 8.2 8.2 9.8 9.8 10.4 9.7 10.3 ... We can plot overlaid densities with its x-axis as x = CL and grouped by species as fill = sp. ## Overlaid densities crabs_sp_CL_plot_den &lt;- ggplot(crabs_data, aes(x=CL, fill=sp)) + geom_density(alpha=.3) + ggtitle(&quot;Crab species CL overlap -- density plots&quot;) + theme_minimal() crabs_sp_CL_plot_den 22) The flexibility of ggplot2 can be evidenced by combining histograms and density plots with the mean values. crabs_sp_CL_plot_his_den &lt;- ggplot(crabs_data, aes(x=CL, fill=sp)) + geom_histogram(aes(y=..density.., fill=sp), binwidth = 1, alpha = .6, colour = &quot;black&quot;, position = &quot;identity&quot;) + geom_density(mapping = aes(x=CL, fill=sp), alpha = .4, position = &quot;identity&quot;) + ggtitle(&quot;Crab species CL overlap -- histograms and density plots&quot;) + theme_minimal() # change some colors crabs_sp_CL_plot_his_den &lt;- crabs_sp_CL_plot_his_den + scale_color_brewer(palette=&quot;Set3&quot;)+ scale_fill_brewer(palette=&quot;Set3&quot;) crabs_sp_CL_plot_his_den 11.6 Box plots These graphics are similar to histograms and density plots that summarizes a distribution. In these plots, the graphics show the following summaries: (1) First quartile (Q1) or 25th Percentile, which is the middle value between the smallest value and the median of the dataset; (2) Median (Q2) or 50th Percentile, which is the middle value of the dataset; (3) Third quartile (Q3) or 75th Percentile, which is the middle value between the median and the highest value of the dataset; (4) A “box” named the interquartile range (IQR) that is bounded between the 25th and 75th percentiles; (5) Line or whisker that extend to a “minimum” equal to Q1 - 1.5xIQR, (6) Line or whisker that extend to a “maximum” equal to Q3 + 1.5xIQR; and (7) Outliers or values that fall below the “minimum” and above the “maximum.” Source: Understanding Boxplots by Michael Galarnyk. 23) We can use the same crabs dataset to revise how to construct boxplots in ggplot2. ## check is MASS is loaded and get &#39;crabs&#39; database require(MASS) data(crabs) crabs_data &lt;- crabs str(crabs_data) #&#39;data.frame&#39;: 200 obs. of 8 variables: # $ sp : Factor w/ 2 levels &quot;B&quot;,&quot;O&quot;: 1 1 1 1 1 1 1 1 1 1 ... # $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ... # $ index: int 1 2 3 4 5 6 7 8 9 10 ... # $ FL : num 8.1 8.8 9.2 9.6 9.8 10.8 11.1 11.6 11.8 11.8 ... # $ RW : num 6.7 7.7 7.8 7.9 8 9 9.9 9.1 9.6 10.5 ... # $ CL : num 16.1 18.1 19 20.1 20.3 23 23.8 24.5 24.2 25.2 ... # $ CW : num 19 20.8 22.4 23.1 23 26.5 27.1 28.4 27.8 29.3 ... # $ BD : num 7 7.4 7.7 8.2 8.2 9.8 9.8 10.4 9.7 10.3 ... We can present boxplots with its x-axis by species x = sp, y-axis by the CL measurement y = CL and box color as fill = sp. ## check ig ggplot2 is loaded require(ggplot2) ## Boxplots with standard color without legend -- guides(fill=FALSE) crabs_sp_CL_boxplot &lt;- ggplot(crabs_data, aes(x=sp, y=CL, fill=sp)) + geom_boxplot(width=0.3) + labs(title=&quot;Crab species CL overlap -- boxplots&quot;, x=&quot;Species&quot;, y=&quot;CL - carapace length (mm)&quot;) + guides(fill=FALSE) + theme_minimal() crabs_sp_CL_boxplot_v1 &lt;- crabs_sp_CL_boxplot + scale_fill_manual(values =c(&quot;#C7A76C&quot;, &quot;#99B56B&quot;)) crabs_sp_CL_boxplot_v1 You can flip coordinates with coord_flip() and changed the colors of the plots using scale_fill_manual(). ## flip coordinates crabs_sp_CL_boxplot_v2 &lt;- crabs_sp_CL_boxplot + coord_flip() + scale_fill_manual(values =c(&quot;#8C241B&quot;, &quot;#29809E&quot;)) crabs_sp_CL_boxplot_v2 11.7 Violin plots 24) We can use again the crabs dataset for violin plots containing its corresponding boxplot and mean value marker. Notice that we can stack different plots on a ggplot object. For violin plots, we selected a white color with fill= \"white\" and thinner silhouette with width = 0.5. For box plots, we selected a width = 0.3 and also added a diamond symbol for the mean value with stat_summar(). ## check is MASS is loaded and get &#39;crabs&#39; database require(MASS) data(crabs) crabs_data &lt;- crabs str(crabs_data) #&#39;data.frame&#39;: 200 obs. of 8 variables: # $ sp : Factor w/ 2 levels &quot;B&quot;,&quot;O&quot;: 1 1 1 1 1 1 1 1 1 1 ... # $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ... # $ index: int 1 2 3 4 5 6 7 8 9 10 ... # $ FL : num 8.1 8.8 9.2 9.6 9.8 10.8 11.1 11.6 11.8 11.8 ... # $ RW : num 6.7 7.7 7.8 7.9 8 9 9.9 9.1 9.6 10.5 ... # $ CL : num 16.1 18.1 19 20.1 20.3 23 23.8 24.5 24.2 25.2 ... # $ CW : num 19 20.8 22.4 23.1 23 26.5 27.1 28.4 27.8 29.3 ... # $ BD : num 7 7.4 7.7 8.2 8.2 9.8 9.8 10.4 9.7 10.3 ... ## violin and boxplots with mean indicated crabs_sp_CL_violin &lt;- ggplot(crabs_data, aes(x=sp, y=CL, fill=sp)) + geom_violin(trim=FALSE, width=0.5, fill= &quot;white&quot;, color = &quot;black&quot;) + geom_boxplot(width=0.3) + stat_summary(fun = mean, geom=&quot;point&quot;, shape=5, size=4) + labs(title=&quot;Crab species CL overlap -- violin and box plots&quot;, x=&quot;Species&quot;, y=&quot;CL - carapace length (mm)&quot;) + guides(fill=FALSE) + scale_fill_manual(values =c(&quot;#8C241B&quot;, &quot;#29809E&quot;)) + theme_minimal() crabs_sp_CL_violin 11.8 Scatterplots These are graphics where the relationship of two variables presented in cartesian (x versus y) coordinates to display values. In ggplot2, such data points are presented with geom_point() and, in most cases, tendency and regression lines are added with geom_smooth(). As in the case of choices of colors, there are several types of point shapes (markers) and lines. These can be explored with options of the R package ggpubr. This package provides some easy-to-use functions for creating and customizing ggplot2- based publication ready plots. ## intall ggpubr install.packages(&quot;ggpubr&quot;) library(ggpubr) ## we can select from these types of point shapes; the number indicates the shape code. show_point_shapes()+theme_minimal()+labs(y=&quot;&quot;)+theme(axis.text.x = element_blank(), axis.text.y = element_blank()) We can control the line type with geom_line() and its argument linetype. Lines can also be modified manually with scale_linetype_manual() and it argument values. ## we can select from these types of lines, the number indicates the line code. show_line_types()+theme_minimal() Notice that you can also determine the type of line with argument lty that can be used in special type of ggplot function like geom_smooth. These codes are lty = 1 for “solid,” lty = 2 “dashed,” lty = 3 “dotted,” lty = 4 “dotdash,” lty = 5 “longdash” and lty = 6 “twodash.” 25) We will use the oribatid dataset from the R package ade4. This data set contains information about environmental control and spatial structure in ecological communities of Oribatid mites. The environmental variables include substrate a factor with seven levels that describes the nature of the substratum, shrubs a factor with three levels that describes the absence/presence of shrubs, topo a factor with two levels that describes the microtopography, density substratum density (g.L−1) and water content of the substratum (g.L−1). We will merge in a single data frame the environmental data and fauna information of these 70 sites to explore several types of scatterplots. We can prepare oribatid dataset for the scatterplot. ## check is ade4 is loaded and get &#39;oribatid&#39; database require(ade4) data(oribatid) ## environmental and genetic data envir_data &lt;- oribatid$envir fauna_data &lt;- oribatid$fau ## total number of oribatid mites fauna_data$total_mites &lt;- rowSums(fauna_data) ## we add a common column in both data frames, in this case the &#39;site&#39; envir_data$site &lt;- rownames(envir_data) fauna_data$site &lt;- rownames(fauna_data) ## merge both data frames using &#39;site&#39; oribatid_data &lt;- merge(envir_data, fauna_data, by =&quot;site&quot;) head(oribatid_data) # you can explore this data frame content str(oribatid_data) #&#39;data.frame&#39;: 70 obs. of 41 variables: # $ site : chr &quot;1&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; ... # $ substrate : Factor w/ 7 levels &quot;inter&quot;,&quot;litter&quot;,..: 4 4 6 4 4 1 1 1 1 4 ... # $ shrubs : Factor w/ 3 levels &quot;few&quot;,&quot;many&quot;,&quot;none&quot;: 1 2 2 1 2 2 2 1 2 2 ... # $ topo : Factor w/ 2 levels &quot;blanket&quot;,&quot;hummock&quot;: 2 2 1 2 2 1 1 2 2 2 ... # $ density : num 39.2 32.1 35.6 46.8 28 ... # $ water : num 350 221 134 406 244 ... # $ Brachy : int 17 22 36 28 3 41 6 7 9 19 ... # $ PHTH : int 5 4 7 2 2 5 0 2 0 3 ... # $ HPAV : int 5 5 35 12 4 12 6 3 1 7 ... # $ RARD : int 3 3 9 13 12 0 0 2 2 0 ... # $ SSTR : int 2 0 0 0 0 2 0 0 0 0 ... # $ Protopl : int 1 0 2 0 0 0 0 0 0 0 ... # ... We can explore the relationship between density and total_mites with a simple scatterplot. ## make sure that ggplot2 is loaded require(ggplot2) ## basic scatterplot my_scatterplot &lt;- ggplot(data = oribatid_data, aes(x = water, y = total_mites)) + geom_point() + labs(title=&quot;Oribatid data -- simple scatterplot&quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + theme_minimal() my_scatterplot We see and outlier, so we can label the point by the sites that each point represents by adding argument label and geom_text ## labeled scatterplot my_scatterplot_l &lt;- ggplot(data = oribatid_data, aes(x = water, y = total_mites, label=site)) + geom_point() + labs(title=&quot;Oribatid data -- simple scatterplot labeled&quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + geom_text(aes(label=site),hjust=0, vjust=-0.5) + theme_minimal() my_scatterplot_l We identify that point or site 67 as an outlier. We can determine why this site is an outlier, yet we can exclude it in our dataset. ## search the site &quot;67&quot; oribatid_data[oribatid_data$site == &quot;67&quot;, ] # site substrate shrubs topo density water Brachy PHTH HPAV RARD SSTR Protopl MEGR MPRO TVIE HMIN HMIN2 NPRA TVEL ONOV SUCT LCIL #64 67 sph1 none blanket 52.123 826.958 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 723 # Oribatul1 Ceratoz1 PWIL Galumna1 Steganacarus2 HRUF Trhypochth1 PPEL NCOR SLAT FSET Lepidozetes Eupelops Minigalumna LRUG PLAG2 Ceratoz3 #64 0 0 0 0 0 0 7 0 0 0 0 0 0 0 11 0 0 # Oppia.minus Trimalaco2 total_mites #64 0 33 781 ## this site has an unusual high numbers of LCIL mites. We can delete or remove this row by its number indicated by 64 at the row number oribatid_data_2 &lt;- oribatid_data[-64,] ## labeled scatterplot my_scatterplot_l2 &lt;- ggplot(data = oribatid_data_2, aes(x = water, y = total_mites, label=site)) + geom_point() + labs(title=&quot;Oribatid data -- simple scatterplot labeled&quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + geom_text(aes(label=site),hjust=0, vjust=-0.5) + theme_minimal() my_scatterplot_l2 26) We can modify this scatterplot with different shapes, colors and sizes. These increase the amount of information provided by the plot. We can change the shape and color of the points in the scatterplot. ## load color library &#39;ggsci&#39; library(ggsci) ## scatterplot with different shape and color palette my_scatterplot_l3 &lt;- ggplot(data = oribatid_data_2, aes(x = water, y = total_mites, label=site)) + geom_point(aes(color = substrate), alpha = 0.9, size = 5, shape = 18) + labs(title=&quot;Oribatid data -- scatterplot by substrate -- jco palette &quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + scale_color_jco() + theme_minimal() my_scatterplot_l3 We can change the size of points in the scatterplot based on another continuous variable. For example, number of LCIL mite species. ## scatterplot with size of points by LCIL numbers my_scatterplot_l4 &lt;- ggplot(data = oribatid_data_2, aes(x = water, y = total_mites, label=site)) + geom_point(aes(color = substrate, size = LCIL), alpha = 0.9, shape = 18) + labs(title=&quot;Oribatid data -- scatterplot by substrate and size by LCIL mite species -- npg palette &quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + scale_color_npg() + theme_minimal() my_scatterplot_l4 27) We might be interested in separating a group from the rest of data points in this scatterplot. We will try the sph1 different from the others. We can change manually the points colors with scale_colour_manual() and shapes with scale_colour_manual(). ## create a vector for manual colors bases on substrate names_substrates &lt;- as.character(unique(oribatid_data_2$substrate)) names_substrates #[1] &quot;sph1&quot; &quot;sph3&quot; &quot;inter&quot; &quot;litter&quot; &quot;sph4&quot; &quot;sph2&quot; &quot;peat&quot; ## create a vector for colors by substrate type, but binary colors_substrate &lt;- ifelse(names_substrates == &quot;sph1&quot;, &quot;firebrick1&quot;, &quot;darkgrey&quot;) names(colors_substrate) &lt;- names_substrates colors_substrate # sph1 sph3 inter litter sph4 sph2 peat # &quot;darkred&quot; &quot;darkgrey&quot; &quot;darkgrey&quot; &quot;darkgrey&quot; &quot;darkgrey&quot; &quot;darkgrey&quot; &quot;darkgrey&quot; ## scatterplot for &#39;sph1&#39; substrate only color my_scatterplot_l4 &lt;- ggplot(data = oribatid_data_2, aes(x = water, y = total_mites)) + geom_point(aes(color = substrate), alpha = 0.7, shape = 18, size = 4) + labs(title=&quot;Oribatid data -- scatterplot with emphasis on &#39;sph1&#39; substrate &quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + scale_colour_manual(values = colors_substrate) + theme_minimal() my_scatterplot_l4 ## create a vector for shapes by substrate type, but binary shape_substrate &lt;- ifelse(names_substrates == &quot;sph1&quot;, 16, 18) names(shape_substrate) &lt;- names_substrates shape_substrate # sph1 sph3 inter litter sph4 sph2 peat # 16 18 18 18 18 18 18 ## scatterplot for &#39;sph1&#39; substrate with color and shape my_scatterplot_l5 &lt;- ggplot(data = oribatid_data_2, aes(x = water, y = total_mites)) + geom_point(aes(color = substrate , shape = substrate), alpha = 0.7, size = 4) + labs(title=&quot;Oribatid data -- scatterplot with emphasis on &#39;sph1&#39; substrate &quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + scale_colour_manual(values = colors_substrate) + scale_shape_manual(values = shape_substrate) + theme_minimal() my_scatterplot_l5 11.9 Lines and Scatterplots In most cases, you would be interested on plotting trends on your scatterplots and these can be easily added with functions like geom_line() and geom_sooth(). 28) For lines, we will use again the oribatid dataset. As mentioned, this data set contains information about environment and diversity in communities of Oribatid mites. We can prepare oribatid dataset for the scatterplot. ## check is ade4 is loaded and get &#39;oribatid&#39; database require(ade4) data(oribatid) ## enviromental and genetic data envir_data &lt;- oribatid$envir fauna_data &lt;- oribatid$fau ## total number of oribatid mites fauna_data$total_mites &lt;- rowSums(fauna_data) ## we add a common column in both data frames, in this case the &#39;site&#39; envir_data$site &lt;- rownames(envir_data) fauna_data$site &lt;- rownames(fauna_data) ## merge both data frames using &#39;site&#39; oribatid_data &lt;- merge(envir_data, fauna_data, by =&quot;site&quot;) head(oribatid_data) # you can explore this data frame content str(oribatid_data) #&#39;data.frame&#39;: 70 obs. of 41 variables: # $ site : chr &quot;1&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; ... # $ substrate : Factor w/ 7 levels &quot;inter&quot;,&quot;litter&quot;,..: 4 4 6 4 4 1 1 1 1 4 ... # $ shrubs : Factor w/ 3 levels &quot;few&quot;,&quot;many&quot;,&quot;none&quot;: 1 2 2 1 2 2 2 1 2 2 ... # $ topo : Factor w/ 2 levels &quot;blanket&quot;,&quot;hummock&quot;: 2 2 1 2 2 1 1 2 2 2 ... # $ density : num 39.2 32.1 35.6 46.8 28 ... # $ water : num 350 221 134 406 244 ... # $ Brachy : int 17 22 36 28 3 41 6 7 9 19 ... # $ PHTH : int 5 4 7 2 2 5 0 2 0 3 ... # $ HPAV : int 5 5 35 12 4 12 6 3 1 7 ... # $ RARD : int 3 3 9 13 12 0 0 2 2 0 ... # $ SSTR : int 2 0 0 0 0 2 0 0 0 0 ... # $ Protopl : int 1 0 2 0 0 0 0 0 0 0 ... # ... ## as mentioned before, we will remove unusual site 67 oribatid_data_2 &lt;- oribatid_data[-64,] We can explore the relationship between water and total_mites by shrubs groups with geom_line(). Notice that you can change groups based on the argument group. ## make sure that ggplot2 is loaded require(ggplot2) ## add a line for each shrubs group substrate_plots &lt;- ggplot(data = oribatid_data_2, aes(x = water, y = total_mites, group = shrubs)) + geom_line(aes(color = shrubs))+ geom_point(aes(color = shrubs), alpha = 0.7, size = 4) + labs(title=&quot;Oribatid data -- scatterplot with lines by shrub density&quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + theme_minimal() substrate_plots You can also change the type of line and color manually as long as you provide a vector with the same number of groups using scale_linetype_manual() and scale_color_manual(). For our example, group = shrubs has to have vector of 3 elements. ## change line type manually for each shrubs group substrate_plots &lt;- ggplot(data = oribatid_data_2, aes(x = water, y = total_mites, group = shrubs)) + geom_line(aes(color = shrubs, linetype = shrubs))+ geom_point(aes(color = shrubs), alpha = 0.7, size = 4) + labs(title=&quot;Oribatid data -- scatterplot with lines by shrub density&quot;, x=&quot;water content of the substratum (g/L)&quot;, y=&quot;total mites&quot;) + theme_minimal() substrate_plots &lt;- substrate_plots + scale_linetype_manual(values=c(&quot;solid&quot;, &quot;dashed&quot;, &quot;solid&quot;)) substrate_plots 29) We an explore a line and adding a confidence region to a line. For this example, we will use the NYC Central Park temperature record from here. This nyc_temps.txt dataset is also in our class GitHub repository. ## NOTE: remember to update the path to file with the dataset where you downloaded in your computer -- THIS IS EXCLUSIVE TO YOUR COMPUTER AND IT IS NOT THE PATH SHOWN BELOW ## load get &#39;nyc_temps.txt&#39; dataset nyc_temps &lt;- read.table(&quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/ref_files_pdfs/nyc_temps.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, stringsAsFactors = FALSE) head(nyc_temps) # YEAR JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC ANNUAL #1 1869 35.1 34.5 34.8 49.2 57.7 69.3 72.8 71.8 65.6 50.9 40.3 34.7 51.4 #2 1870 37.5 31.3 34.1 50.7 60.9 72.9 76.6 75.3 67.6 56.7 45.5 34.1 53.6 #3 1871 28.3 30.2 44.2 52.0 60.4 68.2 72.3 73.6 60.8 55.6 38.8 29.2 51.1 #4 1872 28.8 29.9 30.5 49.4 61.5 71.2 77.5 75.6 66.4 53.2 41.0 26.7 51.0 #5 1873 28.6 29.5 35.7 46.7 58.8 70.3 75.4 72.0 65.4 55.8 37.0 36.5 51.0 #6 1874 34.2 31.3 37.1 41.1 58.8 70.1 73.9 70.3 67.0 55.1 43.4 33.8 51.3 str(nyc_temps) #&#39;data.frame&#39;: 151 obs. of 14 variables: # $ YEAR : int 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 ... # $ JAN : num 35.1 37.5 28.3 28.8 28.6 34.2 23.8 36.6 27.7 30.3 ... # $ FEB : num 34.5 31.3 30.2 29.9 29.5 31.3 25.2 31.8 37 32.2 ... # $ MAR : num 34.8 34.1 44.2 30.5 35.7 37.1 34.1 34.4 35.8 44.1 ... # $ APR : num 49.2 50.7 52 49.4 46.7 41.1 43.1 47 47.7 53.3 ... # $ MAY : num 57.7 60.9 60.4 61.5 58.8 58.8 60.1 60.2 59.6 59.4 ... # $ JUN : num 69.3 72.9 68.2 71.2 70.3 70.1 69.2 73.5 70.2 67.7 ... # $ JUL : num 72.8 76.6 72.3 77.5 75.4 73.9 74 79.4 75 77.8 ... # $ AUG : num 71.8 75.3 73.6 75.6 72 70.3 72.9 75.2 75.4 74.2 ... # $ SEP : num 65.6 67.6 60.8 66.4 65.4 67 64 63.7 66.9 68.3 ... # $ OCT : num 50.9 56.7 55.6 53.2 55.8 55.1 53.6 50.6 55.8 58.7 ... # $ NOV : num 40.3 45.5 38.8 41 37 43.4 39.3 45.2 44.5 43.8 ... # $ DEC : num 34.7 34.1 29.2 26.7 36.5 33.8 33.9 24.9 37.4 32.8 ... # $ ANNUAL: num 51.4 53.6 51.1 51 51 51.3 49.4 51.9 52.8 53.6 ... ## we need to create a confidence level by determining the max and min temperature values per year ## NOTICE that we need to exclude the YEAR and ANNUAL columns ## max nyc_temps[, &quot;max_temp&quot;] &lt;- apply(nyc_temps[, 2:13], 1, max) ## min nyc_temps[, &quot;min_temp&quot;] &lt;- apply(nyc_temps[, 2:13], 1, min) ## standard deviation nyc_temps[, &quot;sd_temp&quot;] &lt;- apply(nyc_temps[, 2:13], 1, sd) head(nyc_temps) # YEAR JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC ANNUAL max_temp min_temp sd_temp #1 1869 35.1 34.5 34.8 49.2 57.7 69.3 72.8 71.8 65.6 50.9 40.3 34.7 51.4 72.8 34.5 15.57944 #2 1870 37.5 31.3 34.1 50.7 60.9 72.9 76.6 75.3 67.6 56.7 45.5 34.1 53.6 76.6 31.3 17.11538 #3 1871 28.3 30.2 44.2 52.0 60.4 68.2 72.3 73.6 60.8 55.6 38.8 29.2 51.1 73.6 28.3 16.74647 #4 1872 28.8 29.9 30.5 49.4 61.5 71.2 77.5 75.6 66.4 53.2 41.0 26.7 51.0 77.5 26.7 19.35093 #5 1873 28.6 29.5 35.7 46.7 58.8 70.3 75.4 72.0 65.4 55.8 37.0 36.5 51.0 75.4 28.6 17.38056 #6 1874 34.2 31.3 37.1 41.1 58.8 70.1 73.9 70.3 67.0 55.1 43.4 33.8 51.3 73.9 31.3 16.26236 We now can plot the temperature trends in NYC until 2019. For the confidence plot, we use the function geom_ribbon(). ## NYC temperatures plot nyc_temp_plot &lt;- ggplot(nyc_temps, aes(x = YEAR, y = ANNUAL)) + geom_ribbon(aes(ymin = ANNUAL - sd_temp, ymax = ANNUAL + sd_temp), alpha = 0.2) + geom_line(aes(x = YEAR, y = max_temp), colour = &quot;red&quot;, linetype = &quot;dotted&quot;) + geom_line(aes(x = YEAR, y = min_temp), colour = &quot;blue&quot;, linetype = &quot;dotted&quot;) + geom_line() + labs(title=&quot;NYC temperature Central Park&quot;, x=&quot;Year&quot;, y=&quot;Temp (F)&quot;) + ylim(0, 90) + theme_minimal() nyc_temp_plot 30) We can explore by fitting a line to the scatterplot using geom_smooth() to our NYC climate data. ## NYC temperatures plot and fit line loess nyc_temp_plot_smooth &lt;- ggplot(nyc_temps, aes(x = YEAR, y = ANNUAL)) + geom_point(alpha = 0.7, size = 4) + geom_smooth() + labs(title=&quot;NYC temperature Central Park&quot;, x=&quot;Year&quot;, y=&quot;Temp (F)&quot;) + ylim(35, 65) + theme_minimal() nyc_temp_plot_smooth #`geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Notice the upward trend in the temperature. We can plot using a curve with other methods like lm. ## NYC temperatures plot and fit line with lm nyc_temp_plot_smooth_lm &lt;- ggplot(nyc_temps, aes(x = YEAR, y = ANNUAL)) + geom_point(alpha = 0.7, size = 4) + geom_smooth(method=lm, se=FALSE) + labs(title=&quot;NYC temperature Central Park - lm line and without se&quot;, x=&quot;Year&quot;, y=&quot;Temp (F)&quot;) + ylim(35, 65) + theme_minimal() ## I removed the confidence line change by setting in geom_smooth() --&gt; se=FALSE nyc_temp_plot_smooth_lm #`geom_smooth()` using formula &#39;y ~ x&#39; 31) We can also plot multiple lines when we provide groupings. We need to prepare our data. ## we need to prepare our data nyc_average &lt;- subset(nyc_temps, select = c(&quot;YEAR&quot;, &quot;ANNUAL&quot;)) names(nyc_average) &lt;- c(&quot;YEAR&quot;, &quot;TEMP&quot;) nyc_average$group &lt;- &quot;average&quot; nyc_max &lt;- subset(nyc_temps, select = c(&quot;YEAR&quot;, &quot;max_temp&quot;)) names(nyc_max) &lt;- c(&quot;YEAR&quot;, &quot;TEMP&quot;) nyc_max$group &lt;- &quot;max&quot; nyc_min &lt;- subset(nyc_temps, select = c(&quot;YEAR&quot;, &quot;min_temp&quot;)) names(nyc_min) &lt;- c(&quot;YEAR&quot;, &quot;TEMP&quot;) nyc_min$group &lt;- &quot;min&quot; ## all data together nyc_all &lt;- do.call(rbind, list(nyc_average,nyc_max, nyc_min)) nyc_all$group2 &lt;- nyc_all$group str(nyc_all) #&#39;data.frame&#39;: 453 obs. of 4 variables: # $ YEAR : int 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 ... # $ TEMP : num 51.4 53.6 51.1 51 51 51.3 49.4 51.9 52.8 53.6 ... # $ group : chr &quot;average&quot; &quot;average&quot; &quot;average&quot; &quot;average&quot; ... # $ group2: chr &quot;average&quot; &quot;average&quot; &quot;average&quot; &quot;average&quot; ... We now have the data ready and now we can plot the trends on average, maximum and minimum temperatures in NYC. ## NYC temperatures plot and fit line loess nyc_temp_plot_smooth_3 &lt;- ggplot(nyc_all, aes(x = YEAR, y = TEMP, color = group)) + geom_point(aes(color = group), alpha = 0.7, size = 4) + geom_smooth(aes(color = group2), alpha = 0.5) + labs(title=&quot;NYC temperature Central Park&quot;, x=&quot;Year&quot;, y=&quot;Temp (F)&quot;) + ylim(15, 90) + theme_minimal() nyc_temp_plot_smooth_3 #`geom_smooth()` using formula &#39;y ~ x&#39; A more elaborated plot of the same temperature trends. ## NYC temperatures plot and fit line loess color_points &lt;- c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;) color_lines &lt;- c(&quot;black&quot;,&quot;black&quot;,&quot;black&quot;) nyc_temp_plot_smooth_4 &lt;- ggplot(nyc_all, aes(x = YEAR, y = TEMP)) + geom_point(aes(fill=factor(group)),size=4, shape=21, stroke=0) + geom_smooth(aes(color = group2), alpha = 0.5) + labs(title=&quot;NYC temperature Central Park&quot;, x=&quot;Year&quot;, y=&quot;Temp (F)&quot;) + scale_fill_manual(values=color_points) + scale_colour_manual(values=color_lines) + ylim(15, 90) + theme_minimal() nyc_temp_plot_smooth_4 #`geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 11.10 Multiple graphs at once with cowplot In most cases, you might want multiple plots at once before we print our graphics. We will use the R package cowplot that provides various features that help with creating publication-quality figures with ‘ggplot2,’ such as a set of themes, functions to align plots and arrange them into complex compound figures. This R package has a vignette. 32) We need to install cowplot and you need two or more ggplot2 objects to make use of the features of this package. For this purpose, we will create four plots using the dataset fruits of the R package ade4. ## install cowplot install.packages(&quot;cowplot&quot;) library(cowplot) ## make sure that ggplot2 is loaded require(ggplot2) ## “fruits” is dataset about batches of fruits -two types- are judged by two different ways. ## They are classified in order of preference by 16 individuals (J1-J16) library(ade4) data(fruits) fruits_dataset &lt;- fruits jug &lt;- fruits_dataset$jug jug$fruit &lt;- as.character(fruits_dataset$type) rownames(jug) &lt;- NULL head(jug) # J1 J2 J3 J4 J5 J6 J7 J8 J9 J10 J11 J12 J13 J14 J15 J16 fruit #1 10 5 8 3 1 18 5 17 3 4 1 2 5 3 1 1 necta #2 3 1 9 8 6 16 8 10 2 1 8 8 9 5 6 4 necta #3 5 11 5 2 8 8 18 3 4 15 14 4 7 1 3 13 peche #4 6 12 3 4 4 7 17 2 1 16 13 7 3 8 4 14 peche #5 4 2 4 14 17 10 16 1 5 19 21 13 6 2 5 15 peche #6 2 6 16 10 13 2 11 5 13 8 10 14 10 9 15 8 necta ## first plot -- individual J1 versus J2 plot_J1_vs_J2 &lt;- ggplot(jug, aes(x = J1, y = J2, color = fruit)) + geom_point() + labs(title=&quot;J1 versus J2 on fruits&quot;) + theme_minimal() plot_J1_vs_J2 ## second plot -- individual J1 versus J3 plot_J1_vs_J3 &lt;- ggplot(jug, aes(x = J1, y = J3, color = fruit)) + geom_point() + labs(title=&quot;J1 versus J3 on fruits&quot;) + theme_minimal() plot_J1_vs_J3 ## load reshape2 to make more amenable library(reshape2) jug_reshape &lt;- melt(jug) names(jug_reshape) &lt;- c(&quot;fruit&quot;, &quot;judge&quot;, &quot;score&quot;) head(jug_reshape) # fruit judge score #1 necta J1 10 #2 necta J1 3 #3 peche J1 5 #4 peche J1 6 #5 peche J1 4 #6 necta J1 2 ## third plot -- all judges plot_all_judges &lt;-ggplot(jug_reshape, aes(x = judge, y = score, color = fruit)) + geom_boxplot() + labs(title=&quot;all judges&quot;) + theme_minimal() plot_all_judges ## fourth plot -- all fruits plot_all_fruits &lt;-ggplot(jug_reshape, aes(x = fruit, y = score, color = judge)) + geom_boxplot() + labs(title=&quot;fruits&quot;) + theme_minimal() plot_all_fruits 32) With this four plots, we can make use of several of the options of cowplot. We can arrange two plots side by side in a row using the function plot_grid(). We will label them A and B. plot_grid(plot_J1_vs_J2, plot_J1_vs_J3, labels = c(&#39;A&#39;, &#39;B&#39;), label_size = 12) We can arrange two plots one on top of the other with plot_grid() and the arguments for one column ncol = 1 and align them vertically align = \"v\". plot_all_fruits &lt;- plot_all_fruits + theme(legend.position = &quot;none&quot;) plot_grid(plot_all_judges, plot_all_fruits, ncol = 1, align = &quot;v&quot;) All four plots in a single one. ## combine all into one plot all_four_plots &lt;- plot_grid(plot_J1_vs_J2, plot_J1_vs_J3, plot_all_judges, plot_all_fruits, rel_heights = c(.6, 1), labels = &quot;auto&quot;) all_four_plots 11.11 Save your plots as PDF or PNG 34) You can save your plots using the function ggsave(). ## save &#39;all_plot&#39; graph as pdf ggsave(&quot;all_plots.pdf&quot;) ## save &#39;all_plot&#39; graph as png ggsave(&quot;all_plots.png&quot;) ## save &#39;all_plot&#39; graph as png with some size defintions ggsave(&quot;all_plots2.png&quot;, width = 4, height = 4) ggsave(&quot;all_plots3.png&quot;, width = 20, height = 20, units = &quot;cm&quot;) Note that it has to be last plot you have in environment. 11.12 Other resources 33) Online resources: a) ggplot Wizardry is a nice page with some examples see here b) ggplot tutorial is another page from the same author see here c) tidytuesady is a page of datasets see here "],["sequence-alignments.html", "Session 12 – Sequence Alignments 12.1 Retrieving sequences using ‘rentrez’ 12.2 Retrieving sequences manually from NCBI 12.3 Retrieving accession numbers 12.4 Alignment with DECIPHER 12.5 Visualize alignment 12.6 Save alignment to file 12.7 Concatenate and align several markers 12.8 Concatenate with data.table 12.9 Concatenate with Biostrings 12.10 Visualizing alignments in R 12.11 Other sequence alignment software", " Session 12 – Sequence Alignments One central objective in many bioinformatics analyses is to understand the relationship between a set of biological sequences (i.e., DNA, RNA, proteins). To address this objective, we must organize these sequences in order to reflect their evolutionary, functional or structural relationships. Therefore, sequence aligning is just the insertion of gaps or spaces of varying length within the otherwise continuous sequences that we want to investigate. A set of diverse alignment programs achieve this by identifying homologous positions and add gaps or spaces in optimal sites to maximize the number of aligned homologous positions. In an evolutionary context, these gaps represent insertions and deletions (indels) within the genome that are hypothesized to have occurred during the evolutionary history of species or populations being studied. However, defining homologous sites is, at times complex, given the noise (i.e., non-homologous similarities in the sequences) resulting from convergent evolution. 12.1 Retrieving sequences using ‘rentrez’ For alignments, you must have a minimum of two sequences to perform an alignment. 1) We will retrieve some sequences from GenBank using the ‘rentrez’ package and we will restrict to one genera of poison frogs: Ameerega. You can try any group of organisms that you want. In our example, we can first retrieve by searching the genus. ## make sure that &#39;rentrez&#39; is loaded require(rentrez) ## Download some nucleotide sequences from NCBI for Ameerega [Organism] that correspond to the gene COI Ameerega_name_COX1 &lt;- &quot;Ameerega[Organism] AND COI[Gene]&quot; Ameerega_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term= Ameerega_name_COX1) str(Ameerega_seq_IDs) #List of 5 # $ ids : chr [1:20] &quot;1952638364&quot; &quot;1952638361&quot; &quot;1952638358&quot; &quot;1074806460&quot; ... # $ count : int 37 # $ retmax : int 20 # $ QueryTranslation: chr &quot;\\&quot;Ameerega\\&quot;[Organism] AND COI[Gene]&quot; # $ file :Classes &#39;XMLInternalDocument&#39;, &#39;XMLAbstractDocument&#39; &lt;externalptr&gt; # - attr(*, &quot;class&quot;)= chr [1:2] &quot;esearch&quot; &quot;list&quot; ## Note: the $ids indicate that 37 sequences exist in Ameerega for COX1 -- make sure to use Ameerega_seq_IDs$ids Ameerega_seqs_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=Ameerega_seq_IDs$ids, rettype=&quot;fasta&quot;) cat(Ameerega_seqs_fasta) 12.2 Retrieving sequences manually from NCBI 2) In some cases, you might have already the NCBI accession numbers and you can use ‘rentrez’ to retrieve such specific sequences. However, you might want to explore if sequences are associated to your group of interest. In this example, we will exemplify this by retrieving some outgroups of Ameerega frogs in the genera Leucostethus, Colostethus, Epipedobates and Silverstoneia. You can search those using a point-and-click approach as follows: Go to the NCBI&gt;Taxonomy. Click on Taxonomy. An type the family, genus or species of interest and click Search. Click on the taxon of interest (e.g., Leucostethus). You will get a list of species associated with this genus. For this example, we will click on Leucostethus fugax. You will get a list of sequences associated with this taxon. For this example, we want the nucleotide sequences and their accession numbers. You will click on 19 next to Nucleotide. For this example, we get the accession number MW042037.1 that is associated with Leucostethus fugax voucher QCAZ16513 cytochrome oxidase subunit I (COI) gene . 12.3 Retrieving accession numbers 3) You can use ‘rentrez’ to explore each genus for such specific sequences rather than the tedious option 2) as indicated above. We will retrieve sequences for the other outgroups of Ameerega: Colostethus, Epipedobates and Silverstoneia. ## sequences for Colostethus and the corresponding COI gene Colostethus_name_COX1 &lt;- &quot;Colostethus[Organism] AND COI[Gene]&quot; Colostethus_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term= Colostethus_name_COX1) Colostethus_seq_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=Colostethus_seq_IDs$ids, rettype=&quot;fasta&quot;) cat(Colostethus_seq_fasta) # for example, we will retrieve “KR862889.1 Colostethus pratti voucher CH 6816 cytochrome oxidase subunit 1 (COI) gene, partial cds; mitochondria” ## sequences for Epipedobates and the corresponding COI gene Epipedobates_name_COX1 &lt;- &quot;Epipedobates[Organism] AND COI[Gene]&quot; Epipedobates_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term= Epipedobates_name_COX1) Epipedobates_seq_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=Epipedobates_seq_IDs$ids, rettype=&quot;fasta&quot;) cat(Epipedobates_seq_fasta) # for example, we will retrieve “MW042036.1 Epipedobates machalilla voucher QCAZ16527 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial” ## sequences for Silverstoneia and the corresponding COI gene Silverstoneia_name_COX1 &lt;- &quot;Silverstoneia[Organism] AND COI[Gene]&quot; Silverstoneia_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term= Silverstoneia_name_COX1) Silverstoneia_seq_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=Silverstoneia_seq_IDs$ids, rettype=&quot;fasta&quot;) cat(Silverstoneia_seq_fasta) # for example, we will retrieve “MW042039.1 Silverstoneia flotator voucher TNHCFS4804 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial” 4) With the NCBI accession numbers, you can use ‘rentrez’ to extract these sequences to vector and append to the Ameerega vector in 1). These accession numbers were determined in 2) and 3): MW042037.1, KR862889.1, MW042036.1 and MW042039.1. ## sequences for outgroups of Ameerega outgroups_seq_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=c(&quot;MW042037.1&quot;, &quot;KR862889.1&quot;, &quot;MW042036.1&quot;, &quot;MW042039.1&quot;), rettype=&quot;fasta&quot;) cat(outgroups_seq_fasta) #&gt;MW042037.1 Leucostethus fugax voucher QCAZ16513 cytochrome oxidase subunit I (COI) gene, partial cds; mitochondrial #GTGATAATTACCCGATGATTATTTTCCACAAACCACAAAGACATTGGAACCCTATACCTAGTATTTGGCG #CTTGGGCAGGAATAGTGGGGACTGCCCTAAGTCTCCTCATTCGTGCAGAATTAAGCCAGCCTGGATCCTT #ACTTGGTGACGACCAAATTTACAACGTAATCGTCACTGCCCACGCTTTCGTAATAATTTTTTTTATGGTT #ATACCCATCTTAATTGGGGGATTTGGCAATTGATTAGTCCCATTAATAATTGGAGCACCAGACATAGCTT #TTCCTCGAATAAACAATATAAGCTTCTGACTTTTACCCCCCTCTTTCCTTCTTCTATTAGCTTCTGCAGG #... Now, we can append this vector to Ameerega_seqs_fasta and we can save these sequences in a text file in your working directory. ## append sequences in a single vector all_sequences &lt;- c(Ameerega_seqs_fasta, outgroups_seq_fasta) ## this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory&quot;) write(all_sequences, &quot;Ameerega_COI_seqs_fasta.txt&quot;) Now we can import the nucleotide sequences from this file that are in fasta format into R. ## make sure Biostrings is loaded require(Biostrings) ## load sequences Ameerega_Biostrings_set &lt;- readDNAStringSet(filepath = &quot;~/Desktop/Teach_R/my_working_directory/Ameerega_COI_seqs_fasta.txt&quot;, format = &quot;fasta&quot;) Ameerega_Biostrings_set #A DNAStringSet instance of length 24 # width seq names # [1] 1539 GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGATATTGGAACCTTATAC...TATACGGGTCTCCCCCACCTTATCACACATTTGAGGAAGCCGTTTACTCCAAAATT MW042032.1 Ameere... # [2] 1539 GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGACATTGGAACCTTATAT...TGTACGGATCCCCCCCTCCCTACCATACATTTGAAGAAGCCGTTTATTCCAAAATT MW042031.1 Ameere... # [3] 1539 GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATAC...TATACGGATCCCCCCCACCTTACCACACTTTTGAGGAAGCCGTTTACTCCAAAATT MW042030.1 Ameere... # [4] 646 AACTTTATACCTAGTATTTGGGGCATGAGCGGGCATAGTCGGTACTGCTCTTAGCCT...GTAACCTAAATACCACTTTTTTTGACCCGGCAGGGGGAGGTGACCCTGTCCTATAC KU494334.1 Ameere... # [5] 647 GAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTCAGCC...GTAACTTAAATACCACCTTCTTTGACCCAGCAGGGGGAGGTGACCCTGTTCTATAC KU494333.1 Ameere... # ... ... ... #[20] 658 AACTTTATATCTAGTATTTGGGGCATGGGCAGGCATAGTCGGTACTGCTCTCAGCCT...CCACTTTCTTTGACCCAGCAGGAGGAGGCGACCCCGTCTTATACCAACACCTGTTC DQ502825.1 Epiped... #[21] 1539 GTGATAATTACCCGATGATTATTTTCCACAAACCACAAAGACATTGGAACCCTATAC...TATACGGAACCCCTCCCCCTTATCACACATTTGAAGAAGCTGTTTACTCCAAAATT MW042037.1 Leucos... #[22] 658 AACTTTATACNTTGTATTNGGGGCATGGGCNGGAATAGTNGGAACCGCCCTAAGCCT...CTACTTTCTTCGACCCGGCTGGTGGAGGAGACCCCGTTCTCTACCAACATCTCTTT KR862889.1 Colost... #[23] 1539 GTGATAATTACCCGATGATTATTCTCCACAAACCATAAAGATATCGGAACCCTATAT...TATATGGCTCCCCTCCTCCTTACCACACATTCGAGGAAGCTGTTTATTCTAAAGTA MW042036.1 Epiped... #[24] 1539 GTGATAATTACCCGATGATTATTTTCTACAAACCACAAAGACATTGGAACTTTATAT...TGTATGGATCACCACCACCATATCACACGTTTGAAGAAGCTGTTTATTCTAAAATT MW042039.1 Silver... 12.4 Alignment with DECIPHER We will align our collected sequences with DECIPHER. This R package has several vignettes, but this one is most useful for alignments. 5) The DECIPHER aligner is very easy to use as long as you have already your sequences as a XStringSet from Biostrings (i.e., DNAStringSet, RNAStringSet or AAStringSet). ## install DECIPHER if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;DECIPHER&quot;) ## make sure that DECIPHER and Biostrings are loaded require(DECIPHER) require(Biostrings) ## load YOUR sequences or the example Ameerega_Biostrings_set &lt;- readDNAStringSet(filepath = &quot;~/Desktop/Teach_R/my_working_directory/Ameerega_COI_seqs_fasta.txt&quot;, format = &quot;fasta&quot;) We just can proceed to do the alignment with function AlignSeqs(). ## simple alignment procedure for DNA Ameerega_aligned &lt;- AlignSeqs(Ameerega_Biostrings_set) #Determining distance matrix based on shared 9-mers: # |==========================================================================================================================================| 100% # #Time difference of 0.08 secs # #Clustering into groups by similarity: # |==========================================================================================================================================| 100% # #Alignment converged - skipping remaining iteration. # #Refining the alignment: # |==========================================================================================================================================| 100% # #Time difference of 0.18 secs ## you can call DNAstringSet aligned #Ameerega_aligned # A DNAStringSet instance of length 24 # width seq names # [1] 1539 GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGATATTGGAACCTTATAC...TATACGGGTCTCCCCCACCTTATCACACATTTGAGGAAGCCGTTTACTCCAAAATT MW042032.1 Ameere... # [2] 1539 GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGACATTGGAACCTTATAT...TGTACGGATCCCCCCCTCCCTACCATACATTTGAAGAAGCCGTTTATTCCAAAATT MW042031.1 Ameere... # [3] 1539 GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATAC...TATACGGATCCCCCCCACCTTACCACACTTTTGAGGAAGCCGTTTACTCCAAAATT MW042030.1 Ameere... # [4] 1539 -----------------------------------------------AACTTTATAC...-------------------------------------------------------- KU494334.1 Ameere... # [5] 1539 ----------------------------------------------GAACTTTATAC...-------------------------------------------------------- KU494333.1 Ameere... # ... ... ... #[20] 1539 -----------------------------------------------AACTTTATAT...-------------------------------------------------------- DQ502825.1 Epiped... #[21] 1539 GTGATAATTACCCGATGATTATTTTCCACAAACCACAAAGACATTGGAACCCTATAC...TATACGGAACCCCTCCCCCTTATCACACATTTGAAGAAGCTGTTTACTCCAAAATT MW042037.1 Leucos... #[22] 1539 -----------------------------------------------AACTTTATAC...-------------------------------------------------------- KR862889.1 Colost... #[23] 1539 GTGATAATTACCCGATGATTATTCTCCACAAACCATAAAGATATCGGAACCCTATAT...TATATGGCTCCCCTCCTCCTTACCACACATTCGAGGAAGCTGTTTATTCTAAAGTA MW042036.1 Epiped... #[24] 1539 GTGATAATTACCCGATGATTATTTTCTACAAACCACAAAGACATTGGAACTTTATAT...TGTATGGATCACCACCACCATATCACACGTTTGAAGAAGCTGTTTATTCTAAAATT MW042039.1 Silver... 12.5 Visualize alignment 6) We can visualize the alignment with BrowseSeqs() and a new tab in your browser will appear with the alignment to visualize. ## simple visualization of alignment BrowseSeqs(Ameerega_aligned) 7) We can align the sequences based on their amino acid translation using AlignTranslation(). Notice that the genetic code needs to be specified, if this is not the standard. ## if the genetic code is standard, you do not need to define it. In this case, we have sequences from the mitochondria and this needs to be defined as follows. mt_vertebrate_code &lt;- getGeneticCode(&quot;SGC1&quot;) mt_vertebrate_code #TTT TTC TTA TTG TCT TCC TCA TCG TAT TAC TAA TAG TGT TGC TGA TGG CTT CTC CTA CTG CCT CCC CCA CCG CAT CAC CAA CAG CGT CGC CGA CGG ATT ATC ATA ATG ACT #&quot;F&quot; &quot;F&quot; &quot;L&quot; &quot;L&quot; &quot;S&quot; &quot;S&quot; &quot;S&quot; &quot;S&quot; &quot;Y&quot; &quot;Y&quot; &quot;*&quot; &quot;*&quot; &quot;C&quot; &quot;C&quot; &quot;W&quot; &quot;W&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;L&quot; &quot;P&quot; &quot;P&quot; &quot;P&quot; &quot;P&quot; &quot;H&quot; &quot;H&quot; &quot;Q&quot; &quot;Q&quot; &quot;R&quot; &quot;R&quot; &quot;R&quot; &quot;R&quot; &quot;I&quot; &quot;I&quot; &quot;M&quot; &quot;M&quot; &quot;T&quot; #ACC ACA ACG AAT AAC AAA AAG AGT AGC AGA AGG GTT GTC GTA GTG GCT GCC GCA GCG GAT GAC GAA GAG GGT GGC GGA GGG #&quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;N&quot; &quot;N&quot; &quot;K&quot; &quot;K&quot; &quot;S&quot; &quot;S&quot; &quot;*&quot; &quot;*&quot; &quot;V&quot; &quot;V&quot; &quot;V&quot; &quot;V&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;D&quot; &quot;D&quot; &quot;E&quot; &quot;E&quot; &quot;G&quot; &quot;G&quot; &quot;G&quot; &quot;G&quot; #attr(,&quot;alt_init_codons&quot;) #[1] &quot;ATT&quot; &quot;ATC&quot; &quot;GTG&quot; ## you translate and align your nucleotide sequence into amino acids (AA). Ameerega_AA_set &lt;- AlignTranslation(Ameerega_Biostrings_set, type=&quot;AAStringSet&quot;, geneticCode = mt_vertebrate_code) ## see the AAStringSet Ameerega_AA_set #A AAStringSet instance of length 24 # width seq names # [1] 555 MMITRWLFSTNHKDIGTLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...SLISLVAVIIMMFIIWEAFSSKRLPFPAEMTPTNVEWLYGSPPPYHTFEEAVYSKI MW042032.1 Ameere... # [2] 555 MMITRWLFSTNHKDIGTLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...SLISLVAVIIMMFIIWEAFSSKRLPLPAEMTPTNVEWLYGSPPPYHTFEEAVYSKI MW042031.1 Ameere... # [3] 555 MMITRWLFSTNHKDIGTLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...SLISLVAVIIMMFIIWEAFSSKRLPLPAEMTPTNVEWLYGSPPPYHTFEEAVYSKI MW042030.1 Ameere... # [4] 555 ---------------+TLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...-------------------------------------------------------- KU494334.1 Ameere... # [5] 555 ---------------+TLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...-------------------------------------------------------- KU494333.1 Ameere... # ... ... ... #[20] 555 ---------------+TLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...-------------------------------------------------------- DQ502825.1 Epiped... #[21] 555 MMITRWLFSTNHKDIGTLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...SLISLVAVILMMFIIWEAFSSKRLFLTAEMTSTNVEWLYGTPPPYHTFEEAVYSKI MW042037.1 Leucos... #[22] 555 ---------------+TLYXVXGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...-------------------------------------------------------- KR862889.1 Colost... #[23] 555 MMITRWLFSTNHKDIGTLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...SLISLVAVIIMMFIIWEAFSSKRLFLNAEMTPTNVEWLYGSPPPYHTFEEAVYSKV MW042036.1 Epiped... #[24] 555 MMITRWLFSTNHKDIGTLYLVFGAWAGMVGTALSLLIRAELSQPGSLLGDDQIYNVI...SLISLVAVILMMFIIWEAFSSKRLFLNAEMTPTNVEWLYGSPPPYHTFEEAVYSKI MW042039.1 Silver... ## simple visualization of AA alignment with references to sequence 1 BrowseSeqs(Ameerega_AA_set, highlight=1) 8) You can further improve the alignment with the function AdjustAlignment(). This function makes small adjustments with the goal of removing artifacts of the progressive alignment process by shifting groups of gaps left and right to find their optimal positioning in a multiple sequence alignment. This function will efficiently correct most obvious inaccuracies that could be found by-eye and this is repeatable and not subjective (i.e., a common criticism of alignments done by-eye). ## some further adjustments if necessary Ameerega_aligned_end &lt;- AdjustAlignment(Ameerega_aligned) Ameerega_AA_set_end &lt;- AdjustAlignment(Ameerega_AA_set) ## simple visualization of alignments BrowseSeqs(Ameerega_aligned_end) BrowseSeqs(Ameerega_AA_set_end) These alignments look pretty good. However, in the Ameerega_AA_set_end one sequence looks problematic: DQ502832.1 Epipedobates braccatus isolate 537 cytochrome oxidase subunit I-like (COI) gene, partial sequence; mitochondrial. This might be the result of the translation, but we will eliminate this sequence in further analyses. ## to remove a sequence from DNAStringSet, AAStringSet, we need to make sure is the one that we want to eliminate ## in this case is [18] str(Ameerega_AA_set_end[18]) #Formal class &#39;AAStringSet&#39; [package &quot;Biostrings&quot;] with 5 slots # ..@ pool :Formal class &#39;SharedRaw_Pool&#39; [package &quot;XVector&quot;] with 2 slots # .. .. ..@ xp_list :List of 1 # .. .. .. ..$ :&lt;externalptr&gt; # .. .. ..@ .link_to_cached_object_list:List of 1 # .. .. .. ..$ :&lt;environment: 0x7fdf44ffa898&gt; # ..@ ranges :Formal class &#39;GroupedIRanges&#39; [package &quot;XVector&quot;] with 7 slots # .. .. ..@ group : int 1 # .. .. ..@ start : int 9436 # .. .. ..@ width : int 555 # .. .. ..@ NAMES : chr &quot;DQ502832.1 Epipedobates braccatus isolate 537 cytochrome oxidase subunit I-like (COI) gene, partial sequence; mitochondrial&quot; # .. .. ..@ elementType : chr &quot;ANY&quot; # .. .. ..@ elementMetadata: NULL # .. .. ..@ metadata : list() # ..@ elementType : chr &quot;AAString&quot; # ..@ elementMetadata: NULL # ..@ metadata : list() ## we remove AA seq 18 and adjust the alignment Ameerega_AA_set_clean &lt;- Ameerega_AA_set[-18] Ameerega_AA_set_clean &lt;- AdjustAlignment(Ameerega_AA_set_clean) ## we make sure that it is not present in the alignment BrowseSeqs(Ameerega_AA_set_clean) The sequence with errors is now removed from further analyses. 12.6 Save alignment to file 9) With our aligned sequences, we can export these as fasta and then they can be used to reconstruct phylogenetic trees, get protein properties and other analyses. We can save our sequences as fasta using the function writeXStringSet(). # this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory&quot;) ## We save the nucleotide alignment writeXStringSet(Ameerega_aligned_end, file=&quot;Ameerega_aligned_end.fasta&quot;, format=&quot;fasta&quot;) writeXStringSet(Ameerega_AA_set_clean, file=&quot;Ameerega_AA_aligned_end.fasta&quot;, format=&quot;fasta&quot;) 12.7 Concatenate and align several markers In many cases, you might combine several markers, segments, exons or other disconnected (not adjacent) sequences. We will follow a procedure in DECIPHER. 10) We need to find a sequence that is not adjacent to our COI marker, in this case we will download cytochrome b (CYTB) and COI sequences of Ameerega, Colostethus, Epipedobates and Silverstoneia. ## make sure rentrez is loaded require(rentrez) ## sequences for of all using a loop and list to store each genus CYTB sequences my_genus &lt;- c(&quot;Ameerega&quot;, &quot;Colostethus&quot;, &quot;Epipedobates&quot;, &quot;Silverstoneia&quot;, &quot;Leucostethus&quot;) ## loop for CYTB collect_CYTB_list_seqs &lt;- list() for (i in 1:length(my_genus)) { one_genus_CYTB &lt;- paste0(my_genus[i], &quot;[Organism] AND CYTB[Gene]&quot;) one_genus_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term= one_genus_CYTB ) one_genus_seq_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=one_genus_seq_IDs$ids, rettype=&quot;fasta&quot;) ## collect sequences in list collect_CYTB_list_seqs[[i]] &lt;- one_genus_seq_fasta ## visualize the collected sequences cat(&quot;these are CYTB sequences from genus:&quot;, my_genus[i], &quot;\\n&quot;) Sys.sleep(1) cat(collect_CYTB_list_seqs[[i]]) } ## loop for COI collect_COI_list_seqs &lt;- list() for (i in 1:length(my_genus)) { one_genus_COI &lt;- paste0(my_genus[i], &quot;[Organism] AND COI[Gene]&quot;) one_genus_seq_IDs &lt;- entrez_search(db=&quot;nuccore&quot;, term= one_genus_COI ) one_genus_seq_fasta &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=one_genus_seq_IDs$ids, rettype=&quot;fasta&quot;) ## collect sequences in list collect_COI_list_seqs[[i]] &lt;- one_genus_seq_fasta ## visualize the collected sequences cat(&quot;these are COI sequences from genus:&quot;, my_genus[i], &quot;\\n&quot;) Sys.sleep(1) cat(collect_COI_list_seqs[[i]]) } 11) Now, we need to inspect these sequences and choose the one sequence per species per maker (e.g., for Ameerega bilinguis one CYTB and one COI). We will record the accession number for these two sequences per species. You will do this for example, but in most cases, you will collect the accession numbers for the markers of interest in your project. You might also need to check the NCBI website. ## visualize sequences in console # &quot;Ameerega&quot; cat(collect_CYTB_list_seqs[[1]]) cat(collect_COI_list_seqs[[1]]) # &quot;Colostethus&quot; cat(collect_CYTB_list_seqs[[2]]) cat(collect_COI_list_seqs[[2]]) # &quot;Epipedobates&quot; cat(collect_CYTB_list_seqs[[3]]) cat(collect_COI_list_seqs[[3]]) # &quot;Silverstoneia&quot; cat(collect_CYTB_list_seqs[[4]]) cat(collect_COI_list_seqs[[4]]) # &quot;Leucostethus&quot; cat(collect_CYTB_list_seqs[[5]]) cat(collect_COI_list_seqs[[5]]) ## build vectors before a dataframe collected_list &lt;- list( data.frame(taxa = &quot;Ameerega_bilinguis&quot;, CYTB = &quot;AF128559.1&quot;, COX = &quot;MW042030.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Ameerega_hahneli&quot;, CYTB = &quot;HQ290575&quot;, COX = &quot;MW042031.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Ameerega_parvula&quot;, CYTB = &quot;HQ290576.1&quot;, COX = &quot;MW042032.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Ameerega_braccata&quot;, CYTB = &quot;DQ502556.1&quot;, COX = &quot;KU494333.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Ameerega_trivittata&quot;, CYTB = &quot;HQ290579.1&quot;, COX = &quot;DQ502903.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Ameerega_silverstonei&quot;, CYTB = &quot;DQ523154.1&quot;, COX = &quot;DQ502851.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Colostethus_panamansis&quot;, CYTB = &quot;HQ290546.1&quot;, COX = &quot;KC129189.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Colostethus_pratti&quot;, CYTB = &quot;HQ290547.1&quot;, COX = &quot;KF807016.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Epipedobates_boulengeri&quot;, CYTB = &quot;HQ290574.1&quot;, COX = &quot;MW042034.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Epipedobates_machalilla&quot;, CYTB = &quot;HQ290542.1&quot;, COX = &quot;MW042036.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Silverstoneia_flotator&quot;, CYTB = &quot;HQ290537.1&quot;, COX = &quot;MW042039.1&quot;, stringsAsFactors = FALSE), data.frame(taxa = &quot;Leucostethus_fugax&quot;, CYTB = NA, COX = &quot;MW042037.1&quot;, stringsAsFactors = FALSE)) ## build a dataframe collected_df &lt;- do.call(rbind, collected_list) collected_df # taxa CYTB COX #1 Ameerega_bilinguis AF128559.1 MW042030.1 #2 Ameerega_hahneli HQ290575 MW042031.1 #3 Ameerega_parvula HQ290576.1 MW042032.1 #4 Ameerega_braccata DQ502556.1 KU494333.1 #5 Ameerega_trivittata HQ290579.1 DQ502903.1 #6 Ameerega_silverstonei DQ523154.1 DQ502851.1 #7 Colostethus_panamansis HQ290546.1 KC129189.1 #8 Colostethus_pratti HQ290547.1 KF807016.1 #9 Epipedobates_boulengeri HQ290574.1 MW042034.1 #10 Epipedobates_machalilla HQ290542.1 MW042036.1 #11 Silverstoneia_flotator HQ290537.1 MW042039.1 #12 Leucostethus_fugax &lt;NA&gt; MW042037.1 12) We can now retrieve these sequences in a data frame and build a fasta file with commo taxon names. ## retrive sequences for each taxon collected_CYTB_seqs &lt;- character() collected_COI_seqs &lt;- character() ## start of loop for(i in 1:nrow(collected_df)) { # i &lt;- 12 one_taxon_name &lt;- collected_df[i,&quot;taxa&quot;] # retrieve sequences if(!is.na(collected_df[i,&quot;CYTB&quot;])) { one_taxon_CYTB &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=collected_df[i,&quot;CYTB&quot;], rettype=&quot;fasta&quot;) } else { one_taxon_CYTB &lt;- &quot;--------&quot;} if(!is.na(collected_df[i,&quot;COX&quot;])) { one_taxon_COI &lt;- entrez_fetch(db=&quot;nuccore&quot;, id=collected_df[i,&quot;COX&quot;], rettype=&quot;fasta&quot;) } else { one_taxon_COI &lt;- &quot;--------&quot;} # create a common name, split sequence vecto by first \\n one_taxon_CYTB_1 &lt;- unlist(strsplit(one_taxon_CYTB, split=&quot;\\n&quot;, perl=TRUE)) one_taxon_CYTB_1 #[1] &quot;&gt;AF128559.1 Epipedobates bilinguis cytochrome b (cytb) gene, partial cds; mitochondrial gene for mitochondrial product&quot; #[2] &quot;CTTCTAGGTCTATGTCTTATTGCCCAAATCGCTACAGGCCTTTTTCTTGCCATACACTATACTGCTGATA&quot; #[3] &quot;CTTCTATAGCTTTTTCCTCTCTAGCCCATATTTGCCGAGATGTCAACAATGGCTGACTTCTTCGTAATCT&quot; #[4] &quot;TCACGCTAACGGAGCCTCATTCTTCTTCATTTGTATTTATCTTCATATTGGCCGAGGAATATACTATGGA&quot; #[5] &quot;TCCTTTTTATTTAAAGAAACATGAAACATTGGAGTAATTTTATTATTTTTAGTTATAGCCAC&quot; #[6] &quot;&quot; # we want to create a common name, so we will replace the first line and concatenate all others one_taxon_CYTB_2 &lt;- paste0(one_taxon_CYTB_1[2:length(one_taxon_CYTB_1)], collapse =&quot;&quot;) one_taxon_CYTB_2 #[1] &quot;CTTCTAGGTCTATGTCTTATTGCCCAAATCGCTACAGGCCTTTTTCTTGCCATACACTATACTGCTGATACTTCTATAGCTTTTTCCTCTCTAGCCCATATTTGCCGAGATGTCAACAATGGCTGACTTCTTCGTAATCTTCACGCTAACGGAGCCTCATTCTTCTTCATTTGTATTTATCTTCATATTGGCCGAGGAATATACTATGGATCCTTTTTATTTAAAGAAACATGAAACATTGGAGTAATTTTATTATTTTTAGTTATAGCCAC&quot; #we will vector for sequences one_taxon_CYTB_3 &lt;- paste0(&quot;&gt;&quot;,one_taxon_name,&quot;\\n&quot;,one_taxon_CYTB_2,&quot;\\n\\n&quot;, sep=&quot;&quot;) cat(one_taxon_CYTB_3) #&gt;Ameerega_bilinguis #CTTCTAGGTCTATGTCTTATTGCCCAAATCGCTACAGGCCTTTTTCTTGCCATACACTATACTGCTGATACTTCTATAGCTTTTTCCTCTCTAGCCCATATTTGCCGAGATGTCAACAATGGCTGACTTCTTCGTAATCTTCACGCTAACGGAGCCTCATTCTTCTTCATTTGTATTTATCTTCATATTGGCCGAGGAATATACTATGGATCCTTTTTATTTAAAGAAACATGAAACATTGGAGTAATTTTATTATTTTTAGTTATAGCCAC collected_CYTB_seqs[i] &lt;- one_taxon_CYTB_3 cat(&quot;\\n\\n&quot;) ## for COI one_taxon_COI_1 &lt;- unlist(strsplit(one_taxon_COI, split=&quot;\\n&quot;, perl=TRUE)) one_taxon_COI_2 &lt;- paste0(one_taxon_COI_1[2:length(one_taxon_COI_1)], collapse =&quot;&quot;) one_taxon_COI_3 &lt;- paste0(&quot;&gt;&quot;,one_taxon_name,&quot;\\n&quot;,one_taxon_COI_2,&quot;\\n\\n&quot;, sep=&quot;&quot;) cat(one_taxon_COI_3) #&gt;Ameerega_bilinguis #GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTTAGCCTTTTAATTCGAGCCGAATTAAGCCAGCCCGGGTCCTTACTAGGCGATGACCAGATCTACAACGTTATTGTTACCGCCCATGCTTTCGTTATAATCTTTTTTATAGTAATGCCAATTCTAATCGGTGGCTTTGGGAATTGATTAGTGCCCCTAATAATTGGAGCCCCAGACATAGCTTTTCCCCGAATAAACAATATGAGCTTTTGGCTTCTTCCCCCCTCTTTCCTACTACTCCTAGCATCCGCAGGCGTTGAAGCAGGCGCCGGTACTGGCTGAACTGTGTACCCTCCCCTTGCAGGCAACCTAGCTCATGCTGGCCCATCAGTTGATTTAACTATTTTTTCACTTCATCTCGCCGGTGTTTCTTCTATTCTAGGGGCAATTAACTTTATTACAACAACCTTAAACATAAAACCCCCTTCATTAACACAATATCAAACCCCATTATTTGTCTGATCTGTATTAATTACTGCAGTCCTTCTTCTTCTCTCCCTCCCAGTTCTGGCTGCCGGAATCACTATACTCTTGACTGACCGAAACCTAAACACCACCTTCTTTGACCCAGCAGGTGGAGGCGACCCTGTCCTGTACCAACACCTGTTCTGATTCTTTGGTCACCCCGAAGTCTACATCCTTATCCTGCCTGGATTTGGTATCATCTCCCATGTTGTCACATTCTACTCTAGCAAAAAAGAACCCTTCGGCTATATAGGAATAGTCTGAGCTATAATATCGATTGGTCTCCTAGGTTTCATTGTTTGAGCTCACCACATATTCACAACAGACCTTAATGTAGACACTCGAGCCTACTTTACCTCAGCTACTATAATCATCGCTATCCCAACAGGTGTCAAAGTCTTTAGCTGACTTGCCACCATGCACGGAGGAATTATTAAATGAGACGCCGCCATATTATGGGCTCTCGGATTCATCTTTTTATTTACAGTTGGAGGACTAACTGGAATCGTTTTAGCCAACTCCTCTTTAGACATTGTTTTGCATGATACATATTATGTAGTAGCCCACTTTCACTACGTTCTTTCTATGGGGGCAGTATTTGCCATTATAGCCGGCTTCGTACACTGATTTCCTCTCTTTTCCGGATTTACCCTTCATGAAGCCTGAACAAAAATTCAATTTGGCGTCATATTTACCGGCGTAAATTTAACATTCTTCCCCCAGCATTTCTTAGGTCTCGCAGGCATGCCTCGACGTTATTCAGACTACCCTGACGCCTACACATTATGAAACACCGTTTCATCAATCGGCTCTTTAATCTCTCTAGTTGCAGTAATCATTATGATGTTTATCATTTGAGAAGCTTTCTCTTCCAAACGCCTACCTCTACCTGCAGAAATAACCCCAACTAATGTAGAATGATTATACGGATCCCCCCCACCTTACCACACTTTTGAGGAAGCCGTTTACTCCAAAATT collected_COI_seqs[i] &lt;- one_taxon_COI_3 } ## end of loop ## collapse all collections before write a text file collected_CYTB_seqs_con &lt;- paste0(collected_CYTB_seqs, collapse=&quot;&quot;) collected_COI_seqs_con &lt;- paste0(collected_COI_seqs, collapse=&quot;&quot;) We can save these sequences in a text file in your working directory. # this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory&quot;) write(collected_CYTB_seqs_con, &quot;my_collected_CYTB_fasta.txt&quot;) write(collected_COI_seqs_con, &quot;my_collected_COI_fasta.txt&quot;) 13) Now we can import the CYTB and COI nucleotide sequences from this file that are in fasta format into R and align each with DECIPHER. ## make sure that Biostrings is loaded require(Biostrings) ## load CYTB CYTB_set &lt;- readDNAStringSet(filepath = &quot;~/Desktop/Teach_R/my_working_directory/my_collected_CYTB_fasta.txt&quot;, format = &quot;fasta&quot;) CYTB_set #A DNAStringSet instance of length 12 # width seq names # [1] 272 CTTCTAGGTCTATGTCTTATTGCCCAAATCGCTACAGGCCTTTTTCTTGCCATACAC...TTATTTAAAGAAACATGAAACATTGGAGTAATTTTATTATTTTTAGTTATAGCCAC Ameerega_bilinguis # [2] 690 GGCCTATGTTTAATTGCCCAAATCATCACAGGTCTCTTCCTAGCCATACACTATACA...TTCTCGGAGATCCAGATAATTTTACCCCAGCTAACCCCCTAGTCACCCCCCCTCAT Ameerega_hahneli # [3] 690 GGCTTATGTCTAATTGCTCAAATCATTACAGGTCTTTTTCTAGCTATACATTATACA...TTCTCGGAGATCCAGACAATTTCACCCCAGCTAACCCCCTAGTCACCCCCCCTCAC Ameerega_parvula # [4] 385 AACACACCCCGCTCTAAAAATTATCAACAACTCATTCATTGACCTTCCATCCCCTGC...GCGTGATTCTTCTATTTTTAGTAATAGCCACCGCCTTCGTCGGCTATGTCCTCCCT Ameerega_braccata # [5] 690 GGCCTCTGCCTAATCGCCCAGATCATCACAGGCCTCTTCTTAGCTATACACTACACA...TTCTCGGAGACCCAGACAATTTCACCCCCGCTAACCCCCTGGTCACCCCACCTCAC Ameerega_trivittata # ... ... ... # [8] 690 GGCCTCTGCCTTATTATCCAGATTGTCACTGGCCTTTTCCTAGCTATGCACTATACA...TCTTGGGCGACCCAGACAACTTCACCCCAGCTAACCCCCTAGTTACTCCCCCCCAC Colostethus_pratti # [9] 690 GGTCTATGTCTTATTGCCCAAATCGCTACAGGACTTTTTCTTGCTATACACTATACT...TTTTAGGAGACCCAGACAACTTCACCCCTGCTAACCCTCTAGTCACCCCTCCTCAT Epipedobates_boul... #[10] 690 GGCCTATGTCTTATTGCCCAAATCGCTACAGGTCTTTTTCTTGCCATACACTATACT...TCCTAGGAGATCCAGACAACTTCACCCCTGCCAACCCTTTAGTCACCCCTCCTCAC Epipedobates_mach... #[11] 690 GGCCTTTGCCTCATTGCCCAAATTGCCACAGGCCTTTTTTTAGCTATACACTACACC...TTCTAGGCGACCCAGACAACTTCACCCCAGCTAATCCCCTAGTAACCCCACCACAC Silverstoneia_flo... #[12] 10 NA-------- ## NOTICE: for alignment, we need to remove any gap of missing gaps. In this case sequence [12] needs to be removed. CYTB_set &lt;- CYTB_set[-12] ## load COI COI_set &lt;- readDNAStringSet(filepath = &quot;~/Desktop/Teach_R/my_working_directory/my_collected_COI_fasta.txt&quot;, format = &quot;fasta&quot;) COI_set #A DNAStringSet instance of length 12 # width seq names # [1] 1539 GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATAC...TATACGGATCCCCCCCACCTTACCACACTTTTGAGGAAGCCGTTTACTCCAAAATT Ameerega_bilinguis # [2] 1539 GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGACATTGGAACCTTATAT...TGTACGGATCCCCCCCTCCCTACCATACATTTGAAGAAGCCGTTTATTCCAAAATT Ameerega_hahneli # [3] 1539 GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGATATTGGAACCTTATAC...TATACGGGTCTCCCCCACCTTATCACACATTTGAGGAAGCCGTTTACTCCAAAATT Ameerega_parvula # [4] 647 GAACTTTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTCAGCC...GTAACTTAAATACCACCTTCTTTGACCCAGCAGGGGGAGGTGACCCTGTTCTATAC Ameerega_braccata # [5] 658 AACTCTATACCTAGTGTTTGGGGCATGAGCAGGCATAGTCGGCACTGCTCTTAGTCT...CCACTTTCTTTGACCCAGCAGGGGGAGGCGACCCCGTCCTGTACCAACACCTGTTC Ameerega_trivittata # ... ... ... # [8] 658 AACTTTATACCTTGTATTTGGGGCATGGGCCGGAATAGTCGGAACCGCCCTAAGCCT...CTACTTTCTTTGACCCGGCCGGCGGAGGTGACCCTGTTCTCTACCAACATCTCTTT Colostethus_pratti # [9] 1539 GTGATAATTACCCGATGATTATTTTCCACAAACCATAAAGATATTGGAACCCTATAT...TATATGGCTCCCCTCCTCCTTACCACACATTTGAGGAAGCCGTTTATTCTAAAGTA Epipedobates_boul... #[10] 1539 GTGATAATTACCCGATGATTATTCTCCACAAACCATAAAGATATCGGAACCCTATAT...TATATGGCTCCCCTCCTCCTTACCACACATTCGAGGAAGCTGTTTATTCTAAAGTA Epipedobates_mach... #[11] 1539 GTGATAATTACCCGATGATTATTTTCTACAAACCACAAAGACATTGGAACTTTATAT...TGTATGGATCACCACCACCATATCACACGTTTGAAGAAGCTGTTTATTCTAAAATT Silverstoneia_flo... #[12] 1539 GTGATAATTACCCGATGATTATTTTCCACAAACCACAAAGACATTGGAACCCTATAC...TATACGGAACCCCTCCCCCTTATCACACATTTGAAGAAGCTGTTTACTCCAAAATT Leucostethus_fugax We just can proceed to do the alignment with function AlignSeqs() and AdjustAlignment(). ## make sure that DECIPHER is loaded require(DECIPHER) ## CYTB alignment and further adjustments if necessary CYTB_aligned &lt;- AlignSeqs(CYTB_set) CYTB_aligned &lt;- AdjustAlignment(CYTB_aligned) CYTB_aligned #A DNAStringSet instance of length 11 # width seq names # [1] 944 ---------------------------------------------------------...-------------------------------------------------------- Ameerega_bilinguis # [2] 944 ---------------------------------------------------------...-------------------------------------------------------- Ameerega_hahneli # [3] 944 ---------------------------------------------------------...-------------------------------------------------------- Ameerega_parvula # [4] 944 AACACACCCCGCTCTAAAAATTATCAACAACTCATTCATTGACCTTCCATCCCCTGC...-------------------------------------------------------- Ameerega_braccata # [5] 944 ---------------------------------------------------------...-------------------------------------------------------- Ameerega_trivittata # ... ... ... # [7] 944 ---------------------------------------------------------...-------------------------------------------------------- Colostethus_panam... # [8] 944 ---------------------------------------------------------...-------------------------------------------------------- Colostethus_pratti # [9] 944 ---------------------------------------------------------...-------------------------------------------------------- Epipedobates_boul... #[10] 944 ---------------------------------------------------------...-------------------------------------------------------- Epipedobates_mach... #[11] 944 ---------------------------------------------------------...-------------------------------------------------------- Silverstoneia_flo... ## check CYTB alignment BrowseSeqs(CYTB_aligned) ## COI alignment and further adjustments if necessary COI_aligned &lt;- AlignSeqs(COI_set) COI_aligned &lt;- AdjustAlignment(COI_aligned) COI_aligned # A DNAStringSet instance of length 12 # width seq names # [1] 1539 GTGATAATTACTCGATGATTATTTTCTACCAACCACAAAGACATCGGAACTTTATAC...TATACGGATCCCCCCCACCTTACCACACTTTTGAGGAAGCCGTTTACTCCAAAATT Ameerega_bilinguis # [2] 1539 GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGACATTGGAACCTTATAT...TGTACGGATCCCCCCCTCCCTACCATACATTTGAAGAAGCCGTTTATTCCAAAATT Ameerega_hahneli # [3] 1539 GTGATAATTACTCGATGATTATTTTCCACCAACCATAAAGATATTGGAACCTTATAC...TATACGGGTCTCCCCCACCTTATCACACATTTGAGGAAGCCGTTTACTCCAAAATT Ameerega_parvula # [4] 1539 ----------------------------------------------GAACTTTATAC...-------------------------------------------------------- Ameerega_braccata # [5] 1539 -----------------------------------------------AACTCTATAC...-------------------------------------------------------- Ameerega_trivittata # ... ... ... # [8] 1539 -----------------------------------------------AACTTTATAC...-------------------------------------------------------- Colostethus_pratti # [9] 1539 GTGATAATTACCCGATGATTATTTTCCACAAACCATAAAGATATTGGAACCCTATAT...TATATGGCTCCCCTCCTCCTTACCACACATTTGAGGAAGCCGTTTATTCTAAAGTA Epipedobates_boul... #[10] 1539 GTGATAATTACCCGATGATTATTCTCCACAAACCATAAAGATATCGGAACCCTATAT...TATATGGCTCCCCTCCTCCTTACCACACATTCGAGGAAGCTGTTTATTCTAAAGTA Epipedobates_mach... #[11] 1539 GTGATAATTACCCGATGATTATTTTCTACAAACCACAAAGACATTGGAACTTTATAT...TGTATGGATCACCACCACCATATCACACGTTTGAAGAAGCTGTTTATTCTAAAATT Silverstoneia_flo... #[12] 1539 GTGATAATTACCCGATGATTATTTTCCACAAACCACAAAGACATTGGAACCCTATAC...TATACGGAACCCCTCCCCCTTATCACACATTTGAAGAAGCTGTTTACTCCAAAATT Leucostethus_fugax ## check COI alignment BrowseSeqs(COI_aligned) 12.8 Concatenate with data.table 14) We can now concatenate all sequences before we proceed to phylogenetic tree estimation. Given that CYTB_aligned lacks one sequence, the process of concatenation is convoluted. Here is procedure derived from a suggestion here using the R package data.table. ## make sure that data.table is loaded require(data.table) ## transform into matrices using data.table CYTB_aligned.dt&lt;-data.table(as.matrix(CYTB_aligned), keep.rownames = TRUE) COI_aligned.dt&lt;-data.table(as.matrix(COI_aligned), keep.rownames = TRUE) COI_CYTB.dt &lt;-merge(COI_aligned.dt, CYTB_aligned.dt, by=&quot;rn&quot;, all=TRUE) ## slightly modified from original, added arg &quot;x&quot; f_dowle &lt;- function(dt, x) { na.replace = function(v,value=x) { v[is.na(v)] = value; v } for (i in names(dt)) eval(parse(text=paste(&quot;dt[,&quot;,i,&quot;:=na.replace(&quot;,i,&quot;)]&quot;))) } ## use f_dowle() to replace NA for &quot;-&quot; f_dowle(COI_CYTB.dt, &quot;-&quot;) ## return to matrix COI_CYTB &lt;- apply(COI_CYTB.dt[ ,!&quot;rn&quot;], 1, paste, collapse=&quot;&quot;) ## Convert back to DNAStringSet and add back names COI_CYTB_DT_path_set &lt;- DNAStringSet(COI_CYTB) names(COI_CYTB_DT_path_set) &lt;- COI_CYTB.dt$rn ## check COI_CYTB alignment BrowseSeqs(COI_CYTB_DT_path_set) 12.9 Concatenate with Biostrings 15) An alternative way to perform concatenation using Biostrings is as follows. ## make sure that Biostrings is loaded require(Biostrings) ## find what sequence (taxon) name is missing missing_COI_taxon &lt;- base::setdiff(names(CYTB_aligned),names(COI_aligned)) missing_COI_taxon #character(0) missing_CYTB_taxon &lt;- base::setdiff(names(COI_aligned),names(CYTB_aligned)) missing_CYTB_taxon #[1] &quot;Leucostethus_fugax&quot; ## The sequence of &quot;Leucostethus_fugax&quot; is missing in CYTB matrix, we need add such sequence as gaps &quot;-&quot; to the aligned matrix. CYTB_aligned #A DNAStringSet instance of length 11 # width seq names # [1] 944 ---------------------------------------------------------...-------------------------------------------------------- Ameerega_bilinguis # [2] 944 ---------------------------------------------------------...-------------------------------------------------------- Ameerega_hahneli # [3] 944 ---------------------------------------------------------...-------------------------------------------------------- Ameerega_parvula # [4] 944 AACACACCCCGCTCTAAAAATTATCAACAACTCATTCATTGACCTTCCATCCCCTGC...-------------------------------------------------------- Ameerega_braccata # [5] 944 ---------------------------------------------------------...-------------------------------------------------------- Ameerega_trivittata # ... ... ... # [7] 944 ---------------------------------------------------------...-------------------------------------------------------- Colostethus_panam... # [8] 944 ---------------------------------------------------------...-------------------------------------------------------- Colostethus_pratti # [9] 944 ---------------------------------------------------------...-------------------------------------------------------- Epipedobates_boul... #[10] 944 ---------------------------------------------------------...-------------------------------------------------------- Epipedobates_mach... #[11] 944 ---------------------------------------------------------...-------------------------------------------------------- Silverstoneia_flo... ## we notice that each aligned CYTB sequences has 944 characters, so we need to create a vector named Leucostethus_fugax with 944 &quot;-&quot;. missing_taxon_CYTB &lt;- paste0(rep(&quot;-&quot;,944), collapse =&quot;&quot;) names(missing_taxon_CYTB) &lt;- &quot;Leucostethus_fugax&quot; missing_taxon_CYTB_set &lt;- DNAStringSet(missing_taxon_CYTB) ## we add missing sequence CYTB_aligned_add &lt;- CYTB_aligned names_on_stringset &lt;- names(CYTB_aligned_add) names_on_stringset &lt;- c(names_on_stringset, &quot;Leucostethus_fugax&quot;) ## append sequence at the [12] slots and add taxon name CYTB_aligned_add[12] &lt;- missing_taxon_CYTB_set ## change names names(CYTB_aligned_add) &lt;- names_on_stringset ## Confirm that we do not have any more missing taxa (i.e., character(0)) missing_COI_taxon &lt;- base::setdiff(names(CYTB_aligned_add),names(COI_aligned)) missing_COI_taxon #character(0) missing_CYTB_taxon &lt;- base::setdiff(names(COI_aligned),names(CYTB_aligned_add)) missing_CYTB_taxon #character(0) ## IMPORTANT: to make concatenation, you have to make sure that the order of names in both COI and CYTB are the same COI_aligned_order &lt;- COI_aligned[order(names(COI_aligned), decreasing=FALSE)] CYTB_aligned_add_order &lt;- CYTB_aligned_add[order(names(CYTB_aligned_add), decreasing=FALSE)] ## Now, we can finally concatenate with xsact() COI_CYTB_String_path_set &lt;- xscat(COI_aligned_order,CYTB_aligned_add_order) names(COI_CYTB_String_path_set) &lt;- names(COI_aligned_order) ## check COI_CYTB alignment BrowseSeqs(COI_CYTB_String_path_set ) 16) We can finally save these concatenated sequences to a fasta file. # this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory&quot;) writeXStringSet(COI_CYTB_DT_path_set, file=&quot;my_collected_COI_CYTB_DT_path_fasta.txt&quot;, format=&quot;fasta&quot;) writeXStringSet(COI_CYTB_String_path_set, file=&quot;my_collected_COI_CYTB_String_path_fasta.txt&quot;, format=&quot;fasta&quot;) 12.10 Visualizing alignments in R 17) Visualizing your alignment in R is not yet optimal, yet there is at least one ggplot2 add-on that can do it ggmsa. This package requires another package: seqmagick for sequence manipulation utilities. The installation of both seem a bit convoluted, yet they would do the job. ## installing the packages install.packages(&quot;ggmsa&quot;) install.packages(&quot;seqmagick&quot;) ## load packages including cowplot require(ggmsa) require(seqmagick) require(cowplot) ## you need just to provide the fasta file that contains the alignment from DECIPHER (change path to your computer) ## Notice: I have renamed the taxa to fit on the plot if called AA_alignment &lt;- &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/Ameerega_AA_aligned_end_renamed.fasta&quot; ## we can plot the amino acid sequence from 1 to 300 residues. You can select any section of sites in your alignment. my_aaplot_1 &lt;- ggmsa(msa = AA_alignment, start = 1, end = 100, font = &quot;helvetical&quot;, color = &quot;Chemistry_AA&quot;, char_width = 0.9) my_aaplot_2 &lt;- ggmsa(msa = AA_alignment, start = 101, end = 200, font = &quot;helvetical&quot;, color = &quot;Chemistry_AA&quot;, char_width = 0.9) my_aaplot_3 &lt;- ggmsa(msa = AA_alignment, start = 201, end = 300, font = &quot;helvetical&quot;, color = &quot;Chemistry_AA&quot;, char_width = 0.9) ## we plot all using cowplot plot_grid(my_aaplot_1, my_aaplot_2, my_aaplot_3, ncol = 1) These plots are very demanding on computer memory and slow (not optimal). Here is the alignment visualized. The order of the sequences is the same one as is in the fasta input file. 18) For nucleotide alignment visualization, we can proceed similarly. ## load packages including cowplot require(ggmsa) require(seqmagick) require(cowplot) ## you need just to provide the fasta file that contains the alignment from DECIPHER (change path to your computer) nt_alignment &lt;- &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/Ameerega_aligned_end.fasta&quot; ## we can plot the amino acid sequence from 1 to 600 residues. You can select any section of sites in your alignment. my_aaplot_1 &lt;- ggmsa(msa = nt_alignment, start = 1, end = 200, font = &quot;helvetical&quot;, color = &quot;Shapely_NT&quot;, char_width = 0.3) my_aaplot_2 &lt;- ggmsa(msa = nt_alignment, start = 201, end = 400, font = &quot;helvetical&quot;, color = &quot;Shapely_NT&quot;, char_width = 0.3) my_aaplot_3 &lt;- ggmsa(msa = nt_alignment, start = 401, end = 600, font = &quot;helvetical&quot;, color = &quot;Shapely_NT&quot;, char_width = 0.3) ## we plot all using cowplot plot_grid(my_aaplot_1, my_aaplot_2, my_aaplot_3, ncol = 1) These plots are very demanding on computer memory and slow (not optimal). Here is the alignment visualized. The order of the sequences is the same one as is in the fasta input file. 12.11 Other sequence alignment software 19) Other widely used programs that can do alignments, but they are less efficient are listed below. We will not cover them here, but most require you to input your sequences as Ameerega_COI_seqs_fasta.txt a) Clustal Omega. This tool can run online, and you need just to paste your sequences in fasta format. b) MAFFT. This tool is a good as DECIPHER, it is best run on your computer. However, you can run MAFFT online and you need just to paste your sequences in fasta format. c) MUSCLE. This tool can run online, and you need just to paste your sequences in fasta format. d) PROMALS. This tool can run online, and you need just to paste your sequences in fasta format. "],["estimating-phylogenetic-trees.html", "Session 13 – Estimating Phylogenetic Trees 13.1 Phylogeny inference with iqtree2 13.2 Installing iqtree2 in macOS 13.3 Installing and running iqtree2 in a PC 13.4 Loading the iqtree_runner_R function 13.5 Running iqtree2 from R 13.6 One marker and no partitions 13.7 One marker and with partitions 13.8 Bootstrap support 13.9 Many markers and with partitions 13.10 Other sequence phylogenetic tree estimation software 13.11 Drawing phylogenetic trees with ggtree 13.12 Save ggtree plots as PDF or PNG", " Session 13 – Estimating Phylogenetic Trees Most sequence alignments can be used to estimate phylogenies. These are hypotheses of evolutionary relationships between sequences and the species or populations that they represent. Phylogenies can be estimated from DNA, RNA and amino acid sequences (other features like morphology can also be used). The tree topology provides information about the observed organisms (i.e., source of sequences) at the tips or leaves of the tree, the internal vertices or nodes that represent intermediate ancestors (i.e., likely extinct), and the edges or connecting lines represent evolutionary ancestor-descendant relationships. The value of phylogenies is fundamental in the study biological diversity from taxonomy, epidemiology and population genetics. Data for molecular phylogenies are easy to collect, yet problems derived from homoplasy, reversals and incomplete lineage sorting might affect the reliability of phylogenetic hypotheses. One of Charles Darwin greatest insights was phylogenetic trees that has shape modern biology. 13.1 Phylogeny inference with iqtree2 I do not use R or any R packages to estimate phylogenies, there are so many good programs on their own that can to this better and faster. We can devote many sessions to talk about these software, yet we will concentrate only in iqtree2 that has emerged as a fast and reliable software that also includes molecular model testing and sequence partition. Likewise, iqtree2 has an extensive manual here and it also can be easily manipulated within R. This will also illustrate how R can manage software outside its environment and bring back results to it. 13.2 Installing iqtree2 in macOS 1) We will start with installing iqtree2 and locating the path of the binary file that will be used R to run phylogenetic estimations. a) You need to go first to iqtree2 download website and choose the 64-bit multicore version for your operative system (i.e., macOS, Windows or Linux). Make sure that it is version 2: COVID-19 release 2.1.2 (October 22, 2020). b) This is a zip file and you need to uncompress it and search for the folder with the precompiled version (in masOS is called bin) and copy that file to directly that you will like to store such software. In my case, I named my_programs c) Now, the my_programs folder includes the binary iqtree2 that is the only thing that you might need to run this program. d) For macOS: You can test if this program is working by opening the Terminal on your computer. Most people do not have Terminal in their dock for easy access software. To open Terminal, you need to access your hard drive (click on it). Then, look for Applications folder (click on it). In this folder look for subfolder Utilities. e) For macOS: the Terminal icon is in the Utilities folder and you can create a shortcut by draging Terminal to your dock. However, you do not have to do this to test iqtree2. f) For macOS: You can activate Terminal by double clicking on it and a new window will appear like the one below. g) For macOS: The working directory by default is root. You need to change working directory by typing cd and dragging and dropping the folder that contains iqtree2 (i.e., in my case, the folder my_programs). h) For macOS: After pressing enter, the working directory will be now my_programs. You can type ls and enter. You will get that iqtree2 file is there. i) For macOS: To make sure that iqtree2 will work, you will need to type ./iqtree2 and press enter. j) For macOS: After pressing enter, you should get the following message. NOTICE: For PCs, the process to run iqtree2 outside R are provided in here. 13.3 Installing and running iqtree2 in a PC 2) We will start with installing iqtree2 and locating the path of the binary file that will be used R to run phylogenetic estimations. a) You need to go first to iqtree2 download website and choose the 64-bit multicore version for your operative system (i.e., macOS, Windows or Linux). Make sure that it is version 2: COVID-19 release 2.1.2 (October 22, 2020). b) This is a zip file and you need to uncompress it and search for the folder with the precompiled version (in PC is called bin) and you will see three files: iqtree, iqtree-click and libiomp5md.dll. c) The iqtree_runner_R function DOES NOT run in PCs, but it will give the commands that you need to run this software. d) On the folder that contains your alignment, you need to copy that files iqtree-click and libiomp5md.dll to be able to run the iqtree2 program. This folder should contain the aligned sequence (Session 12) or you can access some these files with aligned data and download them from the GitHub repository for this class (e.g., Ameerega_aligned_end.fasta). e) To activate the interactive iqtree command building tool, you need to double click on the iqtree-click (Notice that windows might prevent you from running so, you will need to allow the program to run). You will get the following window. f) Now, you will interact with this window bulding commands to run iqtree by presing y. g) The first command will be -s for the alignment input and then e to extend. h) We will add to the command line the name of the alignment in the folder Ameerega_aligned_end.fasta for the alignment input and then e to extend. h) We will continue adding to the command line the options for model -m, then e to extend, then MFP+MERGE. i) After we have included all commands to the command line, we will type y to confirm. j) If correct, the program will run and you will have to wait until it finish and then press return to exit. k) In the folder, you will see the output files. The file *.treefile will contain the resulting phylogeny. l) To run other commands in this format (e.g., bootstraps or with partitions), you need to run iqtree_runner_R first to get all the necessary input commands. They will look as follows on the first few lines of the iqtree_runner_R function output (even if it does not run): ## this is an example of the commands that you need to input in the format for PCs [1] &quot;iqtree2 -s Ameerega_aligned_end.fasta -m MFP+MERGE &quot; 13.4 Loading the iqtree_runner_R function 3) To run software outside R requires several components and scripting outside this workshop, I will not go in detail on these other than provide you with a function that will allow you to run our aligned sequence matrices (Session 12) or you can access some these files with aligned data on the GitHub repository for this class (e.g., Ameerega_aligned_end.fasta). The function provided below allows to run iqtree2 and importing back the tree and its nodal support after the analyses. However, it only works in macOS and for PC users please follow instructions on 2). If you have questions about the iqtree_runner_R function below, please contact me for more details. You need to install two packages doParallel and ape before we run iqtree_runner_R. We will not get into details about doParallel, but we will use ape for other phylogenetic analyses. ## it requires to install these R packages install.packages(&quot;doParallel&quot;) install.packages(&quot;ape&quot;) Please copy an paste the text below for iqtree_runner_R in R. This function is to run iqtree2 from your computer, but it has been used in macOS. We will need to test for PCs ## make sure that you have the required packages require(doParallel) require(ape) ## iqtree_runner function copy text from here until END OF FUNCTION (find this below) iqtree_runner_R &lt;- function(input_alignment_file_user, iqtree_bin_location_user, run_iqtree_call_user = c(&quot;serial&quot;,&quot;parallel&quot;), iqtree_quiet_user = FALSE, iqtree_redo_user = FALSE, iqtree_prefix_user = NULL, iqtree_input_model_m_user = NULL, iqtree_mtree_user = FALSE, iqtree_sequence_type_st_user = NULL, iqtree_UFBoot_B_user = NULL, iqtree_UFBoot_bnni_user = TRUE, iqtree_nonpara_boot_b_user = NULL, iqtree_SHaLRT_alrt_user = NULL, iqtree_multicore_T_user = NULL, iqtree_maxcores_ntmax_user = NULL, iqtree_partition_user = NULL, iqtree_partition_if_by_position_user = TRUE, iqtree_partition_file_user = NULL, iqtree_chronogram_from_tree_file_user = NULL, iqtree_chronogram_mrca_bounds_user = NULL, iqtree_date_file_user = NULL, iqtree_pass_parameters_user = NULL){ ## require libraries require(doParallel) require(ape) ############# input from user ## sequence and tree arguments input_files &lt;- input_alignment_file_user # iqtree location iqtree_bin_location &lt;- iqtree_bin_location_user # serial or parallel calls run_iqtree &lt;- run_iqtree_call_user iqtree_quiet &lt;- iqtree_quiet_user ####### basic iqtree parameters # overwrite all previous output: -redo iqtree_redo &lt;- iqtree_redo_user # add prefi: --prefix iqtree_prefix &lt;- iqtree_prefix_user # add model: -m iqtree_model &lt;- iqtree_input_model_m_user # increase accuracy: -mtree iqtree_mtree &lt;- iqtree_mtree_user # Specify sequence type as either of NULL (autodetect) DNA, AA, BIN, MORPH, CODON or NT2AA for DNA iqtree_st &lt;- iqtree_sequence_type_st_user # UFBoot mode: -B 1000 iqtree_UFBoot_B &lt;- iqtree_UFBoot_B_user # Red_impact_UFBoot iqtree_UFBoot_bnni &lt;- iqtree_UFBoot_bnni_user # nonparametric bootstrap -b 100 iqtree_nonpara_b &lt;- iqtree_nonpara_boot_b_user # Assessing branch supports with single branch tests SH-like approximate likelihood ratio test: -alrt 1000 iqtree_alrt &lt;- iqtree_SHaLRT_alrt_user # multicore -T AUTO -T 2 iqtree_multicore_T &lt;- iqtree_multicore_T_user # maxcores -ntmax iqtree_maxcores_ntmax &lt;- iqtree_maxcores_ntmax_user ############## iqtree_partition_user iqtree_partition &lt;- iqtree_partition_user do_by_codon_pos &lt;- iqtree_partition_if_by_position_user if(!is.null(iqtree_partition)) { iqtree_partition_matrix &lt;- do.call(rbind,iqtree_partition) partitions_names &lt;- rownames(iqtree_partition_matrix) type_partitions &lt;- iqtree_partition_matrix[,1] start_part &lt;- as.numeric(iqtree_partition_matrix[,2]) end_part &lt;- as.numeric(iqtree_partition_matrix[,3]) codon_part &lt;- iqtree_partition_matrix[,4] iqtree_partition_df &lt;- data.frame(partition_name = partitions_names, type = type_partitions, start = start_part, end = end_part, codon = codon_part, stringsAsFactors = FALSE) rownames(iqtree_partition_df) &lt;- NULL # output file name out_part_file &lt;- sub(&quot;.*/&quot;, &quot;&quot;, input_files) out_part_file &lt;- sub(&quot;[.]&quot;, &quot;_&quot;, out_part_file) out_part_file &lt;- paste0(out_part_file,&quot;_partition.txt&quot;) for(i in 1:nrow(iqtree_partition_df)) { # i &lt;- 3 one_line &lt;- paste0(iqtree_partition_df$type[i], &quot;, &quot;, iqtree_partition_df$partition_name[i], &quot; = &quot;, iqtree_partition_df$start[i], &quot;-&quot;, iqtree_partition_df$end[i]) if(!is.na(iqtree_partition_df$codon[i])) { # do_by_codon_pos if(do_by_codon_pos) { pos1_start &lt;- iqtree_partition_df$start[i] pos1_name &lt;- paste0(iqtree_partition_df$partition_name[i],&quot;_pos1&quot;) pos2_start &lt;- iqtree_partition_df$start[i]+1 pos2_name &lt;- paste0(iqtree_partition_df$partition_name[i],&quot;_pos2&quot;) pos3_start &lt;- iqtree_partition_df$start[i]+2 pos3_name &lt;- paste0(iqtree_partition_df$partition_name[i],&quot;_pos3&quot;) one_line &lt;- paste0(iqtree_partition_df$type[i], &quot;, &quot;, pos1_name, &quot; = &quot;, pos1_start, &quot;-&quot;, iqtree_partition_df$end[i], &quot;\\\\&quot;, iqtree_partition_df$codon[i], &quot;\\n&quot;, iqtree_partition_df$type[i], &quot;, &quot;, pos2_name, &quot; = &quot;, pos2_start, &quot;-&quot;, iqtree_partition_df$end[i], &quot;\\\\&quot;, iqtree_partition_df$codon[i], &quot;\\n&quot;, iqtree_partition_df$type[i], &quot;, &quot;, pos3_name, &quot; = &quot;, pos3_start, &quot;-&quot;, iqtree_partition_df$end[i], &quot;\\\\&quot;, iqtree_partition_df$codon[i], &quot;\\n&quot;) } } # one_line &lt;- paste0(one_line, &quot;\\\\&quot;, iqtree_partition_df$codon[i]) } cat(one_line,file=out_part_file,sep=&quot;\\n&quot;,append=TRUE) } rm(i) iqtree_partition_files &lt;- out_part_file } else { iqtree_partition_files &lt;- iqtree_partition_file_user } ############## chronogram_functions iqtree_chronogram_from_tree &lt;- iqtree_chronogram_from_tree_file_user iqtree_mrca_bounds &lt;- iqtree_chronogram_mrca_bounds_user if(!is.null(iqtree_mrca_bounds)) { # name for DATE_FILE out_part_file &lt;- sub(&quot;.*/&quot;, &quot;&quot;, input_files) out_part_file &lt;- sub(&quot;[.]&quot;, &quot;_&quot;, out_part_file) out_date_file &lt;- paste0(out_part_file,&quot;_dates.txt&quot;) # build DATE_FILE n_total &lt;- length(iqtree_mrca_bounds) n_constraints &lt;- seq(1,by=2, to=n_total) for(i in n_constraints) { # i &lt;- 1 taxa_part &lt;- paste0(iqtree_mrca_bounds[[i]], collapse=&quot;,&quot;) date_part &lt;- paste0(&quot;-&quot;, iqtree_mrca_bounds[[i+1]]) one_line &lt;- paste0(taxa_part, &quot; &quot;, date_part) # one_line &lt;- paste0(one_line, &quot;\\\\&quot;, iqtree_partition_df$codon[i]) } cat(one_line,file=out_date_file,sep=&quot;\\n&quot;,append=TRUE) } rm(i) iqtree_date_files &lt;- out_date_file } else { iqtree_date_files &lt;- iqtree_date_file_user } # iq pass paramters iqtree_pass &lt;- iqtree_pass_parameters_user # get work directory to go back master_wd_directory &lt;- getwd() ## get iqtree path path_of_iqtree &lt;- paste0(iqtree_bin_location) ###### prepare iqtree commands list_iqtree_scripts &lt;- list() name_of_analyses_iqtree &lt;- character() counter &lt;- 0 for(i in 1:length(input_files)) { # i &lt;- 1 one_alignment_file &lt;- input_files[i] counter &lt;- counter + 1 # simplify names one_input_file &lt;- sub(&quot;.*/&quot;, &quot;&quot;, one_alignment_file) # prepare commands line_iqtree &lt;- paste0(&quot;iqtree2 -s &quot;, one_input_file, &quot; &quot;) # list of iqtre parameters list_of_iqtree &lt;- c(line_iqtree) counter_2 &lt;- 1 # Specify sequence type as either of NULL (autodetect) DNA, AA, BIN, MORPH, CODON or NT2AA for DNA if(!is.null(iqtree_st)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -st &quot;, iqtree_st, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } # add prefi: --prefix if(!is.null(iqtree_prefix)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; --prefix &quot;, iqtree_prefix, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } # add model: -m if(!is.null(iqtree_model)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -m &quot;, iqtree_model, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } # increase accuracy: -mtree if(iqtree_mtree) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -mtree &quot;) list_of_iqtree[counter_2] &lt;- out_line } # overwrite all previous output: -redo if(iqtree_redo) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -redo &quot;) list_of_iqtree[counter_2] &lt;- out_line } # add partition if(!is.null(iqtree_partition_files)) {counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -p &quot;, iqtree_partition_files[i]) list_of_iqtree[counter_2] &lt;- out_line } # add DATE_FILE if(is.null(iqtree_chronogram_from_tree)) { if(!is.null(iqtree_date_files)) {counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; --date &quot;, iqtree_date_files[i], &#39; --date-tip 0&#39;) list_of_iqtree[counter_2] &lt;- out_line } } # make chronogram from tree if(!is.null(iqtree_chronogram_from_tree)) {counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; --date &quot;, iqtree_date_files[i], &#39; --date-tip 0&#39;, &quot; -te &quot;, iqtree_chronogram_from_tree) list_of_iqtree[counter_2] &lt;- out_line } ##### ultra fast bootstrap # UFBoot mode: -B 1000 if(!is.null(iqtree_UFBoot_B)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -B &quot;, iqtree_UFBoot_B, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } # Red_impact_UFBoot if(!is.null(iqtree_UFBoot_B)) { if(iqtree_UFBoot_bnni) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -bnni &quot;) list_of_iqtree[counter_2] &lt;- out_line } } ##### non-parameteric bootstrap # nonparametric bootstrap -b 100 if(!is.null(iqtree_nonpara_b)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -b &quot;, iqtree_nonpara_b, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } ##### SH-like approximate likelihood ratio test # Assessing branch supports with single branch tests : -alrt 1000 if(!is.null(iqtree_alrt)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -alrt &quot;, iqtree_alrt, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } ### multicore # multicore -T AUTO -T 2 if(!is.null(iqtree_multicore_T)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -T &quot;, iqtree_multicore_T, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } if(!is.null(iqtree_multicore_T)) { if(!is.null(iqtree_maxcores_ntmax)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -ntmax &quot;, iqtree_maxcores_ntmax, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } } ### iqtree pass paramters if(!is.null(iqtree_pass)) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; &quot;, iqtree_pass, &quot; &quot;) list_of_iqtree[counter_2] &lt;- out_line } # quiet if(iqtree_quiet) { counter_2 &lt;- counter_2 + 1 out_line &lt;- paste0(&quot; -quiet &quot;) list_of_iqtree[counter_2] &lt;- out_line } ### all lines together iqtree_out &lt;- paste0(list_of_iqtree, collapse = &quot; &quot;) print(iqtree_out) list_iqtree_scripts[[counter]] &lt;- iqtree_out name_of_analyses_iqtree[counter] &lt;- one_input_file } rm(i) if(exists(&quot;counter&quot;)){rm(counter)} if(exists(&quot;counter_2&quot;)){rm(counter_2)} ##################################################################################### #### open and run iqtree from R # create vector to export path -- export PATH=/opt/local/bin:/opt/local/sbin:$PATH export_path_iqtree_bins &lt;- normalizePath(iqtree_bin_location) export_path_iqtree_vector &lt;- paste0(&quot;export PATH=&quot;,export_path_iqtree_bins,&quot;:$PATH&quot;) ## iqtree execute function execute_iqtree_terminal &lt;- function(export_path_section = export_path_iqtree_vector, iqtree_command, intern = FALSE, wait = FALSE){ system(paste0(export_path_section, &quot;\\n&quot;, iqtree_command, &quot;\\n&quot;), intern = intern, wait = wait) } ## run list elements with iqtree commands ## serial # run_iqtree &lt;- &quot;serial&quot; if(run_iqtree == &quot;serial&quot;) { cat(&quot;\\n****** started serial analisis of: &quot;, length(list_iqtree_scripts), &quot;\\n&quot; ) for(j in 1:length(list_iqtree_scripts)) { cat(&quot;***** iqtree analysis name: &quot;,name_of_analyses_iqtree[j], &quot;\\n&quot;) cat(&quot;***** iqtree commands: &quot;, list_iqtree_scripts[[j]], &quot;\\n&quot;) execute_iqtree_terminal(export_path_section = export_path_iqtree_vector, iqtree_command = list_iqtree_scripts[[j]], intern = FALSE, wait = TRUE) cat(&quot;***** DONE ***** \\n&quot;) } rm(j) } ### parallel # run_iqtree &lt;- &quot;parallel&quot; if(run_iqtree == &quot;parallel&quot;) { cat(&quot;\\n****** started parallel analisis of: &quot;, length(list_iqtree_scripts), &quot;\\n&quot; ) print(name_of_analyses_iqtree) ncores &lt;- parallel::detectCores() cl &lt;- makeCluster(ncores) registerDoParallel(cl) foreach(iqtree_i=list_iqtree_scripts, .export = &quot;execute_iqtree_terminal&quot;) %dopar% {execute_iqtree_terminal(iqtree_command=iqtree_i)} parallel::stopCluster(cl) cat(&quot;\\n ****** DONE ****** \\n&quot;) } ### to return to user grab treefile iqtrees_vector &lt;- list.files(pattern = &quot;\\\\.treefile$&quot;, ignore.case=TRUE) list_of_trees &lt;- list() for(i in 1:length(iqtrees_vector)) {list_of_trees[[i]] &lt;- ape::read.tree(iqtrees_vector[i])} rm(i) ### if chronograms iqtree_chronograms &lt;- list.files(pattern = &quot;\\\\.timetree.nex$&quot;, ignore.case=TRUE) if(length(iqtree_chronograms) &gt; 0) { for(i in 1:length(iqtree_chronograms)) { one_chronogram &lt;- ape::read.nexus(iqtree_chronograms[i]) n_trees &lt;- length(list_of_trees) list_of_trees[[n_trees+1]] &lt;- one_chronogram } rm(i) } ############ open and run iqtree from R: DONE setwd(master_wd_directory) cat(&quot;\\n\\n ************* iqtree outfiles written to ************* \\n&quot;) print(master_wd_directory) return(list_of_trees) } ## END OF FUNCTION After you have copied and pasted the function iqtree_runner_R(), you will not get any result yet. However, the function will be in memory ready to run. 13.5 Running iqtree2 from R After you have loaded the function iqtree_runner_R(), we will perform several analyses. IMPORTANT Your alignment file (fasta or nexus) cannot have taxa, terminals or sequences with the same name; if you want to run including such sequences make sure that you label them accordingly. 13.6 One marker and no partitions In most cases, you just want a phylogenetic tree estimated from your sequences. This makes iqtree2 extremely powerful as it can automatically select molecular model that best fits your data (i.e., ModelFinder section). 4) We will start with the most simple using the COI aligned sequences in the file Ameerega_aligned_end.fasta. a) Set up your working directory (e.g., my_phylogeny_1) and get its path. ## this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory/my_phylogeny_1&quot;) b) find the file with aligned sequences: Ameerega_aligned_end.fasta and copy that file to your working directory (e.g., my_phylogeny_1) ## copy the file Ameerega_aligned_end.fasta to your working directory my_phylogeny_1 ## this is the path to that file and exclusive to your OWN COMPUTER change it accordingly my_path_to_aligned_sequences &lt;- &quot;~/Desktop/Teach_R/my_working_directory/my_phylogeny_1/Ameerega_aligned_end.fasta&quot; c) find the path to the directory that contain the binary of iqtree2. Note that this is path to the directory, NOT the binary itself (check the syntax below). ## macOS: if you drag and drop the binary iqtree2 ~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_programs/iqtree2 ## this is exclusive to your OWN COMPUTER and we just need the path to the directory that contains iqtree2 ## Note: the diffecent from path above and the one below. my_path_to_iqtree2 &lt;- &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_programs&quot; d) Now, we can run the function iqtree_runner_R(). ## SIMPLE iqtree2 run COI_simple_phylogeny &lt;- iqtree_runner_R (input_alignment_file_user = my_path_to_aligned_sequences, iqtree_bin_location_user = my_path_to_iqtree2, run_iqtree_call_user = &quot;serial&quot;, iqtree_input_model_m_user = &quot;MFP+MERGE&quot;) ## you will get this output on the console # [1] &quot;iqtree2 -s Ameerega_aligned_end.fasta -m MFP+MERGE &quot; # # ****** started serial analisis of: 1 # ***** iqtree analysis name: Ameerega_aligned_end.fasta # ***** iqtree commands: iqtree2 -s Ameerega_aligned_end.fasta -m MFP+MERGE # IQ-TREE multicore version 2.1.2 COVID-edition for Mac OS X 64-bit built Oct 22 2020 # Developed by Bui Quang Minh, James Barbetti, Nguyen Lam Tung, # Olga Chernomor, Heiko Schmidt, Dominik Schrempf, Michael Woodhams. # # Host: LFQSANTOSJ2016 (AVX2, FMA3, 8 GB RAM) # Command: iqtree2 -s Ameerega_aligned_end.fasta -m MFP+MERGE # Seed: 487027 (Using SPRNG - Scalable Parallel Random Number Generator) # Time: Tue Feb 23 17:46:25 2021 # Kernel: AVX+FMA - 1 threads (4 CPU cores detected) # # HINT: Use -nt option to specify number of threads because your CPU has 4 cores! # HINT: -nt AUTO will automatically determine the best number of threads to use. # # Reading alignment file Ameerega_aligned_end.fasta ... Fasta format detected # Alignment most likely contains DNA/RNA sequences # Alignment has 24 sequences with 1539 columns, 452 distinct patterns # 378 parsimony-informative, 140 singleton sites, 1021 constant sites # Gap/Ambiguity Composition p-value # 1 MW042032.1 0.00% passed 91.59% # 2 MW042031.1 0.00% passed 61.48% # 3 MW042030.1 0.00% passed 81.11% # 4 KU494334.1 58.02% passed 72.66% # 5 KU494333.1 57.96% passed 99.83% # 6 KU494332.1 57.96% passed 99.83% # 7 AF097506.1 66.21% passed 51.07% # 8 DQ502932.1 57.24% passed 90.55% # 9 DQ502929.1 57.24% passed 55.70% # 10 DQ502922.1 57.24% passed 84.11% # 11 DQ502920.1 57.24% passed 33.21% # 12 DQ502903.1 57.24% passed 51.16% # 13 DQ502902.1 57.24% passed 65.03% # 14 DQ502901.1 57.24% passed 48.46% # 15 DQ502851.1 57.24% passed 94.07% # 16 DQ502849.1 57.24% passed 33.21% # 17 DQ502848.1 57.24% passed 29.31% # 18 DQ502832.1 57.31% passed 91.59% # 19 DQ502831.1 57.24% passed 93.67% # 20 DQ502825.1 57.24% passed 60.02% # 21 MW042037.1 0.00% passed 77.97% # 22 KR862889.1 57.50% passed 41.02% # 23 MW042036.1 0.00% failed 0.13% # 24 MW042039.1 0.00% failed 0.57% # WARNING: 18 sequences contain more than 50% gaps/ambiguity # **** TOTAL 43.41% 2 sequences failed composition chi2 test (p-value&lt;5%; df=3) # NOTE: KU494332.1 is identical to KU494333.1 but kept for subsequent analysis # # # Create initial parsimony tree by phylogenetic likelihood library (PLL)... 0.003 seconds # Perform fast likelihood tree search using GTR+I+G model... # Estimate model parameters (epsilon = 5.000) # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # Perform nearest neighbor interchange... # Estimate model parameters (epsilon = 1.000) # 1. Initial log-likelihood: -7191.031 # Optimal log-likelihood: -7189.967 # Rate parameters: A-C: 7.98907 A-G: 64.12175 A-T: 11.66195 C-G: 1.43333 C-T: 100.00000 G-T: 1.00000 # Base frequencies: A: 0.240 C: 0.266 G: 0.172 T: 0.321 # Proportion of invariable sites: 0.357 # Gamma shape alpha: 0.388 # Parameters optimization took 1 rounds (0.044 sec) # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # Time for fast ML tree search: 0.322 seconds # # NOTE: ModelFinder requires 3 MB RAM! # ModelFinder will test up to 286 DNA models (sample size: 1539) ... # No. Model -LnL df AIC AICc BIC # 1 GTR+F 7870.984 53 15847.967 15851.822 16130.928 # 2 GTR+F+I 7216.150 54 14540.301 14544.303 14828.601 # 3 GTR+F+G4 7199.116 54 14506.232 14510.235 14794.532 # 4 GTR+F+I+G4 7176.558 55 14463.117 14467.270 14756.755 # 5 GTR+F+R2 7295.486 55 14700.971 14705.125 14994.610 # 6 GTR+F+R3 7242.022 57 14598.044 14602.509 14902.361 # 7 GTR+F+R4 7215.932 59 14549.863 14554.650 14864.858 # 8 GTR+F+R5 7202.474 61 14526.949 14532.070 14852.621 # 9 GTR+F+R6 7195.140 63 14516.279 14521.746 14852.629 # 17 SYM+I+G4 7230.681 52 14565.361 14569.071 14842.984 # 30 TVM+F+I+G4 7179.891 54 14467.782 14471.784 14756.081 # 43 TVMe+I+G4 7265.512 51 14633.024 14636.590 14905.307 # 56 TIM3+F+I+G4 7203.542 53 14513.085 14516.939 14796.046 # 69 TIM3e+I+G4 7284.220 50 14668.440 14671.867 14935.384 # 82 TIM2+F+I+G4 7177.515 53 14461.029 14464.884 14743.990 # 95 TIM2e+I+G4 7233.907 50 14567.815 14571.242 14834.759 # 108 TIM+F+I+G4 7202.565 53 14511.129 14514.984 14794.090 # 121 TIMe+I+G4 7283.344 50 14666.689 14670.116 14933.633 # 134 TPM3u+F+I+G4 7205.716 52 14515.433 14519.142 14793.055 # 147 TPM3+F+I+G4 7205.716 52 14515.433 14519.142 14793.055 # 160 TPM2u+F+I+G4 7181.001 52 14466.003 14469.712 14743.625 # 173 TPM2+F+I+G4 7180.996 52 14465.992 14469.702 14743.614 # 186 K3Pu+F+I+G4 7204.884 52 14513.768 14517.477 14791.390 # 199 K3P+I+G4 7315.342 49 14728.684 14731.975 14990.290 # 212 TN+F+I+G4 7204.352 52 14512.704 14516.413 14790.326 # 225 TNe+I+G4 7286.452 49 14670.904 14674.195 14932.510 # 238 HKY+F+I+G4 7206.672 51 14515.343 14518.910 14787.627 # 251 K2P+I+G4 7318.616 48 14733.231 14736.388 14989.498 # 264 F81+F+I+G4 7804.765 50 15709.531 15712.958 15976.475 # 277 JC+I+G4 7964.515 47 16023.030 16026.056 16273.957 # Akaike Information Criterion: TIM2+F+I+G4 # Corrected Akaike Information Criterion: TIM2+F+I+G4 # Bayesian Information Criterion: TPM2+F+I+G4 # Best-fit model: TPM2+F+I+G4 chosen according to BIC # # All model information printed to Ameerega_aligned_end.fasta.model.gz # CPU time for ModelFinder: 6.355 seconds (0h:0m:6s) # Wall-clock time for ModelFinder: 6.536 seconds (0h:0m:6s) # # NOTE: 1 MB RAM (0 GB) is required! # Estimate model parameters (epsilon = 0.100) # Thoroughly optimizing +I+G parameters from 10 start values... # Init pinv, alpha: 0.000, 1.370 / Estimate: 0.000, 0.166 / LogL: -7202.724 # Init pinv, alpha: 0.074, 1.370 / Estimate: 0.071, 0.192 / LogL: -7203.417 # Init pinv, alpha: 0.147, 1.370 / Estimate: 0.149, 0.227 / LogL: -7203.178 # Init pinv, alpha: 0.221, 1.370 / Estimate: 0.592, 1.298 / LogL: -7181.054 # Init pinv, alpha: 0.295, 1.370 / Estimate: 0.593, 1.312 / LogL: -7181.038 # Init pinv, alpha: 0.369, 1.370 / Estimate: 0.592, 1.300 / LogL: -7181.052 # Init pinv, alpha: 0.442, 1.370 / Estimate: 0.593, 1.311 / LogL: -7181.038 # Init pinv, alpha: 0.516, 1.370 / Estimate: 0.593, 1.314 / LogL: -7181.034 # Init pinv, alpha: 0.590, 1.370 / Estimate: 0.593, 1.324 / LogL: -7181.039 # Init pinv, alpha: 0.663, 1.370 / Estimate: 0.603, 1.476 / LogL: -7181.034 # Optimal pinv,alpha: 0.593, 1.314 / LogL: -7181.034 # # Parameters optimization took 2.390 sec # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # Computing ML distances based on estimated model parameters... # Computing ML distances took 0.013737 sec (of wall-clock time) 0.013278 sec(of CPU time) # Computing RapidNJ tree took 0.000465 sec (of wall-clock time) 0.000218 sec (of CPU time) # Log-likelihood of RapidNJ tree: -7193.279 # -------------------------------------------------------------------- # | INITIALIZING CANDIDATE TREE SET | # -------------------------------------------------------------------- # Generating 98 parsimony trees... 0.182 second # Computing log-likelihood of 98 initial trees ... 0.260 seconds # Current best score: -7181.034 # # Do NNI search on 20 best initial trees # Estimate model parameters (epsilon = 0.100) # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # BETTER TREE FOUND at iteration 1: -7181.009 # Estimate model parameters (epsilon = 0.100) # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # BETTER TREE FOUND at iteration 3: -7179.089 # BETTER TREE FOUND at iteration 6: -7179.089 # Estimate model parameters (epsilon = 0.100) # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # BETTER TREE FOUND at iteration 7: -7176.557 # Iteration 10 / LogL: -7178.385 / Time: 0h:0m:3s # Estimate model parameters (epsilon = 0.100) # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # BETTER TREE FOUND at iteration 11: -7176.259 # Estimate model parameters (epsilon = 0.100) # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # BETTER TREE FOUND at iteration 13: -7175.959 # Iteration 20 / LogL: -7176.267 / Time: 0h:0m:3s # Finish initializing candidate tree set (16) # Current best tree score: -7175.959 / CPU time: 1.545 # Number of iterations: 20 # -------------------------------------------------------------------- # | OPTIMIZING CANDIDATE TREE SET | # -------------------------------------------------------------------- # UPDATE BEST LOG-LIKELIHOOD: -7175.952 # Iteration 30 / LogL: -7179.103 / Time: 0h:0m:4s (0h:0m:12s left) # Iteration 40 / LogL: -7176.262 / Time: 0h:0m:5s (0h:0m:9s left) # Iteration 50 / LogL: -7175.952 / Time: 0h:0m:5s (0h:0m:7s left) # Iteration 60 / LogL: -7176.560 / Time: 0h:0m:6s (0h:0m:5s left) # Iteration 70 / LogL: -7180.060 / Time: 0h:0m:6s (0h:0m:4s left) # Iteration 80 / LogL: -7176.560 / Time: 0h:0m:7s (0h:0m:3s left) # Iteration 90 / LogL: -7176.793 / Time: 0h:0m:7s (0h:0m:2s left) # UPDATE BEST LOG-LIKELIHOOD: -7175.950 # UPDATE BEST LOG-LIKELIHOOD: -7175.950 # Iteration 100 / LogL: -7175.950 / Time: 0h:0m:8s (0h:0m:1s left) # Iteration 110 / LogL: -7176.560 / Time: 0h:0m:8s (0h:0m:0s left) # TREE SEARCH COMPLETED AFTER 114 ITERATIONS / Time: 0h:0m:9s # # -------------------------------------------------------------------- # | FINALIZING TREE SEARCH | # -------------------------------------------------------------------- # Performs final model parameters optimization # Estimate model parameters (epsilon = 0.010) # 1. Initial log-likelihood: -7175.950 # Optimal log-likelihood: -7175.948 # Rate parameters: A-C: 11.77863 A-G: 100.00000 A-T: 11.77863 C-G: 1.00000 C-T: 100.00000 G-T: 1.00000 # Base frequencies: A: 0.240 C: 0.266 G: 0.172 T: 0.321 # Proportion of invariable sites: 0.596 # Gamma shape alpha: 1.312 # Parameters optimization took 1 rounds (0.011 sec) # # WARNING: Estimated model parameters are at boundary that can cause numerical instability! # # BEST SCORE FOUND : -7175.948 # Total tree length: 2.452 # # Total number of iterations: 114 # CPU time used for tree search: 6.683 sec (0h:0m:6s) # Wall-clock time used for tree search: 6.730 sec (0h:0m:6s) # Total CPU time used: 9.078 sec (0h:0m:9s) # Total wall-clock time used: 9.182 sec (0h:0m:9s) # # Analysis results written to: # IQ-TREE report: Ameerega_aligned_end.fasta.iqtree # Maximum-likelihood tree: Ameerega_aligned_end.fasta.treefile # Likelihood distances: Ameerega_aligned_end.fasta.mldist # Screen log file: Ameerega_aligned_end.fasta.log # # Date and Time: Tue Feb 23 17:46:41 2021 # #***** DONE ***** # # # # ************* iqtree outfiles written to ************* # #[1] &quot;/Users/santosj/Desktop/Teach_R/my_working_directory/my_phylogeny_1&quot; The following files will be now present in your working directory. e) You can visualize the resulting phylogenetic tree as follows. ## content of output object. COI_simple_phylogeny #[[1]] # #Phylogenetic tree with 24 tips and 22 internal nodes. # #Tip labels: # MW042032.1, MW042031.1, DQ502932.1, DQ502902.1, DQ502922.1, DQ502929.1, ... # #Unrooted; includes branch lengths. COI_simple_phylogeny[[1]] #Phylogenetic tree with 24 tips and 22 internal nodes. # #Tip labels: # MW042032.1, MW042031.1, DQ502932.1, DQ502902.1, DQ502922.1, DQ502929.1, ... # #Unrooted; includes branch lengths. ## you can plot this tree using ape plot(COI_simple_phylogeny[[1]]) The actual phylogenetic tree is in the file Ameerega_aligned_end.fasta.treefile in our working folder. 13.7 One marker and with partitions You provide a partition for your sequences based on information about it. For example, you can divide a protein coding gene in three sections based on the codon position. As it has been demonstrated, the codon code is redundant and variations in 3rd positions usually result in the same amino acid residue after translation, for more information see here. In our example, we can split our split our COI matrix by codon position. 5) We will use the COI aligned sequences in the file Ameerega_aligned_end.fasta. Notice the argument iqtree_partition_user = list(COI = c(\"DNA\",1,1539,3)). We define it a DNA segment, starting on position 1, ending on position 1539 (this number can be find in Session 11.7 or just by counting the number of bases on the aligned matrix) and 3 that means to divide it by 3 for 1st, 2nd, and 3rd codon positions. ## set up a new working directory setwd(&quot;~/Desktop/Teach_R/my_working_directory/my_phylogeny_2&quot;) ## copy as paste again the sequence data ## update the my_path_to_aligned_sequences2 &lt;- &quot;~/Desktop/Teach_R/my_working_directory/my_phylogeny_2/Ameerega_aligned_end.fasta&quot; ## iqtree2 run with defined codon positions COI_with_partitions_phylogeny &lt;- iqtree_runner_R (input_alignment_file_user = my_path_to_aligned_sequences2, iqtree_bin_location_user = my_path_to_iqtree2, run_iqtree_call_user = &quot;serial&quot;, iqtree_input_model_m_user = &quot;MFP+MERGE&quot;, iqtree_partition_user = list(COI = c(&quot;DNA&quot;,1,1539,3)), iqtree_partition_if_by_position_user = TRUE) ## see and plot the new tree COI_with_partitions_phylogeny[[1]] #Phylogenetic tree with 24 tips and 22 internal nodes. # #Tip labels: # MW042032.1, MW042031.1, DQ502932.1, DQ502902.1, DQ502922.1, DQ502929.1, ... # #Unrooted; includes branch lengths. plot(COI_with_partitions_phylogeny[[1]]) The actual phylogenetic tree is in the file Ameerega_aligned_end_fasta_partition.txt.treefile in our working folder. 13.8 Bootstrap support 6) In most cases, you also want to know how reliable is your phylogenetic reconstruction. This is determined by the statistical support of the nodes (the internal junctions of lines). These are usually evaluated over 100% based on the type of the approach used by iqtree2, which means that such phylogenetic relationships are very reliable. If values are less than 75%, your confidence drops significantly and usually means conflicting hypotheses about the evolutionary relationships for that node. We will use the COI aligned sequences in the file Ameerega_aligned_end.fasta. Notice the arguments iqtree_UFBoot_B_user = 1000 and iqtree_UFBoot_bnni_user = TRUE. This will ask for 1000 non-parametric bootstraps with corrections, for details see here. ## set up a new working directory setwd(&quot;~/Desktop/Teach_R/my_working_directory/my_phylogeny_3&quot;) ## copy as paste again the sequence data ## update the my_path_to_aligned_sequences3 &lt;- &quot;~/Desktop/Teach_R/my_working_directory/my_phylogeny_3/Ameerega_aligned_end.fasta&quot; ## iqtree2 run with defined codon positions and get 1000 bootstraps COI_with_partitions_phylogeny_boot &lt;- iqtree_runner_R (input_alignment_file_user = my_path_to_aligned_sequences3, iqtree_bin_location_user = my_path_to_iqtree2, run_iqtree_call_user = &quot;serial&quot;, iqtree_input_model_m_user = &quot;GTR+F+I+G4&quot;, iqtree_UFBoot_B_user = 1000, iqtree_UFBoot_bnni_user = TRUE, iqtree_partition_user = list(COI = c(&quot;DNA&quot;,1,1539,3)), iqtree_partition_if_by_position_user = TRUE) The actual collection of bootstrap trees is in the file Ameerega_aligned_end_fasta_partition.txt.ufboot and its consensus is in the file Ameerega_aligned_end_fasta_partition.txt.treefile in our working folder. 7) I could not find an easy way to visualize these threes other than use the other program to visualize phylogenetic trees in general. You need to install figtree. To visualize your consensus tree after installing figtree you need open this program. The File&gt;Open... find the bootstrap summary file Ameerega_aligned_end_fasta_partition.txt.treefile and allow to see labels. You will see your tree, but without the nodal support. You will clikc on Nodel Labels and then on label. This will show you the bootstraps for your tree. 13.9 Many markers and with partitions 8) A phylogeny derived from two or more markers require some modifications to indicate the start and end of each marker. We start by defining the output directory. ## set up a new working directory by creating one; this is exclusive to your OWN COMPUTER change it accordingly setwd(&quot;~/Desktop/Teach_R/my_working_directory/my_phylogeny_4&quot;) We find the file with aligned sequences: my_collected_COI_CYTB_String_path_fasta and copy that file to your working directory (e.g., my_phylogeny_4) ## copy the file my_collected_COI_CYTB_String_path_fasta to your working directory my_phylogeny_4 ## this is the path to that file and exclusive to your OWN COMPUTER change it accordingly my_path_to_aligned_COI_CYTB_sequences &lt;- &quot;~/Desktop/Teach_R/my_working_directory/my_phylogeny_4/my_collected_COI_CYTB_String_path_fasta.txt&quot; We run iqtree2 and notice the iqtree_partition_user argument that defines the COI and CYTB markers. We define both as DNA segments. For COI, we define the starting on position of COI at 1 and the ending on position at 1539 (this number can be find in Session 11.7 or just by counting the number of bases on the aligned matrix). For CYTB, we check first the that the length of this maker is 944 (see Session 11.9) and we concatenated this maker after COI, then we add 1539 + 944 = 2483. However, the CYTB gene does not start on base 1539 but on 1540, so we add 1. Therefore, we define the starting on position of CYTB at 1540 and the ending on position at 2483. Finally, the 3 on the iqtree_partition_user argument means to divide it by 3 for 1st, 2nd, and 3rd codon positions. ## this is exclusive to your OWN COMPUTER and we just need the path to the directory that contains iqtree2 ## Note: the diffecent from path above and the one below. my_path_to_iqtree2 &lt;- &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_programs&quot; ## iqtree2 run with defined codon positions COI_CYTB_with_partitions_phylogeny &lt;- iqtree_runner_R (input_alignment_file_user = my_path_to_aligned_COI_CYTB_sequences, iqtree_bin_location_user = my_path_to_iqtree2, run_iqtree_call_user = &quot;serial&quot;, iqtree_input_model_m_user = &quot;MFP+MERGE&quot;, iqtree_partition_user = list(COI = c(&quot;DNA&quot;,1,1539,3), CYTB = c(&quot;DNA&quot;,1540,2483,3)), iqtree_partition_if_by_position_user = TRUE) ## see and plot the new tree COI_CYTB_with_partitions_phylogeny[[1]] #Phylogenetic tree with 12 tips and 10 internal nodes. # #Tip labels: # Ameerega_bilinguis, Ameerega_braccata, Ameerega_hahneli, Ameerega_trivittata, Ameerega_silverstonei, Ameerega_parvula, ... # #Unrooted; includes branch lengths. plot(COI_CYTB_with_partitions_phylogeny[[1]]) The actual phylogenetic tree is in the file my_collected_COI_CYTB_String_path_fasta_txt_partition.txt.treefile in our working folder. 13.10 Other sequence phylogenetic tree estimation software 9) Other widely used programs that can do phylogenetic estimations are as follows. You need to follow their vignettes and installation instructions. a) RAxML. A manual and tutorial is provided in here. General info, vignettes and a GUI interface are also provided here. An online version is provided also here. b) PhyML 3.0. A GitHub repository associated is here. This software manual is here. 13.11 Drawing phylogenetic trees with ggtree In most instances you want to draw a phylogenetic tree for your publications, presentations or to visualize taxon/sequence relationships. A phylogenetic tree is graph or diagram that illustrates such relationships usually among organisms (or sequences of nucleotides/proteins that represent them). Each of these diagrams represent hypotheses of relationship derived from the data used to construct them (i.e., it might change if more data are added). Using R, you can plot these phylogenetic trees and noticing the pattern of branching, grouping and the relationship of tips (sources of data) from the series of common ancestors in the nodes of these diagrams. You can read this friendly summary by McLennan 2010, which provides an easy guide of how to read a phylogenetic tree. You can plot trees using diverse R packages such as ape, phylotools, and ggtree. We will use ggtree as this one uses many of the strengths of ggplot for most of color and graph manipulations. Likewise, it has a extensive vignette that make tree plotting easy. 10) You will need to install ggtree and the tree parsing package treeio from Bioconductor and have a tree file to be plotted. The package treeio provides an extensive list of formats to import. We will use the functions read.newick or read.tree to import a newick tree (the format that we get from iqtree2). For trees with support values for iqtree we will use read.iqtree to allow parsing IQ-Tree newick string, with ability to parse SH-aLRT and UFBoot support values. ## we will install ggtree if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;ggtree&quot;) library(ggtree) ## we also need treeio if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;treeio&quot;) library(treeio) ## we can import a regular iqtree file ## Notice: the extionsion is *.treefile my_tree_file_path &lt;- &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/my_phylogeny_1/Ameerega_aligned_end.fasta.treefile&quot; my_tree &lt;- read.newick(my_tree_file_path) my_tree #Phylogenetic tree with 24 tips and 22 internal nodes. # #Tip labels: # MW042032.1, MW042031.1, DQ502932.1, DQ502902.1, DQ502922.1, DQ502929.1, ... # #Unrooted; includes branch lengths. ## we can import the bootstrap summary of iqtree file ## Notice: the extionsion is *txt.treefile my_tree_boot_file_path &lt;- &quot;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/my_phylogeny_3/Ameerega_aligned_end_fasta_partition.txt.treefile&quot; my_tree_boot &lt;- read.iqtree(my_tree_boot_file_path) my_tree_boot #&#39;treedata&#39; S4 object that stored information of # &#39;~/Desktop/Teach_R/class_pages_reference/bioinformatics_gitbook_1/my_working_directory/my_phylogeny_3/Ameerega_aligned_end_fasta_partition.txt.treefile&#39;. # #...@ phylo: #Phylogenetic tree with 24 tips and 22 internal nodes. # #Tip labels: # MW042032.1, MW042031.1, DQ502932.1, DQ502902.1, DQ502922.1, DQ502929.1, ... #Node labels: # , 69, 33, 48, 29, 94, ... # #Unrooted; includes branch lengths. # #with the following features available: # &#39;SH_aLRT&#39;, &#39;UFboot&#39;. 11) Basic tree plots in ggtree are straight forward as if they were ggplot objects. Here, we will plot both trees. Add the following names/labels to our tree. To add tip labels use geom_tiplab() and bootstrap values geom_label2(). In the latter, we indicate that we will use the node labels named label, as a numeric value and only show those that are more than 75. We also added a scale with geom_treescale(). ## make sure that we load cowplot require(cowplot) ## save to an object a both tree p1 &lt;- ggtree(my_tree, color=&quot;blue&quot;) + geom_tiplab(size=1.7, color=&#39;blue&#39;) + geom_treescale(fontsize=4, linesize=1, offset=1, color=&#39;blue&#39;) p2 &lt;- ggtree(my_tree_boot) + geom_tiplab(size=1.7) + geom_label2(aes(label=label, subset = !is.na(as.numeric(label)) &amp; as.numeric(label) &gt; 75), size = 3) + geom_treescale(fontsize=4, linesize=1, offset=1) ## you can call each individually as p1 p2 ## but you can use cowplot to print both side-by-side cowplot::plot_grid(p1, p2, ncol=2, labels = LETTERS[1:2]) 13.12 Save ggtree plots as PDF or PNG 12) You can save your ggtree plots using the function ggsave() and any other ggplot graph. ## save ggtree as pdf and png ggsave(&quot;regular_bootstrap_ggtree.pdf&quot;) ggsave(&quot;regular_bootstrap_ggtree.png&quot;) "],["mining-from-a-reference-dataset.html", "Session 14 – Mining from a reference dataset 14.1 A worked example using the mtcars dataset 14.2 Build a reference dataset 14.3 Build a sampling dataset 14.4 Extracting from reference dataset 14.5 Plots of the extracted data", " Session 14 – Mining from a reference dataset Sometimes you have a list of individuals with some indices and you want to retrieve values associated from a large reference database or data frame. Using these indices will help you retrieve data from these references without going to the painstaking work to copy-and-pasting such value one by one. 14.1 A worked example using the mtcars dataset We will assume that you have four friends (Joe, Moe, Larry, Fred) and each one has a collection of cars. They want to compare the overall collection based on statistics provided by mtcars dataset. 14.2 Build a reference dataset 1) We will create the reference database as mtcars_data from the preloaded mtcars dataset. This dataset exists in the R environment. You can import a data frame (e.g., a txt or excel file) as data frame, but make sure that you include the indexing variable: my_index as demonstrated below. ## this is an example to create a reference index mtcars_data &lt;- mtcars ## we need to create a indexing variable, so will use car names mtcars_names_vector &lt;- rownames(mtcars_data) mtcars_names_vector #[1] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; &quot;Hornet Sportabout&quot; &quot;Valiant&quot; # [7] &quot;Duster 360&quot; &quot;Merc 240D&quot; &quot;Merc 230&quot; &quot;Merc 280&quot; &quot;Merc 280C&quot; &quot;Merc 450SE&quot; #[13] &quot;Merc 450SL&quot; &quot;Merc 450SLC&quot; &quot;Cadillac Fleetwood&quot; &quot;Lincoln Continental&quot; &quot;Chrysler Imperial&quot; &quot;Fiat 128&quot; #[19] &quot;Honda Civic&quot; &quot;Toyota Corolla&quot; &quot;Toyota Corona&quot; &quot;Dodge Challenger&quot; &quot;AMC Javelin&quot; &quot;Camaro Z28&quot; #[25] &quot;Pontiac Firebird&quot; &quot;Fiat X1-9&quot; &quot;Porsche 914-2&quot; &quot;Lotus Europa&quot; &quot;Ford Pantera L&quot; &quot;Ferrari Dino&quot; #[31] &quot;Maserati Bora&quot; &quot;Volvo 142E&quot; ## this vector has spaces (&quot; &quot;) and dashes (-), these might cause problems with some analyses and functions, we will replace them with underscores &quot;_&quot; mtcars_names_vector &lt;- gsub(&quot; &quot;, &quot;_&quot;, mtcars_names_vector) mtcars_names_vector &lt;- gsub(&quot;-&quot;, &quot;_&quot;, mtcars_names_vector) mtcars_names_vector #[1] &quot;Mazda_RX4&quot; &quot;Mazda_RX4_Wag&quot; &quot;Datsun_710&quot; &quot;Hornet_4_Drive&quot; &quot;Hornet_Sportabout&quot; &quot;Valiant&quot; # [7] &quot;Duster_360&quot; &quot;Merc_240D&quot; &quot;Merc_230&quot; &quot;Merc_280&quot; &quot;Merc_280C&quot; &quot;Merc_450SE&quot; #[13] &quot;Merc_450SL&quot; &quot;Merc_450SLC&quot; &quot;Cadillac_Fleetwood&quot; &quot;Lincoln_Continental&quot; &quot;Chrysler_Imperial&quot; &quot;Fiat_128&quot; #[19] &quot;Honda_Civic&quot; &quot;Toyota_Corolla&quot; &quot;Toyota_Corona&quot; &quot;Dodge_Challenger&quot; &quot;AMC_Javelin&quot; &quot;Camaro_Z28&quot; #[25] &quot;Pontiac_Firebird&quot; &quot;Fiat_X1_9&quot; &quot;Porsche_914_2&quot; &quot;Lotus_Europa&quot; &quot;Ford_Pantera_L&quot; &quot;Ferrari_Dino&quot; #[31] &quot;Maserati_Bora&quot; &quot;Volvo_142E&quot; ## we can now add this vector as our indexing variable mtcars_data$my_index &lt;- mtcars_names_vector head(mtcars_data) # mpg cyl disp hp drat wt qsec vs am gear carb my_index #Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda_RX4 #Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Mazda_RX4_Wag #Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Datsun_710 #Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet_4_Drive #Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Hornet_Sportabout #Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Valiant 14.3 Build a sampling dataset 2) Now, we will create a data frame to be used to retrive the information for the friends_data. Our indexing varaible will be my_index, so the information on mtcars_data can be combined, merged or subset to match the friends_data. A similar procedure can be performed by importing a data frame (e.g., a txt or excel file) with the same data frame, make sure that my_index variable is present. This is an illustration of how to make a data frame with data that can be indexed. ## we will start with four random vectors of the my_index variable to serve as what cars these friends have Joe_10_cars_vector &lt;- sample(mtcars_data$my_index, 10, replace = TRUE) Moe_15_cars_vector &lt;- sample(mtcars_data$my_index, 15, replace = TRUE) Larry_12_cars_vector &lt;- sample(mtcars_data$my_index, 12, replace = TRUE) Fred_20_cars_vector &lt;- sample(mtcars_data$my_index, 20, replace = TRUE) ## NOTICE: that this are random samples and each of you will get a different set. ## we build a two-column data frame as follows by making a list of data frames and collect each one friends_list &lt;- list() ## Joe Joe_df &lt;- data.frame(my_index = Joe_10_cars_vector, stringsAsFactors = FALSE) Joe_df$friend &lt;- &quot;Joe&quot; friends_list[[1]] &lt;- Joe_df ## Moe Moe_df &lt;- data.frame(my_index = Moe_15_cars_vector, stringsAsFactors = FALSE) Moe_df$friend &lt;- &quot;Moe&quot; friends_list[[2]] &lt;- Moe_df ## Larry Larry_df &lt;- data.frame(my_index = Larry_12_cars_vector, stringsAsFactors = FALSE) Larry_df$friend &lt;- &quot;Larry&quot; friends_list[[3]] &lt;- Larry_df ## Fred Fred_df &lt;- data.frame(my_index = Fred_20_cars_vector, stringsAsFactors = FALSE) Fred_df$friend &lt;- &quot;Fred&quot; friends_list[[4]] &lt;- Fred_df ## we rbind these data frames into one friends_data &lt;- do.call(rbind,friends_list) str(friends_data) #&#39;data.frame&#39;: 57 obs. of 2 variables: # $ my_index: chr &quot;Porsche_914_2&quot; &quot;Dodge_Challenger&quot; &quot;Merc_450SE&quot; &quot;Fiat_X1_9&quot; ... # $ friend : chr &quot;Joe&quot; &quot;Joe&quot; &quot;Joe&quot; &quot;Joe&quot; ... head(friends_data) # my_index friend #1 Porsche_914_2 Joe #2 Dodge_Challenger Joe #3 Merc_450SE Joe #4 Fiat_X1_9 Joe #5 Merc_280C Joe #6 Merc_230 Joe 14.4 Extracting from reference dataset 3) We can subset the mtcars_data to match the friends_data using the function merge and the argument by.x = \"my_index\". This will subset and merge from the mtcars_data the information about the cars based only on values of my_index in friends_data. ## with reference data frame friends_data_with_mtcars_data &lt;- merge(friends_data, mtcars_data, by.x = &quot;my_index&quot;) head(friends_data_with_mtcars_data) # my_index friend mpg cyl disp hp drat wt qsec vs am gear carb #1 AMC_Javelin Fred 15.2 8 304 150 3.15 3.435 17.30 0 0 3 2 #2 AMC_Javelin Fred 15.2 8 304 150 3.15 3.435 17.30 0 0 3 2 #3 Cadillac_Fleetwood Larry 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4 #4 Cadillac_Fleetwood Larry 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4 #5 Chrysler_Imperial Larry 14.7 8 440 230 3.23 5.345 17.42 0 0 3 4 #6 Datsun_710 Moe 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## we can order by friend name friends_data_with_mtcars_data_order &lt;- friends_data_with_mtcars_data[order(friends_data_with_mtcars_data$friend),] head(friends_data_with_mtcars_data_order) # my_index friend mpg cyl disp hp drat wt qsec vs am gear carb #1 AMC_Javelin Fred 15.2 8 304 150 3.15 3.435 17.30 0 0 3 2 #2 AMC_Javelin Fred 15.2 8 304 150 3.15 3.435 17.30 0 0 3 2 #7 Datsun_710 Fred 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #8 Datsun_710 Fred 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #12 Duster_360 Fred 14.3 8 360 245 3.21 3.570 15.84 0 0 3 4 #13 Ferrari_Dino Fred 19.7 6 145 175 3.62 2.770 15.50 0 1 5 6 4) You can determine different parameters of the car sets per each friend and compere between them ## We will split the large data frame into each friend list_friends &lt;- split(friends_data_with_mtcars_data_order, friends_data_with_mtcars_data_order$friend) ## fred fred_df &lt;- list_friends[[1]] head(fred_df) # my_index friend mpg cyl disp hp drat wt qsec vs am gear carb #1 AMC_Javelin Fred 15.2 8 304 150 3.15 3.435 17.30 0 0 3 2 #2 AMC_Javelin Fred 15.2 8 304 150 3.15 3.435 17.30 0 0 3 2 #7 Datsun_710 Fred 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #8 Datsun_710 Fred 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #12 Duster_360 Fred 14.3 8 360 245 3.21 3.570 15.84 0 0 3 4 #13 Ferrari_Dino Fred 19.7 6 145 175 3.62 2.770 15.50 0 1 5 6 ## you can do the same as: fred_df &lt;- subset(friends_data_with_mtcars_data_order, friend == &quot;Fred&quot;) joe_df &lt;- list_friends[[2]] larry_df &lt;- list_friends[[3]] moe_df &lt;- list_friends[[4]] We can ask which friend has more weight in their car set? sum(fred_df$wt) #[1] 58.893 sum(joe_df$wt) #[1] 30.205 sum(larry_df$wt) #[1] 44.445 sum(moe_df$wt) #[1] 44.445 In this case, Fred clearly has won. We can ask which friend has more horsepower in their car set? sum(fred_df$hp) #[1] 2756 sum(joe_df$hp) #[1] 1166 sum(larry_df$hp) #[1] 1816 sum(moe_df$hp) #[1] 1816 14.5 Plots of the extracted data 5) We can do some plots to exemplify some of these observations. We can plot boxplots for hp. ## check ig ggplot2 is loaded require(ggplot2) ## Boxplots with standard color friends_hp_boxplot &lt;- ggplot(friends_data_with_mtcars_data_order, aes(x=friend, y=hp, fill=friend)) + geom_boxplot(width=0.3) + labs(title=&quot;Friends car collection -- HP boxplots&quot;, x=&quot;Friends&quot;, y=&quot;hp&quot;) + guides(fill=FALSE) + theme_minimal() friends_hp_boxplot We can plot density for wt. ## Overlaid densities friends_wt_plot_den &lt;- ggplot(friends_data_with_mtcars_data_order, aes(x=wt, fill=friend)) + geom_density(alpha=.3) + ggtitle(&quot;Friends car collection -- WT density plots&quot;) + theme_minimal() friends_wt_plot_den We can have a scatterplot for mpg by hp. ## scatterplot friends_scatterplot_mpg_hp &lt;- ggplot(friends_data_with_mtcars_data_order, aes(x=mpg, y = hp, color=friend)) + geom_point() + ggtitle(&quot;Friends car collection -- mpg versus hp&quot;) + theme_minimal() friends_scatterplot_mpg_hp "],["introduction-to-python.html", "Session 15 – Introduction to Python 15.1 Welcome to Python! 15.2 Which Python should I install?", " Session 15 – Introduction to Python 15.1 Welcome to Python! Why Python? This is relatively easy to learn compared to most programming languages and is vastly popular among both programmers and employers. Code written in Python is universal among operating systems; allowing code to be carried between Windows, Mac and Linux without the need to re-write the code. Python code is quite minimalist compared to other languages, making reading and writing code quicker and easier. Even non-programmers are able to read Python code with little issue. Because there is less to type, it is subsequently easier to debug and maintain Python code. Lastly, Python is highly desirable throughout many fields of work. Popular applications of Python include, but are not limited to: web development, machine learning, data science, and analytics. The possibilities with Python are further broadened by its ability to incorporate other languages with easy. Imagine building the foundation of code in Python and utilize other languages like R and Java in places where they exceed Python’s capabilities. 15.2 Which Python should I install? Python has releases dating to 1990 but it was first created in the 1980s. It has evolved over the past 40 years and big updates have greatly expanded Python’s utility and changed its syntax to become more user friendly. It is because of these big changes that we have a major divide between Python2 and Python3. Python2 includes any version conforming with the following structure: Python v2.x.x. Ideally you would install Python3 as it is the most recent version; however, there exists code that will only run on Python2 online. Python3 is not backwards compatible. Long story short, download both recent versions of Python2 and Python3, but we will use only Python3 in this course. If you encounter a program that operates in Python2 you can always use that version you installed to run it. We will install: Python v2.7.18 and Python v3.9.2. ## You write here what it would look in a console code code code "]]
